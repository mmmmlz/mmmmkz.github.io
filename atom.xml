<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>kerwin&#39;s notebook</title>
  
  <subtitle>keep learning</subtitle>
  <link href="http://kerwinblog.top/atom.xml" rel="self"/>
  
  <link href="http://kerwinblog.top/"/>
  <updated>2023-06-04T13:40:42.299Z</updated>
  <id>http://kerwinblog.top/</id>
  
  <author>
    <name>mmmmlz</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>When Search Meets Recommendation</title>
    <link href="http://kerwinblog.top/2023/06/02/20230601/"/>
    <id>http://kerwinblog.top/2023/06/02/20230601/</id>
    <published>2023-06-02T13:21:12.000Z</published>
    <updated>2023-06-04T13:40:42.299Z</updated>
    
    <content type="html"><![CDATA[<p>现代在线服务提供商（如在线购物平台）通常提供搜索和推荐（S＆R）服务，以满足不同的用户需求。很少有有效的方法可以有效的利用S＆R服务中的用户行为数据。大多数现有方法要么只是单独处理S＆R行为，要么通过聚合来自两种服务的数据来联合优化它们，忽略了S＆R中用户意图是有着明显的不同这个前提条件。在这篇论文中，我们提出了一种Search-Enhanced的架构，将用户的搜索意图用于序列推荐。具体来说，SESRec利用用户的搜索query以及item的交互来计算其embedding的相似度，这里使用了两个独立的transformer编码器去学习上下文的表达。然后设计了一个对比学习任务作为学习到的相似和不相似的表达的监督。最后，我们通过三个角度（即上下文表示、包含相似和不相似兴趣点的两个分离行为）使用注意力机制提取用户兴趣。</p><p><a href="https://arxiv.org/pdf/2305.10822.pdf">Learning DisentangledSearch Representation for Recommendation</a></p><h1 id="背景">背景</h1><p>目前，随着互联网数据的大量增加，仅使用推荐系统或搜索引擎不能满足用户的信息需求。因此，许多社交媒体平台（如YouTube和TikTok）为用户提供搜索和推荐服务，以获取信息。由于用户在两种情境下表达了各自的兴趣，因此通过联合建模两者的行为来增强推荐系统是可行的，而核心挑战在于如何有效地利用用户的搜索兴趣来捕捉准确的推荐兴趣。</p><p>大多数先前的工作忽略了用户在搜索和推荐行为中的兴趣差异，在建模时不考虑它们之间的相关性。然而，在现实世界的应用中，搜索行为可能会加强或补充推荐行为中所揭示的兴趣。例如，图1(a)展示了短视频场景下用户部分行为历史记录。当用户浏览推荐系统建议的视频/商品时，他们可能会自发地开始搜索，这些搜索通常与推荐流中的视频内容不同。我们将这种情况称为自发搜索。相反，用户也可以通过点击与当前正在播放的项目/视频相关的建议查询来开始搜索，我们将其称为被动搜索。为了验证这种现象的普遍性，我们从快手应用程序收集的真实数据进行了数据分析，如图1(b)所示。数据分析基于数百万用户的行为。对于每个搜索行为，如果项目的类别存在于用户在过去七天内与之交互的项目的类别集中，则该搜索行为类似于最近的推荐行为，否则不相似。类似的搜索行为反映了用户在推荐行为中重叠的强烈兴趣，并应加强。不相似的行为可能是未被发现的兴趣，这些兴趣可能是新兴的，并且在推荐流中没有得到满足。<strong>因此，分离S&amp;R行为之间的相似和不相似表示至关重要</strong>。</p><p><img src="/2023/06/02/20230601/image-20230603170633369.png" style="zoom:80%;"></p><p>为了解决这个问题，我们设计了一个名为SESRec的搜索增强框架，在推荐时同时学习解耦后的搜索表达。具体而言，为了解耦两个行为之间的相似和不相似兴趣，我们提出将每个历史序列分解为两个子序列，分别表示相似和不相似的兴趣，以便我们可以从多个方面提取用户兴趣。为了学习两个行为之间的相似性，我们首先基于用户的query-item交互，使用InfoNCE损失来学习query-item的embeddings。然后，我们使用两个单独的编码器来建模S&amp;R行为并生成它们的上下文表示。由于缺乏label来表示所获得的上下文表示之间的兴趣相似性，我们提出利用自监督来指导学习相似和不相似的兴趣。具体而言，我们利用co-attention机制来学习S&amp;R上下文表示之间的相关性。基于co-attention分数，对于两个上下文表示，我们不仅将它们聚合起来生成被认为维护S&amp;R共享兴趣的锚点，还将它们分成两个子序列，分别被认为表示S&amp;R之间的相似和不相似行为（分别称为正例和负例）。然后，我们按照对比学习原则设计了一种新的相似性损失函数，以鼓励嵌入空间中的正样本比负样本更接近。通过联合优化InfoNCE损失和相似性损失，我们可以学习到推荐模型的解耦搜索表示。最后，我们使用注意力机制从三个方面提取用户兴趣，即S&amp;R的上下文表示、正例和负例。通过这种方式，S&amp;R行为的解耦兴趣增强了对下一次交互的预测。</p><h1 id="方法">方法</h1><h2 id="problem-setting">problem setting</h2><p>定义用户、商品、query的集合分别为<span class="math inline">\(\mathcal{U},\mathcal{I},\mathcal{Q}\)</span>,对于用于<span class="math inline">\(u \in \mathcal{U}\)</span>,定义<span class="math inline">\(S_i^u=[i_1,i_2...i_{T_r}]\)</span>为按照时间顺序的交互商品列表，<span class="math inline">\(S_q^u=[q_1,q_2...q_{T_s}]\)</span>为用户的搜索历史query，同时，在用户进行搜索时，对每一个query，都可能会有一些交互过的商品，<span class="math inline">\(S_c^u =[i_{q_1}^{(1)},i_{q_1}^{(2)},i_{q_2}^{(1)},i_{q_2}^{(2)},...i_{q_{T_s}}^{(1)},i_{q_{T_s}}^{(2)}，i_{q_{T_s}}^{(3)}]\)</span>其中<span class="math inline">\(i_{q_{k}}^{(j)}\)</span>代表用户在搜索<span class="math inline">\(q_k\)</span>时，点击的第<span class="math inline">\(j\)</span>个商品。</p><p>基于以上定义，我们定义使用搜索数据的序列推荐任务为，给定<span class="math inline">\(S_i^u,S_q^u,S_c^u\)</span>,去预测用户最有可能点击的下一个item：<span class="math inline">\(P(i_{t+1}|S_i^u,S_q^u,S_c^u)\)</span>,而传统的序列推荐任务可以表示为<span class="math inline">\(P(i_{t+1})|S_i^u\)</span></p><h2 id="method">method</h2><p>SESRec的整体结构如图2所示。首先经过embedding层将稀疏数据稠密化。然后，我们利用transformer层来学习历史行为的上下文表示。为了分离兴趣，我们将两个行为序列分成表示相似和不相似兴趣的子序列。我们将行为序列聚合成向量，以表示用户对候选项的兴趣。最后，我们将所有向量连接在一起，得到整体表示向量，然后使用多层感知机（MLP）生成最终预测。</p><p>具体来说，我们设计了几个组件来通过自监督来分离用户兴趣并从各个方面聚合用户兴趣。这些设计的组件在图2中用彩色框表示。我们使用InfoNCE损失将查询和项表示对齐到相同的语义空间中。然后我们分别将S＆R行为序列分成子序列。我们利用三元组损失来指导分离的自监督信号。最后，我们引入一个兴趣提取模块，将原始序列和构建的子序列聚合起来，形成行为的聚合、相似和不相似兴趣表示。</p><p><img src="/2023/06/02/20230601/image-20230603174706794.png"></p><h3 id="encoding-sequential-behaviors">Encoding SequentialBehaviors</h3><h4 id="embedding-layer">Embedding Layer</h4><p>我们将用户（商品）的表达定义为<span class="math inline">\(e^u =e^{ID_u}||e^{a_1}||...||e^{a_n}(e^i=e^{ID_i}||e^{b_1}||...||e^{b_n})\)</span>其中<span class="math inline">\(||\)</span>表示concat操作，<span class="math inline">\(a,b\)</span>分别代表用户、商品的各种属性。特别的对于query，每一个query包含了一些交互items<span class="math inline">\((w_1,w_2...w_{|q|})\)</span>，我们将query的embedding定义为queryid的embedding与所有相关items的mean poooling之后进行concat。<span class="math inline">\(e^q =e^{ID_q}||\text{MEAN}(e^{w_1},...e^{w_{|q|}})\)</span>(这个操作其实增加了query的丰富性，因为搜索中有大量的重复query，单纯用idembedding 很难区分)</p><p>这时我们就可以将用户的item序列、query系列、以及query交互序列的embedding表示出来<span class="math inline">\(\text{E}_i \in \mathbb{R}^{T_r \timesd_i},\text{E}_q \in \mathbb{R}^{T_s \times d_q},\text{E}_c \in\mathbb{R}^{T_r \timesd}\)</span>,特别的，对于搜索来说，用户可能从不同的source进行搜索，比如用户输入的、历史搜索的、还有当前item相关的搜索等，这些组成了搜索域的embedding矩阵<span class="math inline">\(\text{M}_s \in \mathbb{R}^{k \timesd}\)</span>,其中<span class="math inline">\(k\)</span>代表所有的搜索域数量。</p><p>由于在不对齐的向量空间中使用查询和物品表示来建模用户兴趣非常具有挑战性，因此我们将item和query的embedding转换为具有相同的维度。计算方式为：<span class="math display">\[\hat{\text{E}_i} = \text{E}_i\text{W}_i,\ \hat{\text{E}_q} =\text{E}_q\text{W}_q, \  \hat{\text{E}_c} = \text{E}_c\text{W}_i\]</span> 其中<span class="math inline">\(\text{W}_i \in \mathbb{R}^{d_i\times d},\ \text{W}_q \in \mathbb{R}^{d_q \timesd}\)</span>，这样处理后，item序列、query系列、以及query交互序列都会变成<span class="math inline">\(d\)</span>维。</p><h4 id="bias-embedding">Bias Embedding</h4><p>为了建模用户序列的顺序，我们设计了可学习的positionembedding分别用于item序列以及query序列，<span class="math inline">\(\text{P}_r \in \mathbb{R}^{T_r \times d},\\text{P}_s \in \mathbb{R}^{T_s \timesd}\)</span>,最终得到的item序列Embedding为： <span class="math display">\[\text{E}_r = \hat{\text{E}}_i+\text{P}_i\]</span> 对于query序列来说要复杂一点，除了positionembedding外，我们还设计了type embedding $ _s$。 <span class="math display">\[\text{E}_s = \hat{\text{E}_q} + \tilde{\text{E}_c} + \text{P}_s +\hat{\text{M}_s}\]</span> 其中，<span class="math inline">\(\tilde{\text{E}_c} \in\mathbb{R}^{T_s \times d }\)</span>为在当前query下，所有交互过的item的embedding的mean pooling。typeembedding <span class="math inline">\(\tilde{\text{M}_s} \in\mathbb{R}^{T_s \times d }\)</span>定义为搜索历史中每个query的类型嵌入序列，其中 $ $中的每个元素是从查找表$ M_s $中获得的。我们添加typeembedding以模拟搜索行为和搜索来源之间的相关性。</p><h4 id="transformer-layer">Transformer Layer</h4><p>为了学习给定序列中每个元素的增强上下文表示，我们使用Transformer层来捕捉S＆R序列中每个元素与其他元素之间的关系。Transformer层通常由两个子层组成，即多头自注意力层和点式前馈网络。我们分别将Transformer层应用于S＆R序列：</p><p><img src="/2023/06/02/20230601/image-20230604003456190.png"></p><h3 id="self-supervised-interest-disentanglement">Self-supervisedInterest Disentanglement</h3><p>正如之前提到的，搜索和推荐行为之间的用户兴趣存在重叠和差异。由于不存在任何用户兴趣的注释标签，我们利用对比学习技术来通过自我监督解开搜索和推荐行为，并从三个方面提取用户兴趣，即聚合行为、包含相似和不相似兴趣的两个分离行为。</p><h4 id="query-item-alignment">Query-item Alignment</h4><p>行为编码器面临的挑战是共同学习具有不对齐embeddings的S＆R行为中的用户兴趣。此外，如果不知道query-item之间的语义相似性，则无法将用户兴趣与S＆R行为分离开来。因此，在进一步从中提取用户兴趣之前，我们如下对query-item的嵌入进行对齐。由于项目和查询具有不同形式的特征，它们原始的嵌入在不同的向量空间中不对齐。如方程（1）所示，我们首先将query-item的embeddings转换为相同的维度。然后，受到多模型学习工作的启发，我们利用对比学习损失来教导模型哪些query-item是相似或不同的。我们之前得到的query系列、以及query交互序列为：<span class="math inline">\(\hat{\text{E}_q}=[\hat{e_1^q},\hat{e_2^q},...\hat{e_{T_s}^q}]\in \mathbb{R}^{T_s \times d}\)</span>,<span class="math inline">\(\hat{\text{E}_c}=[\hat{e_1^i},\hat{e_2^i},...\hat{e_{S_c^u}^i}]\in \mathbb{R}^{|S_c^u| \times d}\)</span></p><p>我们最小化如下两个InfoNCE损失的和，一个是query-to-item，另一个是item-to-query。</p><p><img src="/2023/06/02/20230601/image-20230603195513486.png"></p><p>其中，<span class="math inline">\(\tau\)</span>是可学习的温度系数，<span class="math inline">\(|q_j|\)</span>表示在某个query下，用户点击商品的数量，满足<span class="math inline">\(\sum_{j=1}^{T_s} |q_j| = |S_c^u|\)</span>,<span class="math inline">\(\mathcal{I}_{neg},\mathcal{Q}_{neg}\)</span>分别是随机采样的items和querys，<span class="math inline">\(s\)</span>为相关性函数，<span class="math inline">\(s(p,q) = \tanh(p^TW_Aq),W_A \in \mathbb{R}^d\timesd\)</span>,（这里的w参数矩阵可以使得在计算q-i的相关性时，与在线推理时可以不同）最终的Query-itemAlignment 损失函数如下： <span class="math display">\[\mathcal{L}_{ali}^{u,t} = \frac{1}{2} (\mathcal{L}_{A_{q2i}}^{u,t} +\mathcal{L}_{A_{i2q}}^{u,t})\]</span></p><h4 id="interest-contrast">Interest Contrast</h4><p>为了将相似和不相似的兴趣解耦，我们采用对比学习机制来区分<span class="math inline">\(H_s\)</span>和<span class="math inline">\(H_r\)</span>的上下文表示中相似和不相似的兴趣。在transformer层之后，给定矩阵<span class="math inline">\(H_s\)</span>和<span class="math inline">\(H_r\)</span>，我们构建了两个行为的共同依赖表示矩阵，生成了两个序列的相似度分数。我们利用了co-attention技术。我们首先计算一个亲和矩阵<span class="math inline">\(\text{A} \in \mathbb{R}^{T_s \timesT_r}\)</span>，计算方式如下： <span class="math display">\[\text{A} = \tanh(\text{H}_s\text{W}_l(\text{H}_r)^{\text{T}})\]</span> 其中，<span class="math inline">\(\text{W}_l \in \mathbb{R}^{d\times d}\)</span>是一个可学习的参数矩阵，亲和矩阵<span class="math inline">\(\text{A}\)</span>包含与所有推荐行为和搜索行为对应的亲和度分数。我们将亲和矩阵A和搜索矩阵<span class="math inline">\(H_s\)</span>（或推荐矩阵<span class="math inline">\(H_r\)</span>）相乘，然后对乘积结果进行归一化，以获得一个序列中每个元素在另一个序列中所有元素上的相似度分数：<span class="math display">\[\text{a}^s = \text{softmax}(\text{W}_r\text{H}_r^{\text{T}}\text{A}^{\text{T}}), \ \text{a}^r =\text{softmax}(\text{W}_s\text{H}_s^{\text{T} }\text{A}) \\\text{a}^r \in \mathbb{R}^{T_r},\text{a}^s \in \mathbb{R}^{T_s},\text{W}_r ,\text{W}_s  \in \mathbb{R}^{1 \times d}\]</span>接下来，我们利用三元组损失来自监督地分离两个行为之间相似和不相似的兴趣。给定相似度分数<span class="math inline">\(\text{a}^s\)</span>和<span class="math inline">\(\text{a}^r\)</span>，在我们得到的推荐行为序列以及搜索行为序列<span class="math inline">\(H_s\)</span>和<span class="math inline">\(H_r\)</span>中，分数较高的可以作为兴趣的代表，这里我们使用阈值截断的方式将所有行为分成两个子序列。如下所示：</p><p><img src="/2023/06/02/20230601/image-20230603233931534.png"></p><p>这里由于相似度分数<span class="math inline">\(\text{a}^s\)</span>和<span class="math inline">\(\text{a}^r\)</span>经过了softmax归一化，阈值设定为<span class="math inline">\(\frac{1}{T_s}\)</span>和<span class="math inline">\(\frac{1}{T_{r}}\)</span>，这样，相似的行为是那些得到大于平均分的，不相似的行为则是得分小于平均分的。</p><p>最后，为了使用三元组损失，我们要先定义要锚点、正样本和负样本，</p><p><img src="/2023/06/02/20230601/image-20230603234733851.png"></p><p><img src="/2023/06/02/20230601/image-20230603234744618.png"></p><p>其中，<span class="math inline">\(\text{h}_j^s,\text{h}_j^r\)</span>分别为行为序列矩阵<span class="math inline">\(H_s\)</span>和<span class="math inline">\(H_r\)</span>中第<span class="math inline">\(j\)</span>个向量，在对比学习中，我们希望将锚点与正样本的距离拉近，与负样本的距离拉远，那么损失函数可以写成：<span class="math display">\[\mathcal{L}_{tri}(a,p,n) = \max{\{d(a,p)-d(a,n)+m,0\}}\]</span> <span class="math inline">\(d\)</span>时距离度量函数，这里使用了euclideandistance（欧式距离），<span class="math inline">\(m\)</span>为控制边界的超参数，<span class="math inline">\(a,p,n\)</span>分别表示锚点、正样本与负样本。最后，InterestContrast 的总loss可以看成是推荐序列的损失和搜索序列的损失的和：</p><p><img src="/2023/06/02/20230601/image-20230603235534965.png"></p><blockquote><p><strong>备注。</strong>在大多数情况下，用户使用S&amp;R服务的频率不同。由于推荐和搜索的行为序列是从不同的服务中收集的，所以两种行为的长度和更新频率也不同。这就是为什么文章分别对这两种行为采用三元组损失。使用各自构建的兴趣表示来更新每种行为的模型参数，这可以确保模型训练的一致性。此外，考虑到相似和不相似的兴趣通常在一定程度上重叠，它们之间没有明确的区别。三元组损失执行成对比较，减少了相似事物之间的差异，增加了不同事物之间的差异。这就是为什么本文使用三元组损失而不是其他对比损失函数（例如InfoNCE），后者对正样本和负样本之间的相似性施加了过于强烈的惩罚。</p></blockquote><h4 id="multi-interest-extraction">Multi-interest Extraction</h4><p>基于原始行为和包含相似和不相似兴趣的分离行为，我们从三个方面提取用户兴趣，即聚合兴趣、相似兴趣和不相似兴趣。给定一个候选商品<span class="math inline">\(v\)</span>，我们利用注意力机制重新分配用户兴趣，以候选项为中心。对于推荐行为，可以从以下三个方面提取兴趣：</p><ol type="1"><li>聚合兴趣：将用户历史行为中所有与候选项相关的兴趣进行聚合，形成一个全局的兴趣表示。</li><li>相似兴趣：从用户历史行为中提取与候选项相似的兴趣，例如相同的类别、标签等。</li><li>不相似兴趣：从用户历史行为中提取与候选项不相似的兴趣，例如与候选项相关的但不同类别、不同标签等。</li></ol><p><img src="/2023/06/02/20230601/image-20230604000120437.png"></p><p>最终将这三个兴趣项拼接起来，就得到了最终的兴趣表征<span class="math inline">\(u_r =u_r^{all}||u_r^{sim}||u_r^{diff}\)</span>,同样的方法可以得到搜索的兴趣表征。</p><h3 id="prediction-and-model-training">Prediction and ModelTraining</h3><h4 id="prediction">Prediction</h4><p>预测的过程相对简单，首先通过上文的模型得到用户的兴趣表征，最后的打分为：</p><p><img src="/2023/06/02/20230601/image-20230604001112889.png"></p><p>就是将两个兴趣表征（推荐&amp;搜索）加上用户和item的Embedding拼接到一起，过两层MLP算出打分。</p><h4 id="model-training">Model Training</h4><p><img src="/2023/06/02/20230601/image-20230604001303664.png"></p><p>训练时的损失函数如上，其中<span class="math inline">\(\mathcal{O}\)</span>是一个batch的训练数据，有一个正样本，N-1个负样本，为了增加额外的query-item的语义相似性和兴趣区分信息，我们用multi-task的方式设计了端到端的模型，总的损失函数如下：</p><p><img src="/2023/06/02/20230601/image-20230604001644491.png"></p><p>其中，<span class="math inline">\(\mathcal{U}\)</span>为user的集合，<span class="math inline">\(T_u\)</span>是用户最近的交互行为。<span class="math inline">\(\alpha,\beta\)</span>为超参数，后面还增加了L2正则项防止过拟合。</p><h1 id="结论">结论</h1><p>这篇文章的模型虽然不复杂，但是从理论到算法实现都很流畅，从用户的搜索方法以及情景不同，反应到用户的兴趣不同，再到进行推荐时需要考虑到用户的搜索行为以及交互行为，同时使用对比学习的方式将不同行为序列进行提纯，对不同兴趣的划分看起来比较符合实际情况，给出的解法也简单可行。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;现代在线服务提供商（如在线购物平台）通常提供搜索和推荐（S＆R）服务，以满足不同的用户需求。很少有有效的方法可以有效的利用S＆R服务中的用户行为数据。大多数现有方法要么只是单独处理S＆R行为，要么通过聚合来自两种服务的数据来联合优化它们，忽略了S＆R中用户意图是有着明显的不</summary>
      
    
    
    
    <category term="paper Reading" scheme="http://kerwinblog.top/categories/paper-Reading/"/>
    
    
    <category term="Rec System" scheme="http://kerwinblog.top/tags/Rec-System/"/>
    
  </entry>
  
  <entry>
    <title>他山之石————推荐系统中的跨域建模</title>
    <link href="http://kerwinblog.top/2023/05/25/0525/"/>
    <id>http://kerwinblog.top/2023/05/25/0525/</id>
    <published>2023-05-25T11:01:41.000Z</published>
    <updated>2023-06-02T13:23:40.145Z</updated>
    
    <content type="html"><![CDATA[<p>推荐系统是在线服务的重要组成部分，特别是对于电子商务平台而言。在推荐系统中，转化率（CVR）预测对于优化电子商务的总交易额目标至关重要。然而，CVR存在众所周知的样本选择偏差（SSB）和数据稀疏性（DS）问题。虽然现有的方法ESMM和ESM2通过建模用户行为路径来训练所有展示样本，但是此方法也只是通过构建在某个特点场景下的行为路径来解决DS问题，而在实际的推荐系统中，面临的往往是有着不同性质的场景之间的样本稀疏问题。</p><p>这段时间梳理了几篇针对解决跨域建模等一系列问题的几篇论文，在这里整理一下。</p><h1 id="全域学习">全域学习</h1><p><a href="https://arxiv.org/pdf/2303.00276.pdf">《Entire SpaceLearning Framework: Unbias Conversion Rate Prediction in Full Stages ofRecommender System》</a></p><h2 id="方法">方法</h2><p>为了解决推荐链路中的样本选择偏差问题和数据稀疏问题，我们提出了基于全域学习的信息流推荐全链路无偏学习解决方案，重新构建了概率空问，充分利用了推荐系统的漏斗结构，直接建模了漏斗结构中上一阶段汳回结果到正样本的概率，使得训练与预测在推荐系統中一致，从而消除样本选择偏差；同时模型引入全网的领域知识辅助本场景的排序模型学习，将上一阶段返回结果到王样本的概率进一步拆分成上一阶段返回结果到全网正样本的概率与全网正样本概率到本场景正样本概率的乘积，有效的解決了数据稀疏问题。</p><h3 id="形式化定义">形式化定义</h3><p>我们的研究主要集中在排序阶段，以GMV优化为目标的推荐模型可以拆解为：流量X点击率X转化率X笔单价，传统对<span class="math inline">\(\text{CTR} \times\text{CVR}\)</span>的建模方式如下：<span class="math inline">\(p_{pvTopay}=p(Click=1|Pv=1) *p(Pay=1|Click=1)\)</span>然而这样建模存在两个问题，一个是样本选择偏差问题，加上训练数据为曝光的样本，而预测的样本为推荐系统上一阶段的结果，例如精排模型的预测样本为粗排返回的结果，粗排模型的预测样本为召回返回的结果。存在训练和预测不一致的问题，进一步产生数据闭环导致问题加重；另一个问题是数据稀疏问题，在排序阶段往往只会用到本场景的曝光样本，忽略了用户在全场景的丰富行为。因此，如何解决样本选择偏差使排序阶段的训练和预测保持一致，以及如何在推荐系统的全链路阶段均引入用户在全场景的样本是非常重要的问题。</p><p>基于此，我们提出了基于全域学习的信息流推荐全链路无偏学习解決方案，重新构建概率空问，作为现有概率<span class="math inline">\(P_{PvToPay}\)</span>的补充，同时引入用户在全网的行为，以此缓解上述问题。首先我们重新构建了概率空间，以精排模型为例，为了使训练空问与预测空间一致从而解决样本选择偏差问题，我们充分利用推荐漏斗式的结构，直接构建粗排返回结果到正样本的概率，即：<span class="math inline">\(p_{DrToPay}=p(Pay=1|Drr=1)\)</span>, 其中 <span class="math inline">\(\{Drr =1\}\)</span>是粗排返回的结果集合。进一步的为了在排序的每个阶段均引1入用户在全网的行为从而缓解数据稀疏问题，我们将概率<span class="math inline">\(P_{DrrToPay}\)</span> 进一步拆解成如下形式： <span class="math display">\[P_{DrrToPay}= p(Pay=1|Drr =1)=p(Pay_{all} =1|Drr =1)*p(Pay_{gul}=1|Pay_{all} =1)\]</span> ，其中 $p(Pay_{all} =1|Drr =1)$是粗排返回结果到全网成交正样本的概率， <span class="math inline">\({Pay_{all} = 1}\)</span>是粗排返口结果且用户在全网成交的样本集合，<span class="math inline">\(P(Pay_{gul}=1|Pay_{all} =1)\)</span>是全网成交正样本到本场景成交正样本的概率，<span class="math inline">\({Pay_{gul} =1}\)</span>是最终在本场景的成交正样本集合。这样建模有3个优点：1）统一了模型训练和预测的候选数据集，以精排模型为例其训练样本集合与预测样本集合均是粗排返回的结果集合，不再存在线上预测与离线训练不一致的问题，缓解了样本选择偏差问题；2）在推荐系统链路阶段显示且直接地建模了用户在全网的成交行为，建模推荐系统漏斗结构中上一阶段结果到全网正样本的概率，缓解了数据稀疏问题；3）建模全网样本到本场景样本的概率，从而使得排序模型的最终建模目标为本场景的正样本，更符合本场景的心智，滅少无用信息的引入，减少适合其他场景而不适合本场景的样本曝光。最终精排模型建模了人粗排返回结果到本场景成交的概率，公式如下： <span class="math display">\[p_{DrrToPay} = p(Pay_{all}=1|Drr=1)*p(Pay_{gul}=1|Pay_{all}=1)\]</span>如上概率构建方式能够使得训练和预测的候选数据集保持一致，有效缓解数据偏差问题，同时在本场景有效引入全域领域知识，缓解数据稀疏问题。最终<span class="math inline">\(P_{DrrToPay}\)</span>可以作为 <span class="math inline">\(P_{PvToPay}\)</span>​的补充，在线上共同进行预测，最终以GMV 为优化目标的问题形式化如下： <span class="math display">\[GMV = (p_{PvToPay}+ a * p_{DrrToPo}）* Price\]</span>粗排模型的优化目标与精排类似,不同的是候选集合用到的一阶段的召回结果集合，建模了从召回到本场景成交的概率，形式化如下：<span class="math display">\[P_{RecallToPay}=p(Pay_{all} =1 |Recall =1)*p（Pay_{gul} =1| Pay_{all}=1)\]</span></p><h3 id="模型">模型</h3><p>模型的整体结构分为两部分，左边的一部分<span class="math inline">\(p(Pay_a=1|PS=1)\)</span>建模的是上一阶段结果到全网成交的概率，右边一部分<span class="math inline">\(p(Pay_g =1|Pay_a=1,PS=1)\)</span>建模的是全网成交到本场景成交的概率。模型的整体结构如下：</p><p><img src="/2023/05/25/0525/image-20230525201707274.png"></p><h4 id="embedding-layer">Embedding Layer</h4><p>模型的输入层是Embedding层，它将输入的大量稀疏特征映射到低维稠密的向量空间中，模型的输入特征主要包括用户特征画像特征、商品属性特征、上下文特征、交叉特征和行为序列特征。</p><h4 id="layer-normal-target-attention-layer">Layer Normal TargetAttention Layer</h4><p>用于处理行为序列特征，该层捕获了目标商品Target item与用户历史行为序列的相似性，考虑向量的物理意义，我们使用内积来计算Attention，两个item越相似，权重越大；并采用Mutil-HeadAttention的方式有效的将序列特征放到多个平行的空间进行计算，提升模型的容错性和精准度。<span class="math display">\[Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})\]</span></p><h4 id="mlp-layer">MLP layer</h4><p>得到所有特征的向量表示之后，最终将向量串联起来输入到 MLP Layer 中通过Sigmoid 函数获得<span class="math inline">\(p(Pay_{all} =1|Drr=1)\)</span> 。为了建模用户隐式反馈中的bias，比如 Position Bias，很多时候用户点击某个商品井不是因为真的最喜欢这个商品，而仅仅只是因为其排序的位置比较靠前；UserBias，不同用户的点击偏好不同，有些用户就是具有较高的点击率，而另一些用户本身就偏爱浏览而不爱点击。模型分为Mainnet 和 Biasnet, Mainnet 输入全部特征用于建模用户的隐士反馈，Biasnet输入用户信息以及 position 信息用于捕捉隐式反馈中的Bias。最终两个网络的输出相加，通过Sigmoid 函数获得<span class="math inline">\(p(Pay_{all} =1|Drr =1)\)</span></p><h4 id="损失函数">损失函数</h4><p>通过MLPLayer之后，最终的损失函数如下，训练数据集为上一阶段的正样本和随机负采样的样本集合，其中全网成交的样本标签为1，随机负采样的样本标签为0。<span class="math display">\[Loss_{Pay} = -\frac{1}{N}\sum_{(x,y)\in D}^{N}(y\log p(x) +(1-y)\log(1-p(x)))\\\]</span> <span class="math display">\[y =\begin{cases}&amp;1,\ 如果x是全网成交 \\&amp;0,\ 如果x不是全网成交\\\end{cases}\]</span></p><p>建模第二部分的损失函数同上，只是label和训练数据集不同。</p><h1 id="ctnet">CTNet</h1><p><a href="https://arxiv.org/pdf/2208.05728.pdf">《Continual TransferLearning for Cross-Domain Click-Through Rate Prediction atTaobao》</a></p><h2 id="方法-1">方法</h2><p>分析现有的学术界和工业界的跨域推荐(Cross-DomainRecommendation,CDR）的相关工作，主要可分为两大类：联合训练 (JointLearning）和预训练-微调 (Pre-training &amp;Fine-tuning)。其中，联合训练方法同时优化源域(Source Domain）和目标域(TargetDomain）模型。然而，这一类方法需要在训练中引入源域的数据，而源域数据通常规模很大，从而消耗巨大的计算和存储资源，很多较小业务难以承担如此大的资源开销。另一方面，这一类方法需同时优化多个场景目标，场景之问的差异也可能带来目标冲突的负面影响，因此预训练-微调类方法在工业界很多场景有着更广泛的应用。工业界推荐系统一个重要的特点是模型训练遵循连续学习 (ContinualLearning）范式，即模型需要使用最新的用户反馈数据，利用离线增量更新(incremental Learning)或在线学习 (OnlineLearning）等方式学习最新的用户兴趣分布。对于本文研究的跨域推荐任务而言，源域和目标域的模型都是遵循连续学习的训练方式。我们由此提出了一个学术和工业上将有广泛应用的新问题：连续迁移学习(Continual TransferLearning），定义为从一个随时间变化的领域到另一个也随时间变化的领域的知识迁移。我们认为，现有的跨域推荐和迁移学习方法在工业推荐系统、搜索引擎、计算广告等的应用应当遵循连续迁移学习范式，即迁移的过程应当是持续的、多次的。原因在于用户的兴趣分布变化较快，只有通过连续的迁移才可以保证稳定的迁移效果。结合这一工业推荐系统的特点，我们可以发现预训练-微调在实际应用上的问题。由于源域和目标域的场景差异，通常需要用非常大规模的数据（例如数百亿样本）才可以利用源域模型调到一个效果较好的结果。而为了实现连续迁移学习，我们需要每隔一段时间都利用最新的源域模型重新微调，造成了非常巨大的训练代价，这样的训练方式也是难以上线的。此外，利用这些大规模的数据微调也可能使得源域模型遗忘掉保留的有用知识；利用源域模型参数去替换掉原有的目标域已经学好的参数也丢弃了原有模型历史上获得的有用知识。因此，我们需要设计一个更加高效，适用于工业推荐场景的连续迁移学习模型。本文提出了一个简单有效的模型CTNet (Continual TransferNetwork，连续迁移网络）解決了上述问题。不同手传统的预训练-微调类方法，CTNet的核心思想是不能选忘和丢弃所有模型在历史上获取的知识，保留了原有的源域模型和目标域模型的所有参数。这些参数中保存了通过非常久的历史数据学习得到的知识（例如淘宝有好货的精排模型已经连续增量训练两年以上）。CTNet采用了简单的双塔结构，利用了一个轻量级的Adapter层将连续预训练(Continually Pre-trained)的源域模型中间层表示结果映射并作为目标域模型的额外知识。不同于预训练-微调类方法需回溯数据以实现连续迁移学习，CTNet只需要增量数据进行更新，从而实现了高效的连续迁移学习。</p><h3 id="问题定义">问题定义</h3><p>本文探讨的是连续迁移学习这一新问题：给定随时间变化的源域和目标域，连续迁移学习（ContinualTransferLearning）希望能够利用历史或者当前获得的源域和目标域知识提升在未来目标域上的预测准确率。本文提出的方法的应用场景具有以下特点：</p><ul><li>不同的推荐场景规模相差较大，可以利用较大规模数据训练得到的源域模型的知识提升目标域的推荐效果</li><li>不同场景的用户和商品共享同一个大底池。但不同场景由于精选商品池、核心用户、图文等展示效果不同存在较为明显的领域差异。</li><li>所有推荐场景的模型都是基于最新收集是数据持续增量训练的。</li></ul><p><img src="/2023/05/25/0525/image-20230525211512666.png"></p><p>上图展示了我们的方法部署上线的情景，在<span class="math inline">\(t\)</span>时刻之前源域模型和目标域模型都是只利用各自场景的监督数据单独连续增量训练的。从<span class="math inline">\(t\)</span>时刻开始，我们在目标域上部署了跨域推荐模型CTNet，该模型将在不能還忘历史上获取的知识的情况，继续在目标域数据上持续增量训练，同时连续不断地从最新的源域模型中迁移知识。</p><h2 id="模型-1">模型</h2><p><img src="/2023/05/25/0525/image-20230525211739407.png"></p><p>我们在原有的目标域的精排模型中嵌入了源域模型的全部特征及其网络参数，形成一个双塔的结构，其中CTNet的左塔为源塔(Source Tower），右塔为目标塔 (TargetTower)。不同于常见的只利用源域模型打分分数或只利用一些浅层表示（如Embedding)的方法，我们通过Adapter网络，将源域模型MLP的所有中问隐藏层(特别是源域MLP深层蕴含的user和item的高阶特征交互信息）的表示结果z°映射到目标推荐域，并将结果加入到TargetTower的对应层z中(如下面公式所示）。CTNet效果提升的关键就是利用了MLP中深层表征信息的迁移。借鉴GatedLinear Units(GLU)的思想，Adapter网络，采用了门控的线性层，可以有效实现对源域特征的自适应特征选择，模型里有用的知识会做迁移，而与场景特点不符的无用的信息就丢奔掉。由于源域模型持续不断的使用最新的源域监督数据进行连续预训练，在我们的训练过程中，SourceTower也将持续不断的加载最新更新的源域模型参数并在反向传播过程中保持固定，保证了连续迁移学习的高效进行。模型适用于连续学习范式，使得目标域模型持续的学习到源域模型提供的最新知识，以适应最新的用户兴趣变化。同时由于模型仅在目标域数据上进行训练，保证了模型不受源域训练目标的影响，同时完全不需要源域数据训练，避免了大量的存储和计算开销。此外，这样的网络结构借鉴了无损添加新特征的设计方法，实现了模型的热启动，TargetTower完全由原有的目标域线上模型初始化，Adapter的初始参数较小，可以在最大程度上保证原有模型的效果不受损害，仅需较少增量数据就可得到很好的效果。</p><figure><img src="/2023/05/25/0525/image-20230525212156399.png" alt="image-20230525212156399"><figcaption aria-hidden="true">image-20230525212156399</figcaption></figure><h1 id="ugic">UGIC</h1><p><a href="https://arxiv.org/pdf/2208.10174.pdf">《KEEP: An IndustrialPre-Training Framework for Online Recommendation via KnowledgeExtraction and Plugging》</a></p><h2 id="方法-2">方法</h2><h3 id="预训练">预训练</h3><p>在CV和NLP领域，预训练方法已经被广泛应用于各种主流任务中并取得了很好的效果，尤其在BERT横空出世后，大规模无监督预训练语言模型横扫了NLP各种任务。受BERT影响，CV领域的预训练工作也从有监督预训练开始向无监督预训练转向，MOCO、Simsiam、MAE等一系列优秀的无监督预训练工作也使得视觉领域无监督预训练模型在下游任务的效果可以媲美甚至超过有监督预训练模型。推荐领域的预训练模型也已经被广泛研究，如BERT4ReC,，S3Rec等。这些工作基本沿着预训练语言模型的思路将MLM任务迁移到了用户行为序列上进行预训练，并针对推荐系统的特点对预训练任务进行了一定的微调。相比BERT4Rec这种无监督预训练方法，我们在实践中选择了一种更为简单的有监督预训练方案，这种选择主要基于以下两点考虑：1、用户在手淘app上的浏览点击行为天然的为我们提供了训练的标签，每天用户在猜你喜欢推荐页大概会产生接近100亿次的曝光行为，常年累月的数据积累使得我们可以轻松的获得海量的有标签数据进行预训练。2、仅在用户行为序列进行无监督预训练本质上还是仅利用了用户的正反馈行为进行表征学习，无法有效利用用户的曝光数据。基于以上考虑我们收集了用户在首猜场景的曝光/点击，点击/加购和点击/购买日志作为我们的预训练样本。其中曝光/点击样本占比超过了95%是预训练样本的主体，点击/加购和点击/购买样本的加入是希望我们的预训练模型可以学习到多种用户兴趣以服务多个下游模型。在用户行为日志的基础上我们构建了曝光/点击(clk)、点击/转化(CV)、点击/加购(cart)三种预训练预估任务，其中clk预估任务直接采用了常用的pointwise的交叉熵损失，cv和cart预估任务除了pointwiseloss外还额外引 了pairwiseloss来缓解任务的稀疏性，最后三个预估任务以multi-task的方式结合在一起，网络结构示意图如下</p><p><img src="/2023/05/25/0525/image-20230525213056789.png"></p><p>同时在实践中我们发现，预训练的样本量对最后的效果有着至关重要的影响，预训练样本量从1个月提高到6个月再到年，在预训练任务上的testauc指标分别会有1个百分点以上的提升，在下游任务的主模型中离线gauc也会从无任何提升到点几个千分点的提升再到4点几个千分点的提升，具体数据见在离线效果部分，因此我们最终上线的方案采用了用户过去两年在首猜上的行为日志作为我们的预训练数据。</p><h3 id="特征选择模型结构">特征选择&amp;模型结构</h3><p>如前文所述，用户每天在首猜场景会产生约100亿的行为数据，而我们的预训练任务的目标是要利用用户两年时间的累积数据。如此庞大的预训练数据量给我们的预训练任务带来了非常大的挑战，为了能够尽量加快模型训练速度，我们一方面尽量精简了模型的输入特征，另一方面也采用了尽量简单的模型结构。具体而言，特征选择方面我们仅选择了主模型的极小一部分特征子集来构建我们的预训练模型，item侧特征我们仅保留了item_id、cate_ idshop_id几个最重要的id特征，用户行为特征我们也仅保留了一条用户行为序列，整体特征量约是主模型的1%。除了主模型特征子集，我们也在预训练任务中加入了更为稀疏的useric特征。user_id的全特征空间约有10亿+，在我们的主模型中直接加入这一特征会面临严重的过拟合问题(该现象我们目前仅有一些不成熟的猜想，还没有定论，也欢迎大家一起讨论)，但是在大了两个数量级的预训练数据上，我们相信可以对更加稀疏的user_id充分的训练，同时user_id庞大的参数空间可以给我们的预训练模型提供更好的记忆性，从而得到更好的表征用于下游任务。模型结构部分我们选择采用了最简单的DNN结构，同时仅在用户行为序列和targetitem之间加入了attention模块，去掉了计算更加耗时的GRU等模块。通过特征逆择和模型结构的精简，以及batch_size的调整，与主模型相比，预训练任务的训练加速比提升了约100倍，除此之外，我们还对负样本进行了随机负采样，仅保留1/5的负样本来进一步加速训练，这些操作使得我们有能力在大约1周左右的时问里完成2年数据的预训练。预训练任务的模型结构和特征精简本质上是在预训练模型能力和训练样本量之问进行tradeoff，目前我们的经验来看预训练数据量的影响远大于精细的预训练模型结构的影响。</p><h3 id="预训练知识表征">预训练知识表征</h3><p>我们认为预训练好的预训练模型参数中encode了从预训练数据中抽取到的知识，因此我们用预训练模型一些层的输出作为预训练的知识表征。我们定义的预训练知识表征可以分为3个方面user-level,item-level,user-item-iteration-level。其中user-level的知识表征用user_id的embedding来表示，记为<span class="math inline">\(\mathscr{K}_u\)</span>，item-level的知识表征用item特征的embedding来表示，记为<span class="math inline">\(\mathscr{K}_i\)</span>，user-item-interaction-level的知识表征用fc的倒数第二层输出表表示，记为<span class="math inline">\(\mathscr{K}_{ui}\)</span>，同时由于存在3个预训练任务，因此<span class="math inline">\(\mathscr{K}_{ui}\)</span>会有三种不同的表现形式分别记为<span class="math inline">\(\mathscr{K}_{ui}^{clk}\)</span>,<span class="math inline">\(\mathscr{K}_{ui}^{cv}\)</span>,<span class="math inline">\(\mathscr{K}_{ui}^{cart}\)</span>。最终我们将所有抽取到的知识concat到一起供给下游模型。<span class="math display">\[\mathscr{K}(u,i) =[\mathscr{K}_u,\mathscr{K}_i,\mathscr{K}_{ui}^{clk},\mathscr{K}_{ui}^{cv},\mathscr{K}_{ui}^{cart}]\]</span></p><h3 id="预训练知识表征与主模型融合">预训练知识表征与主模型融合</h3><p>在利用预训练模型抽取到合适的知识表征之后，接下来就是要把抽取到的预训练知识表征合理的应用到下游任务中。传统的预训练模型在应用中通常是采用pre-training&amp;fine-tuning机制，即在下游任务中，加载pre-training参数作为模型参数的初始化，然后在下游任务的训练样本上进行finetune。然而这种pre-training &amp;fine-tuning机制在实际的工业级推荐系统中并不完全适用。一方面，线上服务的模型常采用参数增量更新的ODL模式，即每次模型训练都会完全加载之前一个版本的模型参数，然后利用此后一段时间的训练数据进行参数更新，如此循环下去如下图所示。在我们的线上系统中，线上服务的模型己经滚动更新了超过1年以上的时间。在此模式下如果仅将预训练模型参数作为模型初始化，那在后续的增量训练过程中可能会遇到灾难遗忘问题(catastrophicforgetting)，实践中我们也确实发现仅加载预训练参数作为下游任务模型初始化参数虽然能够有效的加快模型收敛速度，但是随着训练过程的进行，效果gap会逐渐缩小至完全无提升。另一方面，预训练模型同样也需要在用户行为日志上持续进行增量训练以捕捉用户最新兴趣，pre-training&amp; fine-tuning机制也难以满足预训练模型的参数更新需求。</p><p><img src="/2023/05/25/0525/image-20230525214122824.png"></p><p>针对以上问题，我们采用了另一种预训练知识表征的融入方式，将预训练知识表征作为下游任务模型的额外输入，帮助下游任务更好的进行预估。同时为了够更好的保留主模型的网络结构，我们设计了一个KnowledgePlug-inNetwork结构，用加法操作代替了传统新加特征的concat操作。这种结构的好处一方面是得主模型可以加载之前版本的模型参数，避免了从随机初始化重新训练，实现特征“热后动”的功能。另一方面在预训练知识表征发生更新时（如新增了一组知识表征时）也可以通过finetune较少的模型参数快速上线，给预训练知识表征保留了一定的拓展空间</p><p><img src="/2023/05/25/0525/image-20230525214546148.png"></p><h2 id="在线服务">在线服务</h2><p>到目前为止我们所尝试的算法方案都非常的简单，对于各种问题基本上就是直觉上的解法，并没有花里胡哨的模型结构设计和算法堆砌。然而预训练模型在工业级推荐系统的应用其实不仅是一个算法问题，一个通用的大规模预训练模型如果想在工业级推荐系统上大规模上线必须要解决好预训练模型的在线服务问题，即如何在满足线上存储和r约束的条件下，高效的服务好多个下游模型，并且不会影响下游模型的迭代节奏。接下来，我们将详细介绍我们将预训练模型在定向广告业务上进行线上服务的经验。</p><h3 id="分解和退化策略">分解和退化策略</h3><p>为了不给下游任务增加额外的算力负担，我们采用的是缓存策略将预训练模型提取到的知识表征cache在高性能参数服务器中，这样下游任务可以直接请求对应的预训综知识表征<span class="math inline">\(\mathscr{K}(u,i)\)</span>而无需额外的实时计算。然而直接缓存<span class="math inline">\(\mathscr{K}(u,i)\)</span>需要遍历所有的(u,i)pair，对于手淘约10<sup>9量级的用户空间以及10</sup>9量级的商品空问，组合起来的大(4，2)数量将达到恐怖的10^18，如此大的量级肯定无法直接缓存。对于这一问题我们设计了预训练知识表征的分解和退化策略。</p><p>分解策略是指将预训练模型结构分解成双塔结构，从user塔和item塔分别得到<span class="math inline">\(\mathscr{K}_u,\mathscr{K}_i\)</span>然后用二者的乘积来表征<span class="math inline">\(\mathscr{K}_{ui}\)</span>。然而这种简单的双塔模型结构无法有效的捕捉<span class="math inline">\(\mathscr{K}_u,\mathscr{K}_i\)</span>之间的高阶交叉关系，因此在分解策略的基础上，我们又增加了退化策略。退化策略是指将模型的打分粒度人item粒度退化至更粗的cate粒度，因为cate的空间远小于item，使得我们可以直接遍历(u，c)pair进行缓存。 <span class="math display">\[\mathscr{K}(u,i) \rightarrow \mathscr{K}(u,i,c) =[\mathscr{K}_u,\mathscr{K}_i,\mathscr{K}_u \times \mathscr{K}_i,\mathscr{K}_{uc}]\]</span>分解策略和退化策略的组合使得我们可以同时拥有细粒度的打分能力和复杂模型的特征交叉能力，并且预训练知识表征的缓存空间大小也从<span class="math inline">\(N_u*N_i\)</span>。减少到<span class="math inline">\(N _u+ N_i+ N_u*N_c\)</span>。</p><figure><img src="/2023/05/25/0525/image-20230525215321332.png" alt="image-20230525215321332"><figcaption aria-hidden="true">image-20230525215321332</figcaption></figure><h3 id="ugic服务">UGIC服务</h3><p>定向广告业务繁多，旦每个业务场景下会包括粗排/精排两个阶段，每个阶段下又会有ctrlcvr/cart等预估模型，组合起来目前线上几乎有40+的模型在同时服务主流量，于此同时还有相当数量的实验模型在小流量服务。经过大规模预训练的知识表征可以认为编码了用户长期且稳定的兴趣偏好，实验证明预训练表征至个场景多个阶段多个任务的下游模型均能带来一定的提升。为了能够方便的服务多个下游模型，我们将预训练知识表征服务从RTP中抽离出来，构建了UGIC(UserGeneral nterestCenter)服务。具体而言，经过分解和退化策略，使得我们有能力将预训练知识表征在UGIC进行缓存，服务过程中，UGIC接收一个(u，i,c）的pair，并返口对应的知识表征下<span class="math inline">\(\mathscr{K}(u,i,c)\)</span>。同时考虑到预训练模型与下游模型的更新己经解耜，不同的下游任务可能会对应不同的预训练模型版本，为了保证在离线一致性，我们在UGIC中也增加了多版本的服务能力，下游任务可以根据自己特定的版本号v访问相应的预训练知识表征，具体过程如下图所示。</p><p><img src="/2023/05/25/0525/image-20230525215520586.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;推荐系统是在线服务的重要组成部分，特别是对于电子商务平台而言。在推荐系统中，转化率（CVR）预测对于优化电子商务的总交易额目标至关重要。然而，CVR存在众所周知的样本选择偏差（SSB）和数据稀疏性（DS）问题。虽然现有的方法ESMM和ESM2通过建模用户行为路径来训练所有展</summary>
      
    
    
    
    <category term="paper Reading" scheme="http://kerwinblog.top/categories/paper-Reading/"/>
    
    
    <category term="RS" scheme="http://kerwinblog.top/tags/RS/"/>
    
  </entry>
  
  <entry>
    <title>Optimizing Feature Set for Click-Through Rate Prediction</title>
    <link href="http://kerwinblog.top/2023/05/20/0520/"/>
    <id>http://kerwinblog.top/2023/05/20/0520/</id>
    <published>2023-05-20T11:01:41.000Z</published>
    <updated>2023-05-25T11:23:51.529Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景">背景</h1><p>点击率预测在真实世界的商业推荐系统和在线广告系统中一直是一项至关重要的任务。它的目的是预测某个用户点击推荐项物品(如电影、广告)的概率。CTR预测的标准输入主要由一组组织为特征字段的分类特征组成。例如，在CTR预测中，每个样本包含一个特征字段性别，字段性别可能包括男性、女性和未知三个特征值。<a href="https://arxiv.org/pdf/2301.10909.pdf">原文链接</a></p><p>点击率预测（CTR）模型将特征转换为潜在向量，并枚举可能的特征交互作用以改进性能，基于输入特征集。因此，在选择最佳特征集时，我们应考虑特征和它们之间交互的影响。但是，大多数先前的工作都集中在特征字段选择或仅根据固定特征集选择特征交互方面进行研究。前者将搜索空间限制在特征字段上，这太粗略了，无法确定微妙的特征。他们也没有过滤无用的特征交互，导致计算成本更高，模型性能降低。后者从所有可用特征中识别有用的特征交互，导致特征集中存在许多冗余特征。在这篇论文中，我们提出了一种名为OptFS的新方法来解决这些问题。为了统一特征和它们之间的选择，我们将每个特征交互的选择分解为两个相关特征的选择。这种分解使得模型可通过各种特征交互操作进行端到端训练。通过采用特征级别的搜索空间，我们设置一个可学习门，以确定每个特征是否应包含在特征集中。由于搜索空间很大，我们开发了一种学习-继续训练方案来学习这些门。因此，OptFS生成特征集，其中包含可提高最终预测结果的特征。在实验上，我们在三个公共数据集上评估OptFS，并证明它可以优化特征集，从而提高模型性能并进一步降低存储和计算成本。</p><h1 id="引言">引言</h1><p>点击率预测一直是现实世界中商业推荐系统和在线广告系统中的关键任务。它旨在预测某个特定用户点击推荐项目（例如电影、广告）的概率。CTR预测的标准输入主要由大量分类特征组成，以特征字段的形式组织。例如，在CTR预测中，每个样本都包含一个性别特征字段，该字段可能包含三个特征值：男、女和未知。为避免歧义，我们将特征值以下简称为特征。</p><p>通常CTR预测模型首先通过嵌入表将特征集中的每个特征映射到唯一的实值密集向量上。然后，这些向量被馈送到特征交互层，通过枚举特征集明确建模低阶特征交互来提高预测效果。分类器的最终预测基于特征嵌入和特征交互，这两者都受输入特征集的重要影响。通用框架如图1所示。因此，输入特征集在CTR预测中扮演着重要角色。</p><p><img src="/2023/05/20/0520/image-20230520151935935.png" alt="image-20230520151935935" style="zoom:80%;"></p><p>盲目地将所有可用特征输入到特征集中既不是有效的，也不是高效的。从有效性的角度来看，某些特征可能对模型性能有害。首先，这些特征本身可能只引入可学习参数，使预测模型容易出现过拟合。其次，某些无用的交互由这些特征引入，还会带来不必要的噪声并使训练过程变得复杂，这会降低最终的预测结果。请注意，在选择特征集时，这两个因素密切相关。如果将一个特征x𝑖从集合中过滤出去，则相关的所有交互也应在模型中排除。相应地，有信息量的交互<span class="math inline">\(&lt;x_i,x_j&gt;\)</span>是保留x𝑖在特征集中的强有力指标。从效率的角度来看，将冗余特征引入特征集可能在存储空间和计算成本方面效率低下。由于嵌入表在CTR模型中占主导地位，没有冗余特征的特征集将大大减少模型的大小。此外，具有有用特征的特征集可以使许多无用特征交互的计算为零，从而在实践中大大减少计算成本。一个最佳的特征集应该同时考虑有效性和效率</p><p>本文提出了一种名为OptFS的方法，用于解决搜索最佳特征集的问题。我们的OptFS面临两个主要挑战。第一个挑战是如何联合选择特征及其交互，给定各种特征交互操作。如上所述，最佳特征集应该排除在模型中引入无用交互的特征。我们通过将每个特征交互的选择分解为两个相关特征的选择来解决这个挑战。因此，OptFS减少了特征交互的搜索空间，并通过各种特征交互操作对模型进行端到端训练。第二个挑战是大规模数据集中的特征数。请注意，我们研究考虑的可能特征数可以达到10^6，这比之前的工作中使用的100个特征字段要大得多。为了在庞大的搜索空间中进行导航，我们为每个特征引入一个可学习门，并采用学习-继续训练(learning-by-continuation)的训练方案。我们的主要贡献总结如下：</p><ul><li>本文首先区分了最佳特征集问题，它聚焦于特征级别，并考虑了特征和特征交互的有效性，提高模型性能和计算效率。</li><li>我们提出了一种名为OptFS的新方法来优化特征集。OptFS采用高效的学习-继续训练方案，以端到端方式训练预测模型和特征交互操作。</li><li>我们在三个大规模公共数据集上进行了广泛的实验。实验结果表明，所提出的方法具有高效性和有效性。</li></ul><h1 id="方法">方法</h1><h2 id="problem-formulation">Problem Formulation</h2><p>在本小节中，我们提供了特征集优化问题的公式化表述。在CTR模型中，通常认为对准确预测有用的特征是有用的。在我们的设置中，将所有可能的特征表示为<span class="math inline">\(X = {x_1, x_2, · · · , x_𝑚}\)</span>。<span class="math inline">\(x_i\)</span>是一种one-hot表示，非常稀疏和高维的。正如先前讨论的，特征集优化问题旨在从所有可能的特征中确定有用的特征，这可以定义为找到最佳特征集<span class="math inline">\(X^g ⊂ X\)</span>。这可以表示如下：</p><figure><img src="/2023/05/20/0520/image-20230520153440234.png" alt="image-20230520153440234"><figcaption aria-hidden="true">image-20230520153440234</figcaption></figure><h2 id="feature-selection">Feature Selection</h2><p>首先定义<span class="math inline">\(z_t\)</span>为特征字段，可以看做是某一组特征的集合。<span class="math display">\[z_i = {x_{k_i}},\ 1\leq k_i \leq m\]</span>这表明字段和特征之间的关系是一对多映射。在实际中，字段数𝑛远小于特征数𝑚。例如，在线广告系统通常有<span class="math inline">\(𝑛≤100\)</span>且<span class="math inline">\(𝑚 ≈10^6\)</span>。因此，从特征和字段的角度来看，CTR模型的输入可以重写为：<span class="math display">\[\text{z}=[z_1,z_2,...z_n] = [x_{k_1},x_{k_2}...x_{k_n}]\]</span> 其中第二个等号表示对于输入z，字段<span class="math inline">\(z_𝑖\)</span>的相应特征为<span class="math inline">\(x_{𝑘_𝑖}\)</span>，如公式所示。通常，我们使用嵌入表将<span class="math inline">\(z_𝑖\)</span>转换为低维和密集的实值向量。这可以表示为<span class="math inline">\(e_𝑖 = E × z_𝑖 = E ×x_{𝑘_𝑖}，1≤𝑖≤𝑛，1≤𝑘_𝑖≤𝑚\)</span>，其中<span class="math inline">\(E∈R^{𝑚×D}\)</span>是嵌入表，𝑚是特征值的数量，𝐷是嵌入的大小。然后将嵌入堆叠在一起形成一个嵌入向量<span class="math inline">\(\text{e}=[e_1,e_2,⋯,e_𝑛]\)</span>。在我们的工作中，我们提出特征级别选择。我们不是进行字段级别的选择，而是将选择定义为为每个特征嵌入<span class="math inline">\(e_{𝑘_𝑖}\)</span>分配一个二进制门<span class="math inline">\(g_{𝑘_𝑖}∈\{0,1\}\)</span>。选择后，可以将特征嵌入表示为：<span class="math display">\[e^g_{k_i} = g_{k_i} \odot e_{k_i} = g_{k_i} \odot(E\times x_{k_i})\]</span> 当<span class="math inline">\(g_{𝑘_𝑖} =1\)</span>时，代表该特征<span class="math inline">\(x_{𝑘_𝑖}\)</span>在最佳特征集<span class="math inline">\(X^g\)</span>中，反之则不在。请注意，先前的工作分配字段级别的特征选择。这意味着对于每个字段<span class="math inline">\(𝑧_𝑖，g_{𝑘_𝑖}≡g_𝑖∈{0,1}\)</span>，表示保留或丢弃相应字段中所有可能的特征{<span class="math inline">\(x_{𝑘_𝑖}\)</span>}。然后，将这些嵌入堆叠在一起形成一个特征选择的嵌入向量<span class="math inline">\(e^𝑔=[e^g_{𝑘_1},e^g_{𝑘_2},⋯,e^g_{𝑘_𝑛}]\)</span>。最终的预测可以表示为<span class="math display">\[\hat{y} = \mathscr{F}(g \odot E \times \text{x|W}) = \mathscr{F} (E^g\times \text{x|W})\]</span> 上式中，<span class="math inline">\(g \in\{0,1\}^m\)</span>是一个门向量，代表最后是否选择该特征，<span class="math inline">\(E^g = g \odot \timesE\)</span>表示选择后的特征经过embedding后的表达。</p><h2 id="feature-interaction-selection">Feature InteractionSelection</h2><p>特征交互选择旨在选择有益的特征交互以进行显式建模。主流CTR模型中的特征交互层将基于e执行。先前的研究中存在几种类型的特征交互，例如内积。之后通过某种方式将特征从嵌入空间到特征交互空间进行转换。最后进行预测，主流模型中转换函数G(·)、特征交互方式O(·)和预测函数H(·)的组合如表1所示。</p><p><img src="/2023/05/20/0520/image-20230520163101149.png"></p><p>在现实中，探索所有可能的特征交互的一种直接方法是引入一个特征交互矩阵<span class="math inline">\({g&#39;_{(𝑘_𝑖,𝑘_𝑗)}}\)</span>来表示2阶特征交互<span class="math inline">\(\{x_{𝑘_𝑖},x_{𝑘_𝑗}\}\)</span>。但是这是不可能的，因为我们将会有<span class="math inline">\(C^2_m ≈10^{12}\)</span>个门变量。为了高效地缩小如此庞大的空间，先前的工作将搜索空间限制在特征字段交互上，将变量数量减少到<span class="math inline">\(C^2_n ≈ 1000\)</span>。可以将其表述为<span class="math inline">\(g&#39;(𝑖,𝑗)≡g&#39;(𝑘_𝑖,𝑘_𝑗)\)</span>。然而，这种松弛可能无法区分同一字段内有用和无用特征交互之间的差异。因为已经证明，特征之间的信息交互往往来自于信息低阶交互，所以我们将特征交互分解如下:<span class="math display">\[{g&#39;_{(𝑘_𝑖, 𝑘_𝑗)}} = g_{k_i} \times g_{k_j}\]</span>这表明，仅当两个特征都是有用的时，特征交互才被认为是有用的。如图2所示，对分解进行了说明。因此，最终预测可以表示为：<span class="math display">\[\hat{y} =H((g\times g \odot V) \oplus G(g \odot e))\]</span>这意味着，选择特征的门控向量g也可以根据O(·)选择特征交互。这种设计可以减少搜索空间，并以端到端的方式获取最佳特征集。</p><p><img src="/2023/05/20/0520/image-20230520164433063.png"></p><h2 id="learning-by-continuation">Learning by Continuation</h2><p>尽管在第上节中，搜索空间已经从<span class="math inline">\(C^2_m +m\)</span>缩小到了<span class="math inline">\(m\)</span>,但我们仍需要确定是否保留特征集中的每个特征。这可以表示为一个l0规范化问题。然而，二元门控向量𝑚很难计算有效梯度。此外，l0优化被认为是一个NP难问题。为了高效地训练整个模型，我们引入了一个连续学习训练方案。这种训练方案已被证明是逼近<span class="math inline">\(L_0\)</span>规范化的有效方法，与我们的目标相关。learning-by-continuation方案由两部分组成：确定门控向量g的搜索阶段和确定嵌入表<span class="math inline">\(e\)</span>和其他参数<span class="math inline">\(W\)</span>的倒带阶段。我们将分别在以下几节中介绍它们。</p><h3 id="searching">Searching</h3><p>为了高效地优化具有特征级别粒度的特征集，我们引入一个连续门<span class="math inline">\(g_𝑐∈R^𝑚\)</span>。在搜索阶段，我们引入一个指数增加的温度值<span class="math inline">\(𝜏\)</span>来逼近<span class="math inline">\(L_0\)</span>规范化。具体而言，实际门控g计算如下：<span class="math display">\[g = \frac{\sigma(g_c \times \tau)}{\sigma(g_c^{0})},\ \tau=\gamma^{t/T}\]</span> 其中，<span class="math inline">\(g^{(0)}_c\)</span>是连续门控向量<span class="math inline">\(g_c\)</span>的初始值， <span class="math inline">\(σ\)</span>是作用在每个元素上的sigmoid函数<span class="math inline">\(σ(x)=1/(1+e^{-x})\)</span>，t是当前的训练epoch数，T是总的训练epoch数，<span class="math inline">\(γ\)</span>是训练T epoch后<span class="math inline">\(𝜏\)</span>的最终值。这样可以使得连续门控向量 <span class="math inline">\(g_c\)</span>在早期阶段就可以收到有效的梯度，随着epoch数t的增加而越来越逼近二元门控向量。上式的说明如图3(a)所示。</p><p>最终的损失函数采用的是改进的交叉熵： <span class="math display">\[\text{CE}_{(y,\hat{y})} = ylog(\hat{y})+(1-y)log(1-\hat{y})\]</span> 改进版如下： <span class="math display">\[\mathscr{L}_{CE} (D|\{E,W\}) = - \frac{1}{|D|} \sum_{(x,y)\in D}CE(y,\mathscr{F}(E \times x|W))\]</span>其中D是训练数据集，W是除了嵌入表E以外的网络参数。因此，最终的训练目标变成了：<span class="math display">\[\min_{g_c,E,W} \mathscr{L}_{CE}(D|{g_c \odot E,W}) + \lambda ||g||_1\]</span> 其中,<span class="math inline">\(\lambda\)</span>时正则惩罚项，<span class="math inline">\(∥·∥_1\)</span>表示l1范数以鼓励稀疏性。这里我们重新将l0范数表示为l1范数，因为对于二元门控向量g而言<span class="math inline">\(∥g∥_0=∥g∥_1\)</span>。</p><p>在训练T个epoch后，最终的门控向量g通过一个单位阶跃函数计算如下： <span class="math display">\[g_{i}=\left\{\begin{array}{ll}1 &amp; \text { if } g_{i}&gt;0 \\0 &amp; \text { otherwise }\end{array}\right.\]</span> <img src="/2023/05/20/0520/image-20230520173237487.png"></p><h3 id="retraining">Retraining</h3><p>在搜索阶段，所有可能的特征都被输入到模型中来探索最佳特征集<span class="math inline">\(X^g\)</span>。因此，无用的特征可能会损害模型的性能。为了解决这个问题，在获得最佳特征集<span class="math inline">\(X^g\)</span>之后，我们需要重新训练模型。确定门控向量g之后，我们将模型参数<span class="math inline">\(E\)</span>和<span class="math inline">\(W\)</span>重新训练为<span class="math inline">\(𝑇_𝑐\)</span>epoch时相应的值，该值在我们的设置中进行了精心调整。这是因为大多数CTR模型在几个epoch后就早停了，使得它们对初始化更加敏感，并容易过拟合。最终的参数E和W的训练如下：<span class="math display">\[\min_{E,W} \mathscr{L}_{CE}(D|{g_c \odot E,W})\]</span> 整体的算法流程如下图：</p><p><img src="/2023/05/20/0520/image-20230520173713025.png"></p><h1 id="结论">结论</h1><p><img src="/2023/05/20/0520/image-20230520173826139.png"></p><p>本文首先区分了特征集优化问题。这种问题统一了两个相互影响的问题：特征选择和特征交互。据我们所知，之前没有任何工作以统一的方式考虑这两个问题。此外，我们还将问题的粒度从字段级别升级到特征级别。为了高效地解决这个特征集优化问题，我们提出了一种名为OptFS的新方法，为每个特征分配一个门控值来确定其有用性，并采用连续学习方法进行高效优化。对三个大型数据集进行的广泛实验表明了OptFS在模型性能和特征减少方面的优越性。多项消融研究也说明了我们设计的必要性。此外，我们还解释了特征字段及其交互的结果，突显了我们的方法如何妥善解决特征集优化问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;背景&lt;/h1&gt;
&lt;p&gt;点击率预测在真实世界的商业推荐系统和在线广告系统中一直是一项至关重要的任务。它的目的是预测某个用户点击推荐项物品(如电影、广告)的概率。CTR预测的标准输入主要由一组组织为特征字段的分类特征组成。例如，在CTR预测中，每个样本包含一</summary>
      
    
    
    
    <category term="paper Reading" scheme="http://kerwinblog.top/categories/paper-Reading/"/>
    
    
    <category term="RS" scheme="http://kerwinblog.top/tags/RS/"/>
    
  </entry>
  
  <entry>
    <title>强化学习笔记5————AC与PPO</title>
    <link href="http://kerwinblog.top/2023/05/12/rl-05/"/>
    <id>http://kerwinblog.top/2023/05/12/rl-05/</id>
    <published>2023-05-12T13:00:18.000Z</published>
    <updated>2023-05-16T11:47:14.495Z</updated>
    
    <content type="html"><![CDATA[<p>关于AC，很多书籍和教程都说AC是DQN和PG的结合，虽然正确但从这个角度去看不便于理解，安装我们之前的思路，PG利用带权重的梯度下降方法更新策略，而获得权重的方法是蒙地卡罗计算G值。蒙地卡罗需要完成整个游戏过程，直到最终状态，才能通过回溯计算G值。这使得PG方法的效率被限制。那我们可不可以更快呢？相信大家已经想到了，那就是改为TD。</p><h1 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h1><h2 id="什么是AC"><a href="#什么是AC" class="headerlink" title="什么是AC"></a>什么是AC</h2><p>但改为TD还有一个问题需要解决，就是：在PG，我们需要计算G值；那么在TD中，我们应该怎样估算每一步的Q值呢？</p><p>熟悉的问题：其实这个问题我们在学DQN的时候也遇到过。 熟悉的套路：我们用上万能的神经网络解决。</p><p>也就是说，Actor-Critic，其实是用了两个网络：</p><p>两个网络有一个共同点，输入状态S: 一个输出策略，负责选择动作，我们把这个网络成为Actor； 一个负责计算每个动作的分数，我们把这个网络成为Critic。</p><h2 id="TD-error"><a href="#TD-error" class="headerlink" title="TD-error"></a>TD-error</h2><p>注意:这是AC的重点。很多同学在这里会和DQN搞乱，也就是容易产生误解的地方。在DQN预估的是Q值，在AC中的Critic，估算的是V值。你可能会说，为什么不是Q值呢？说好是给动作评价呢。我们可以直接用network估算的Q值作为更新值，但效果会不太好。原因我们可以看下图：</p><p><img src="/2023/05/12/rl-05/image-20230514105431293.png" alt="image-20230514105431293"></p><p>假设我们用Critic网络，预估到S状态下三个动作A1，A2，A3的Q值分别为1,2,10。但在开始的时候，我们采用平均策略，于是随机到A1。于是我们用策略梯度的带权重方法更新策略，这里的权重就是Q值。于是策略会更倾向于选择A1，意味着更大概率选择A1。结果A1的概率就持续升高…这就掉进了正数陷阱。我们明明希望A3能够获得更多的机会，最后却是A1获得最多的机会。</p><p>这是为什么呢？这是因为Q值用于是一个正数，如果权重是一个正数，那么我们相当于提高对应动作的选择的概率。权重越大，我们调整的幅度将会越大。其实当我们有足够的迭代次数，这个是不用担心这个问题的。因为总会有机会抽中到权重更大的动作，因为权重比较大，抽中一次就能提高很高的概率。但在强化学习中，往往没有足够的时间让我们去和环境互动。这就会出现由于运气不好，使得一个<strong>很好</strong>的动作没有被采样到的情况发生。要解决这个问题，我们可以通过减去一个baseline，令到权重有正有负。而通常这个baseline，我们选取的是权重的平均值。减去平均值之后，值就变成有正有负了。而Q值的期望(均值)就是V。</p><p><img src="/2023/05/12/rl-05/image-20230514105718124.png" alt="image-20230514105718124"></p><p>所以我们可以得到更新的权重$Q(s,a)-V(s)$随之而来的问题是，这就需要两个网络来估计Q和V了。但马尔科夫告诉我们，很多时候，V和Q是可以互相换算的。$Q(s,a)$用$gamma <em> V(s’) + r $来代替，于是整理后就可以得到：$gamma </em> V(s’) + r - V(s) $ 我们把这个差，叫做TD-error,这个和之前DQN的更新公式非常像，只不过DQN的更新用了Q，而TD-error用的是V。眼尖的同学可能已经发现，如果Critic是用来预估V值，而不是原来讨论的Q值。那么，这个TD-error是用来更新Critic的loss了！没错，Critic的任务就是让TD-error尽量小。然后TD-error给Actor做更新。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ol><li><p>定义两个network：Actor 和 Critic</p></li><li><p>j进行N次更新。</p></li><li><ol><li>从状态s开始，执行动作a，得到奖励r，进入状态s’</li><li>记录的数据。</li><li>把输入到Critic，根据公式： $TD<em>{error} = gamma * V(s’) + r - V(s)$ 求$TD</em>{error}$，并缩小$TD_{error}$</li><li>把输入到Actor，计算策略分布 。</li></ol></li></ol><p><img src="/2023/05/12/rl-05/image-20230514110611974.png" alt="image-20230514110611974" style="zoom: 67%;"></p><p><strong>更新流程</strong> 我们可以把更新流程和PG做对比： 在PG，智能体需要从头一直跑到尾，直到最终状态才开始进行学习。 在AC，智能体采用是每步更新的方式。但要注意，我们需要先更新Critic，并计算出TD-error。再用TD-error更新Actor。</p><p>但在实做得时候，很多时候我们会把Actor和Critic公用网络前面的一些层。例如state是一张图片，我们可以先通过几层的CNN进行特征的提取，再分别输出Actor的动作概率分布和Critic的V值。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li>为了避免正数陷阱，我们希望Actor的更新权重有正有负。因此，我们把Q值减去他们的均值V。有：$Q(s,a)-V(s)$</li><li>为了避免需要预估V值和Q值，我们希望把Q和V统一；由于$Q(s,a) = gamma <em> V(s’) + r - V(s)$。所以我们得到TD-error公式： $TD-error = gamma </em> V(s’) + r - V(s)$</li><li>TD-error就是Actor更新策略时候，带权重更新中的权重值；</li><li>现在Critic不再需要预估Q，而是预估V。而根据马可洛夫链所学，我们知道TD-error就是Critic网络需要的loss，也就是说，Critic函数需要最小化TD-error。</li></ol><h1 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h1><h2 id="on-policy与off-policy"><a href="#on-policy与off-policy" class="headerlink" title="on-policy与off-policy"></a>on-policy与off-policy</h2><p>邻近策略优化（Proximal Policy Optimization，PPO）算法的网络结构有两个。PPO算法解决的问题是 <strong>离散动作空间和连续动作空间</strong> 的强化学习问题，是 <strong>on-policy</strong> 的强化学习算法。论文原文见《<a href="https://arxiv.org/pdf/1707.06347.pdf">Proximal Policy Optimization Algorithms</a>》</p><p>关于on-policy和off-policy的定义，网上有很多不同的讨论，比较常见的说法是看behavior policy（行为策略，即与环境进行交互的策略）和target policy（目标策略，即学习准确地评估Q值的策略）是否为同一个，如果为同一个，那么就为on-policy，反之为off-policy。我认为， 更加通俗一点的理解是，<strong>on-policy和off-policy的差异</strong> 在于 <strong>训练目标策略 所用到的数据$ (s,a,r,s′)$（有时候也表现为数据 $(s,a,r,s′,a′)$ ）是不是当前目标策略（此时还没开始训练）得到的</strong> ，如果是目标策略得到的，那么就是on-policy，如果不是，那么就是off-policy。</p><p>这样说有点难以理解，我们举个例子：</p><p>如果我们在智能体和环境进行互动时产生的数据打上一个标记。标记这是第几版本的策略产生的数据,例如 1， 2… 10.现在我们的智能体用的策略 10，需要更新到 11。如果算法只能用 10版本的产生的数据来更新，那么这个就是在线策略；如果算法允许用其他版本的数据来更新，那么就是离线策略。所以，我们需要用到重要性更新的，就可以用上之前策略版本的数据了。</p><p>我们来看看之前学习到的算法都属于哪一种？</p><h3 id="SARSA算法"><a href="#SARSA算法" class="headerlink" title="SARSA算法"></a><strong>SARSA算法</strong></h3><p>首先回顾一下SARSA的更新公式：</p><script type="math/tex; mode=display">Q(S_t,A_t) \leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t})-Q(S_{t},A_{t})]</script><p>想一想，在SARSA选择当前状态的策略时是怎么选择的呢？没错，使用epsilon-greedy选择Q值最大的那一个。那再想一想，在更新当前行为状态的Q值时，我们需要用下一个状态的某一个Q值来更新，那选择哪一个Q值呢？使用相同策略下的动作的Q值！那么显然，在Sarsa中更新Q函数时用的A就是贪婪策略得出来的，下一回合也用的是这个A去进行step。两个A一定相同就是（同策略）on-policy。</p><h3 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q_learning"></a>Q_learning</h3><p>Q_learning的更新公式为：</p><p><img src="/2023/05/12/rl-05/image-20230514115222702.png" alt="image-20230514115222702"></p><p>同样的分析，在选择策略时同样是使用epsilon-greedy，但是在更新Q值的时候，这里选择的是Q值最大的那一个。这时两者可能不一样，就是（异策略）off-policy。</p><h2 id="Proximal-Policy-Optimization"><a href="#Proximal-Policy-Optimization" class="headerlink" title="Proximal Policy Optimization"></a>Proximal Policy Optimization</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>PPO算法的基础框架就是上面提到的AC算法，所以理所当然的ppo的网络结构也分为两部分：Actor网络和Critic网络。</p><p><img src="/2023/05/12/rl-05/image-20230514125114964.png" alt="image-20230514125114964" style="zoom:67%;"></p><p>一个actor网络，一个<a href="https://www.zhihu.com/search?q=critic网络&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;384497349&quot;}">critic网络</a>。</p><p>actor网络的输入为状态，输出为动作概率 $\pi(a_t|s_t)$ （对于离散动作空间而言）或者动作概率分布参数（对于连续动作空间而言）</p><p>critic网络的输入为状态，输出为状态的价值。</p><p>显然，如果actor网络输出的动作越能够使优势（优势的定义等下给出）变大，那么就越好。如果critic网络输出的状态价值越准确，那么就越好。</p><h3 id="产生连续输出"><a href="#产生连续输出" class="headerlink" title="产生连续输出"></a>产生连续输出</h3><p>首先，我们要想办法处理连续动作的输出问题。我们先说离散动作。离散动作就像一个个的按钮，按一个按钮就能智能体就做一个动作。就像在CartPole游戏里的智能体，只有0,1两个动作分别代表向左走，向右走。那什么是连续动作呢。这就相当于这些按钮不但有开关的概念，而且还有力度大小的概念。就像我们开车，不但是前进后退转弯，并且要控制油门踩多深，刹车踩多少的，转弯时候转向转多少的问题。于是问题来了，在离散动作空间的问题中，我们最终输出的策略呈现这样的形式。</p><p><img src="/2023/05/12/rl-05/image-20230516190405663.png" alt="image-20230516190405663"></p><p>假设动作空间有只有action1 和 action2，有40%的概率选择action1 ，60%概率选择action2。也就是说在这个状态下的策略分布: pi = [0.4, 0.6]。但连续型动作怎么表示呢，还记得之前学习DQN的时候，DQN的函数就像一张布覆盖到了Qtable上吗？其实连续型动作的理解也是一样的。我们可以理解，把连续型概率切成很多很多份。每一份的数值，就代表该动作的概率。</p><p>在连续型，我们不再用数组表示，而是用函数表示。例如，策略分布函数 ： P = （action）代表在策略 下，选择某个action的概率P。但这就有个问题，用神经网络预测输出的策略是一个固定的shape，而不是连续的。那又什么办法可以表示连续型的概率呢？我们先假定策略分布函数服从一个特殊的分布，这个特殊的分布可以用一两个参数表示它的图像。正态分布就是这样一个分布，他只需要两个参数，就可以表示了。</p><p><img src="/2023/05/12/rl-05/image-20230516190617708.png" alt="image-20230516190617708" style="zoom: 50%;"></p><p>正态分布长得就是这个形状，中间高，两边低。他的形状由两个参数表示sigma，mu。</p><ul><li>mu表示平均数，也就是整个正态分布的中轴线。mu的变化，表示整个图像向左右移动。</li><li>sigma表示方差，当sigma越大，图像越扁平；sigma约小，图像越突出。而最大值所在的位置，就是中轴线。</li></ul><p>我们的神经网络可以直接输出mu和sigma，就能获得整个策略的概率密度函数了。</p><p>现在我们已经有概率密度函数，那么当我们要按概率选出一个动作时，就只需要按照整个密度函数抽样出来就可以了。</p><h3 id="Off-Policy"><a href="#Off-Policy" class="headerlink" title="Off Policy"></a><strong>Off Policy</strong></h3><p>如上文中所说，PG，就是一个在线策略。因为PG用于产生数据的策略（行为策略），和需要更新的策略（目标策略）是一致。而DQN则是一个离线策略。我们会让智能体在环境互动一定次数，获得数据。用这些数据优化策略后，继续跑新的数据。但老版本的数据我们仍然是可以用的。也就是说，我们产生数据的策略，和要更新的目标策略不是同一个策略。所以DQN是一个离线策略。</p><p>但为什么PG和AC中的Actor更新，就不能像DQN一样，把数据存起来，更新多次呢？答案是在一定条件下，能，PPO做的工作就是这个。在了解在什么条件下可以的时候，我们需要先了解一下，为什么不能。</p><p>为了给大家直观的理解，我们来看这么一个简化的例子。</p><p>我们来看这么一个简化的例子：</p><p><img src="/2023/05/12/rl-05/image-20230516191437053.png" alt="image-20230516191437053"></p><p>假设，我们已知在同一个环境下，有两个动作可以选择。现在两个策略，分别是P和B：</p><p>P: [0.5,0.5] B: [0.1,0.9]</p><p>现在我们按照两个策略，进行采样；也就是分别按照这两个策略，以S状态下出发，与环境进行10次互动。获得如图数据。那么，我们可以用B策略下获得的数据，更新P吗？</p><p>答案是不行，我们可以回顾一下PG算法，PG算法会按照TD-error作为权重，更新策略。权重越大，更新幅度越大；权重越小，更新幅度越小。</p><p>但大家可以从如下示意图看到，如果用行动策略B[0.1,0.9]产出的数据，对目标策略P进行更新，动作1会被更新1次，而动作2会更新9次。虽然动作A的TD-error比较大，但由于动作2更新的次数更多，最终动作2的概率会比动作1的要大。</p><p>这自然不是我们期望看到的更新结果，因为动作1的TD-error比动作2要大，我们希望选择概率动作1的能更多呀。</p><p>从这个例子，大家可以大致明白，为什么我们在更新策略的时候，不能用其他策略产生的数据了。</p><p>但为什么DQN可以多次重复使用数据呢？</p><p>我们可以从两个角度看： </p><ol><li>更新Q值，和策略无关。 在同一个动作出发，可能会通往不同的state，但其中的概率是有环境所决定的，而不是我们的策略所决定的。所以我们产生的数据和策略并没有关系。</li><li>在DQN的更新中，我们是有”目标”的。 虽然目标比较飘忽，但每次更新，其实都是尽量向目标靠近。无论更新多少次，最终都会在目标附近徘徊。但PG算法，更新是不断远离原来的策略分布的，所以远离多少，远离的次数比例，我们都必须把握好。</li></ol><h3 id="Important-sampling"><a href="#Important-sampling" class="headerlink" title="Important-sampling"></a>Important-sampling</h3><p>那么，PPO是怎样做到离线更新策略的呢？答案是Important-sampling，重要性采样技术。如果我们想用策略B抽样出来的数据，来更新策略P也不是不可以。但我们要把td-error乘以一个重要性权重（IW：importance weight）。重要性权重：$IW = P（a）/ B（a）$应用在PPO，就是目标策略出现动作a的概率 除以 行为策略出现a的概率。回到我们之前的例子，我们可以计算出，每个动作的 重要性权重，P: [0.5,0.5] B: [0.1,0.9]</p><p>我们以a1为例，计算重要性权重$IW = P / B = 0.5/0.1 = 5$我们把重要性权重乘以td-error，我们发现，a1的td-error大幅提升，而a2的td-error减少了。现在即使我们用P策略: [0.5,0.5]进行更新，a1提升的概率也会比a2的更多。PPO应用了importance sampling，使得我们用行为策略获取的数据，能够更新目标策略，把AC从在线策略，变成离线策略。</p><h3 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h3><p>1、将环境信息s输入到actor-new网络， 得到两个值， 一个是mu， 一个是<a href="https://www.zhihu.com/search?q=sigma&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A&quot;62225540&quot;}">sigma</a>， 然后将这两个值分别当作正态分布的均值和方差构建正态分布(意义是表示action的分布)，然后通过这个正态分布sample出来一个action， 再输入到环境中得到奖励r和下一步的状态s<em>，然后存储[(s,a,r),…], 再将s</em>输入到actor-new网络，循环步骤1， 直到存储了一定量的[(s, a, r), …]， 注意这个过程中actor-new网络没有更新。</p><p>2、将1循环完最后一步得到的s<em>输入到critic-NN网络中， 得到状态的v</em>值， 然后计算折扣奖励：<img src="/2023/05/12/rl-05/image-20230516193624201.png" alt="image-20230516193624201">其中T为最后一个时间步。</p><p>3、将存储的所有s组合输入到critic-NN网络中， 得到所有状态的V<em>值， 计算$At  = G – V$</em></p><p>4、求c_loss = mean(square(At )), 然后反向传播更新critic-NN网络。</p><p>5、将存储的所有s组合输入actor-old和actor-new网络(网络结构一样)， 分别得到正态分布Normal1和Normal2， 将存储的所有action组合为actions输入到正态分布Normal1和Normal2， 得到每个actions对应的prob1和prob2， 然后用prob2除以prob1得到important weight, 也就是ratio。</p><p>6、根据论文公式7计算$a_loss = mean(min((ration<em> At,  clip(ratio, 1-ξ, 1+ξ)</em> At)))$, 然后反向传播， 更新actor-new网络。</p><p>7、循环5-6步骤， 一定步后， 循环结束， 用actor-new网络权重来更新actor-old网络(莫凡代码是在循环开始前更新的， 效果是一样的)。</p><p>8、循环1-7步骤。</p><p><img src="/2023/05/12/rl-05/image-20230516194612456.png" alt="image-20230516194612456"></p><h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><ol><li>我们可以用AC来解决连续型控制问题。方法是输入mu和sigma，构造一个正态分布来表示策略； </li><li>PPO延展了TD(0)，变成TD(N)的N步更新； </li><li>AC是一个在线算法，但为了增加AC的效率，我们希望把它变成一个离线策略，这样就可以多次使用数据了。为了解决这个问题，PPO使用了重要性采样。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;关于AC，很多书籍和教程都说AC是DQN和PG的结合，虽然正确但从这个角度去看不便于理解，安装我们之前的思路，PG利用带权重的梯度下降方法更新策略，而获得权重的方法是蒙地卡罗计算G值。蒙地卡罗需要完成整个游戏过程，直到最终状态，才能通过回溯计算G值。这使得PG方法的效率被限</summary>
      
    
    
    
    <category term="Notes" scheme="http://kerwinblog.top/categories/Notes/"/>
    
    
    <category term="RL" scheme="http://kerwinblog.top/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>强化学习笔记4————DQN与策略梯度(PG)</title>
    <link href="http://kerwinblog.top/2023/05/10/rl-04/"/>
    <id>http://kerwinblog.top/2023/05/10/rl-04/</id>
    <published>2023-05-10T15:12:58.000Z</published>
    <updated>2023-05-14T03:02:22.038Z</updated>
    
    <content type="html"><![CDATA[<p>之前了解了Qlearning的基本思路，在Qlearning中，我们有一个Qtable，记录着在每一个状态下，各个动作的Q值。Qtable的作用是当我们输入状态S，我们通过<strong>查表</strong>返回能够获得最大Q值的动作A。也就是我们需要找一个S-A的对应关系。这种方式很适合格子游戏。因为格子游戏中的每一个格子就是一个状态，但在现实生活中，很多状态并不是<strong>离散</strong>而是<strong>连续</strong>的。而且当例如在GYM中经典的CartPole游戏，杆子的角度是<strong>连续</strong>而不是<strong>离散</strong>的。在Atari游戏中，状态也是连续的。遇到这些情况，Qtable就没有办法解决。我们刚才说了Qtable的作用就是找一个S-A的对应关系。所以我们就可以用一个函数F表示，我们有F(S) = A。这样我们就可以不用查表了，而且还有个好处，函数允许<strong>连续</strong>状态的表示。这时候，我们深度神经网络就可以派上用场了。因为我们之前说过，神经网络就可以看成一个万能的函数。这个万能函数接受输入一个状态S，它能告诉我，每个动作的Q值是怎样的。</p><h1 id="Deep-Qlearning"><a href="#Deep-Qlearning" class="headerlink" title="Deep Qlearning"></a>Deep Qlearning</h1><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="/2023/05/10/rl-04/image-20230513193048889.png" alt="image-20230513193048889"></p><p>假设我们需要更新当前状态St下的某动作A的Q值：Q(S,A),我们可以这样做： 1. 执行A，往前一步，到达St+1; 2. 把St+1输入Q网络，计算St+1下所有动作的Q值； 3. 获得最大的Q值加上奖励R作为更新目标； 4. 计算损失 - Q(S,A)相当于有监督学习中的logits - maxQ(St+1) + R 相当于有监督学习中的lables - 用mse函数，得出两者的loss 5. 用loss更新Q网络。</p><p>也就是，我们用Q网络估算出来的两个相邻状态的Q值，他们之间的距离，就是一个r的距离。</p><p><img src="/2023/05/10/rl-04/image-20230514110221011.png" alt="image-20230514110221011"></p><h2 id="训练方式"><a href="#训练方式" class="headerlink" title="训练方式"></a>训练方式</h2><p>假设我们需要更新s状态的Q值(allQ),allQ是所有动作的Q值[0.1 0.2 0.1 0.6]，分别代表采取动作上、下、左、右。 如果我们用贪婪算法，我们会选出的动作是【下】，因为【下】的Q值最大。但很遗憾，由于我们采用epsilon-greedy，这次我们随机并随机到a是动作2【右】，然后我们求出下一状态s1的Q值(Q1),现在我们有下图：</p><p><img src="/2023/05/10/rl-04/image-20230513194153192.png" alt="image-20230513194153192"></p><p>其中，绿色格子代表动作a。蓝色格子是Q1的最大值，也就是maxQ1 = np.max(Q1)。现在我们需要构造target，因此我们把allQ先复制给targetQ。然后把r + lambd * maxQ1复制给targetQ中对应a的位置。</p><p>所以，我们现在有allQ向targetQ更新：</p><p><img src="/2023/05/10/rl-04/image-20230513195955628.png" alt="image-20230513195955628"></p><p>我们可视化一下，就可以看到Q网络的调整方向：动作1,3,4将会不变，而将会把动作2的Q值向targetQ的动作2方向靠近。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol><li>其实DQN就是Qlearning扔掉Qtable，换上深度神经网络。</li><li>我们知道，解决连续型问题，如果表格不能表示，就用函数，而最好的函数就是深度神经网络。</li><li>和有监督学习不同，深度强化学习中，我们需要自己找更新目标。通常在马尔科夫链体系下，两个相邻状态状态差一个奖励r经常能被利用。</li></ol><h1 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h1><p>从蒙多卡洛（MC）到时序差分（TD）,再到Qlearning，最后发展到DQN，其实我们都是在想办法计算在不同我们在马尔科夫状态链每个状态下的Q值和V值，然后在旋转能够使Q值或者V值最大的方向去执行，但是细想一下，我们在做决策时真的需要这样去计算吗，为什么不能直接告诉我在某个状态下该往哪个方向去决策呢？策略梯度(Policy Gradient)就是这样一直方法。</p><p>如果说DQN是一个TD+神经网络的算法，那么PG是一个蒙地卡罗+神经网络的算法。在神经网络出现之前，当我们遇到非常复杂的情况时，我们很难描述，我们遇到每一种状态应该如何应对。但现在我们有了神经网络这么强大的武器，我们就可以用一个magic函数直接代替我们想要努力描述的规则。我们用 $\pi$表示策略，也就是动作的分布。那么我们期望有这么一个magic函数，当我输入state的时候，他能输出$\pi_i$，告诉智能体这个状态，应该如何应对：$\pi_i$= magic(state)。如果智能体的动作是对的，那么就让这个动作获得更多被选择的几率；相反，如果这个动作是错的，那么这个动作被选择的几率将会减少。问题在于，我们怎么衡量对和错呢？PG的想法非常简单粗暴：蒙地卡罗的G值！</p><p>我们先来复习一下蒙地卡罗:我们从某个state出发，然后一直走，直到最终状态。然后我们从最终状态原路返回，对每个状态评估G值。所以G值能够表示在策略 下，智能体选择的这条路径的好坏。</p><p><img src="/2023/05/10/rl-04/image-20230513202029508.png" alt="image-20230513202029508"></p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>我们先用数字，直观感受一下PG算法。从某个state出发，可以采取三个动作。假设当前智能体对这一无所知，那么，可能采取平均策略  $\pi_0$  = [33%,33%,33%]。智能体出发，选择动作A，到达最终状态后开始回溯，计算得到 G = 1。</p><p><img src="/2023/05/10/rl-04/image-20230513202422602.png" alt="image-20230513202422602" style="zoom: 80%;"></p><p>我们可以更新策略，因为该路径<strong>选择了A</strong>而产生的，<strong>并获得G = 1</strong>；因此我们要更新策略：让A的概率提升，相对地，BC的概率就会降低。 计算得新策略为：  $\pi_1$  = [50%,25%,25%]，虽然B概率比较低，但仍然有可能被选中。第二轮刚好选中B。智能体选择了B，到达最终状态后回溯，计算得到 G = -1。</p><p><img src="/2023/05/10/rl-04/image-20230513202550228.png" alt="image-20230513202550228"></p><p>所以我们对B动作的评价比较低，并且希望以后会少点选择B，因此我们要降低B选择的概率，而相对地，AC的选择将会提高。计算得新策略为： $\pi_2$  = [55%,15%,30%]，最后随机到C，回溯计算后，计算得G = 5。</p><p><img src="/2023/05/10/rl-04/image-20230513202945331.png" alt="image-20230513202945331"></p><p>C比A还要多得多。因此这一次更新，C的概率需要大幅提升，相对地，AB概率降低。 $\pi_3$ = [20%,5%,75%]</p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>我们以某一个状态为例，在某个状态下，通过网络得到的预测值$y^*$，真实值$y$，G值,如下图</p><p><img src="/2023/05/10/rl-04/image-20230513204429480.png" alt="image-20230513204429480"></p><p>我们可以把这个过程想象成一个分类任务。在训练的时候，只有真实值为1，其他为0。所以动作1,3,4的概率将会向0靠，也就是减少。而动作2的概率将会向1靠，也就是说会有所提升。我们可以使用交叉熵对这个分布进行调整。</p><p>这里的G值的作用类似于权重，在代码中的实现方式为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(neg_log_prob * G) </span><br></pre></td></tr></table></figure><p>当G值调整大小的时候，相当于每次训练幅度进行调整。例如G值为2，那么调整的幅度将会是1的两倍。如果G值是一个负数呢，那么相当于我们进行反向的调整。如下图，如果G值为-1，那么说明选择动作2并不是一个“明智”的动作。于是我们让这个动作2的预测值降低，相当于“远离”真实值1。而其他动作的概率有所提升，相当于“远离”真实值0。</p><p>总结一下：1、通过网络，求出预测值pre的分布。 2、和真实值action进行比较，求得neg_log_prob 3、最终求得neg_log_prob乘以G值，求得loss</p><h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><p>PG用一个全新的思路解决了问题。但实际效果显得不太稳定，在某些环境下学习较为困难。另外由于采用了MC的方式，需要走到最终状态才能进行更新，而且只能进行一次更新，这也是PG算法的效率不高的原因。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;之前了解了Qlearning的基本思路，在Qlearning中，我们有一个Qtable，记录着在每一个状态下，各个动作的Q值。Qtable的作用是当我们输入状态S，我们通过&lt;strong&gt;查表&lt;/strong&gt;返回能够获得最大Q值的动作A。也就是我们需要找一个S-A的对应关</summary>
      
    
    
    
    <category term="Notes" scheme="http://kerwinblog.top/categories/Notes/"/>
    
    
    <category term="RL" scheme="http://kerwinblog.top/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>Segment Anything</title>
    <link href="http://kerwinblog.top/2023/05/10/0508/"/>
    <id>http://kerwinblog.top/2023/05/10/0508/</id>
    <published>2023-05-10T11:01:41.000Z</published>
    <updated>2023-05-10T11:58:54.807Z</updated>
    
    <content type="html"><![CDATA[<p>随着最近大模型在NLP领域的大展神威，CV届对比起来略显沉闷了一点，不少人好奇，CV相关的大模型也不少（VIT，CLIP），那为什么没有出现类似大模型在NLP领域展示出来的“涌现”能力呢？<span id="more"></span> 知乎上有一个问题感觉很好：<a href="https://www.zhihu.com/question/597657073">为何 CV 里没有出现类似 NLP 大模型的涌现现象？</a>，这里我认为有一个答案说得不错：</p><blockquote><p>CV处理的是自然世界信息，信息往往离散、<a href="https://www.zhihu.com/search?q=semantic密度&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A3016292348}">semantic密度</a>低；而NLP处理的是文本，它抽象程度极高，semantic密度极高。文本既能承载真实存在的信息（“Earth is a planet in the solar system”），也能表述逻辑、情感（“I like this dog because it’s <a href="https://www.zhihu.com/search?q=cute&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A3016292348}">cute</a>“），还能非常方便地和人类进行交互，在某种程度上和人类以很简单的方式互相”理解“。</p></blockquote><p>虽然能够一统CV的大模型没有出现，但是在细分领域，似乎已经有模型做到了类似的事情。</p><p>今天介绍的这一篇face book的论文<a href="https://ai.facebook.com/research/publications/segment-anything/">Segment Anything</a>是一个用于图像分割的新任务、模型和数据集。在他刚出来的那一天，知乎等平台就已经高呼CV已死。为了这个项目，作者创建了迄今为止最大的分割数据集，1100万张在10亿次授权且尊重隐私的图像上的数据集。模型也被设计和训练成了promptable,就是说可以给他一些提示。作者在多个数据集测试了他的结果并认为结果令人满意。</p><p>项目地址<a href="https://segment-anything.com/">https://segment-anything.com/</a></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>作者在引言中讨论了NLP工作中prompt的巨大作用，并回顾了视觉领域多模态的重要工作CLIP和ALIGN，最后说明了他们的目标和工作。</p><p>首先，在网络上经过预训练的大型语言模型凭借其强大的zero-shot和few-shot能力革新NLP，prompt的引入使得这些模型zero-shot和few-shot性能与微调模型出奇的好。经验趋势表明，这种行为随着模型规模、数据集大小和总训练计算的增加而改善。</p><p>CLIP和ALIGN使用对比学习来训练对齐两种模态的文本和图像编码器。经过训练后，prompt可以实现对新视觉概念和数据分布的zero-shot概括。这种编码器还与其他模块有效组合，以实现下游任务，如图像生成（例如，DALL·E）。虽然在视觉和语言编码器方面已经取得了很大进展，但计算机视觉包括了超出这一范围的广泛问题，而且对于其中许多问题，还不存在丰富的训练数据。</p><p>作者提到在这项工作中，他们的目标是建立一个图像分割的基础模型。也就是说，他们在寻求开发一个可提示的模型，并使用能够实现强大泛化的任务在广泛的数据集上对其进行预训练。有了这个模型，他们的目标是使用即时工程解决新数据分布上的一系列下游分割问题。</p><p>这个计划的成功取决于三个组成部分：任务、模型和数据。为了开发它们，作者解决了以下关于图像分割的问题： 1. 什么样的样本可以实现零样本泛化 2. 相应的模型架构是什么 3. 什么样的数据可以支撑这个人物和模型</p><p>这些问题错综复杂，作者首先定义了一个promptable的分割任务，这可以提供强大的预训练目标，并且有广泛的下游任务可以应用。这个任务需要一个支持灵活的prompt的模型并且可以输出分割结果。为了训练这个模型，作者需要一个多样性的，大型的数据集，因此作者构建了一个数据引擎，使用高效的模型进行迭代。作者介绍了每个组件然后是创建的数据集和有效性的实验。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>作者从NLP领域获得灵感，在NLP的任务中，预测下一个token用于基础模型的训练，并通过prompt engineering 解决不同的下游任务。为了建立这样一个分割的基础模型，作者的目标书建立一个具有类似能力的任务 ### Task promptable的分割任务是给定任何prompt都能返回有效的分割掩码。有效的mask意味着即使prompt是不准确的或者涉及到多个对象的也应该的能够输出正确的或者合理的掩码。如图所示，每列显示SAM从单个不明确的点提示生成的3个有效掩码。</p><p><img src="/2023/05/10/0508/20230407194759.png" alt="image.png"></p><h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>promptable segmentation task 提出了一种自然的预训练算法，该算法模拟每个训练样本的提示序列（例如，点、框、掩码），并将模型的掩码预测与基本事实进行比较。作者将这种方法从交互式分割中改编出来，尽管与交互式分割不同，交互式分割的目的是在足够的用户输入后最终预测有效的掩码，但promptable segmentation task 的目的是始终预测任何提示的有效掩码，即使提示不明确的/错误的/荒谬的。</p><h2 id="Zero-shot-推理"><a href="#Zero-shot-推理" class="headerlink" title="Zero-shot 推理"></a>Zero-shot 推理</h2><p>直观地说，预训练任务赋予了模型在推理时对任何提示做出适当响应的能力，因此下游任务可以通过设计适当的提示来解决。一般来说，一系列实用的分割任务可以作为提示。除了自动数据集标记外，作者还在第7部分中的实验中探索了五个不同的示例任务。 ## 模型 SAM包括了三个部分 一个 image encoder, 一个 flexible prompt encoder, 和一个 fast mask decoder</p><p><img src="/2023/05/10/0508/20230407203046.png" alt="image.png" style="zoom:50%;"></p><h2 id="image-encoder"><a href="#image-encoder" class="headerlink" title="image encoder"></a>image encoder</h2><p>受可扩展性和强大的预训练方法的启发，作者使用了MAE预训练的视觉转换器（ViT），该转换器至少适用于处理高分辨率输入。图像编码器每个图像运行一次，并且在prompt运行之前运行</p><h2 id="prompt-encoder"><a href="#prompt-encoder" class="headerlink" title="prompt encoder"></a>prompt encoder</h2><p>作者考虑了两组提示：稀疏(sparse)（点、框、文本）和密集(dense)（掩码）。MAE通过位置编码来表示点和框，这些位置编码与每个使用CLIP的现成文本编码器来编码过的prompt的学习嵌入相加。dense prompt（即掩码）使用卷积嵌入，并与图像嵌入逐元素求和。</p><h2 id="mask-decoder"><a href="#mask-decoder" class="headerlink" title="mask decoder"></a>mask decoder</h2><p>掩码解码器有效地将图像嵌入、提示嵌入和输出标记映射到掩码。这种设计受到的启发，对 Transformer decoder 进行了修改，然后是动态掩码预测头。修改后的解码器块在两个方向上使用提示自注意力和交叉注意力（(prompt-to-image embedding，反之亦然）来更新所有嵌入。在运行两个块后，对图像嵌入进行上采样，MLP将输出标记映射到动态线性分类器，然后计算每个图像位置的mask foreground 概率。</p><h1 id="Data-engine"><a href="#Data-engine" class="headerlink" title="Data engine"></a>Data engine</h1><p>由于分割掩码在互联网上并不丰富，作者构建了一个数据引擎来实现1.1B 掩码数据集 SA-1B 的集合。</p><p>数据引擎分为三个阶段：（1）模型辅助手动注释阶段，（2）混合自动预测掩码和模型辅助注释的半自动阶段，以及（3）全自动阶段，</p><h3 id="手动阶段"><a href="#手动阶段" class="headerlink" title="手动阶段"></a>手动阶段</h3><p>在第一阶段，类似于经典的交互式分割，一组专业注释者通过使用由 SAM 驱动的基于浏览器的交互分割工具点击前景/背景对象点来标记掩码。可以使用像素精确的“刷”和“擦除”工具来细化掩码。模型辅助注释直接在浏览器内实时运行（使用预先计算的图像嵌入），从而实现真正的交互体验。标注不受语义约束，可以自由地标注”stuff” and “things”</p><p><strong>注释者被要求按突出顺序标记对象，一旦掩码需要超过 30 秒进行注释，便鼓励继续下一个图像。</strong></p><p>在SOTA之后，SAM就开始使用公共数据集进行训练，在经过了足够多的数据标注后，就用新标注的数据重新训练。随着收集更多的掩码，图像使用了ViT-H作为编码器。这样的模型训练一共进行了六次。随着模型的改进，每个掩码的平均注释时间从 34 秒减少到 14 秒。随着SAM的改进，每张图像的平均掩码数从20个掩码增加到44个掩码。总体而言，作者在这个阶段从 120k 张图像收集了 4.3M 掩码。</p><h3 id="半自动化阶段"><a href="#半自动化阶段" class="headerlink" title="半自动化阶段"></a>半自动化阶段</h3><p>这个阶段的目标是增加mask的多样性。为了将标记集中在不太突出的对象上，首先自动检测confident masks。然后向注释者展示了用这些掩码预先填充的图像，并要求他们注释任何额外的未注释对象。为了检测confident masks，作者使用通用的“对象”类别在所有第一阶段掩码上训练了一个边界框检测器。在这个阶段，作者在 180k 图像中收集了一个额外的 5.9M 掩码（总共 10.2M 掩码）。在第一阶段，在新收集的数据（5 次）上定期重新训练模型。每个掩码的平均注释时间可以回到了到 34 秒（不包括自动掩码），因为这些对象对标签更具挑战性。每张图像的平均掩码数从 44 个掩码到 72 个掩码（包括自动掩码）。</p><h3 id="全自动化阶段"><a href="#全自动化阶段" class="headerlink" title="全自动化阶段"></a>全自动化阶段</h3><p>这个阶段的主要目的是解决歧义</p><p>正文部分说的不太清楚，在附录部分作者做了详细的解释。一共分为四个部分，Cropping，Filtering，Postprocessing，Automatic mask generation model。</p><p>这个过程作者使用32∗32网格的点对图像进行预测，并为每个点预测一组可能对应于有效对象的掩码。如果说一个点位于一个部件或子部件上，我们的模型将返回子部分，部分和整个对象(subpart, part, and whole object)。利用模型中的IoU预测模块来选择confident mask.如果将概率图阈值设为0.5−δ和0.5 + δ会产生相似的掩码，那么这个掩码就被认为是是稳定的。为了进一步提高小mask的质量，还处理了多个重叠的放大mask。</p><p>全自动掩码生成应用于数据集中的所有 11M 图像，总共产生了 1.1B 的高质量掩码。</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>SA的数据集使用data engine 构建的多样的高分辨率的有隐私保护的图像和1.1B个掩码组成。作者发布了这个这个数据集来帮助未来计算机视觉基础模型。SA-1B 将在某些研究用途的有利许可协议下发布，并为研究人员保护。 ### 图像 作者团队从直接与摄影师一起工作的提供商那里获得了一组新的高分辨率的11M图像。即使在下采样之后，这些图像的分辨率也明显高于许多现有的视觉数据集</p><h3 id="掩码"><a href="#掩码" class="headerlink" title="掩码"></a>掩码</h3><p>数据引擎产生了 1.1B 掩码，其中 99.1% 是全自动生成的。因此，自动掩码的质量至关重要。作者团队将这些mask与专业标记的数据集进行标记，发现自动掩码对于训练模型是高质量和有效的。受这些发现的启发，SA-1B 仅包含自动生成的掩码。</p><blockquote><p>To estimate mask quality, we randomly sampled 500 images (∼50k masks) and asked our professional annotators to improve the quality of all masks in these images. Annotators did so using our model and pixel-precise “brush” and “eraser” editing tools. This procedure resulted in pairs of automatically predicted and professionally corrected masks. We computed IoU between each pair and found that 94% of pairs have greater than 90% IoU (and 97% of pairs have greater than 75% IoU). For comparison, prior work estimates inter-annotator consistency at 85-91% IoU [44, 60]. Our experiments in §7 confirm by human ratings that mask quality is high relative to a variety of datasets and that training our model on automatic masks is nearly as good as using all masks produced by the data engine. ## Responsible AI</p></blockquote><h2 id="Zero-Shot推理实验"><a href="#Zero-Shot推理实验" class="headerlink" title="Zero-Shot推理实验"></a>Zero-Shot推理实验</h2><p>作者在这里讨论了五个任务，其中四个与训练数据完全不同。这也避免了模型训练过程中能够看到答案。这几个任务分别是 1. zero-shot单点有效掩码评估 2. 执行边缘检测 3. 分割所有内容，即对象提议生成 4. 分割检测到的对象，即实例分割， 5. 作为概念验证，从自由形式的文本中分割对象。</p><h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><p>自机器学习的早期以来，预训练模型已经适应下游任务。近年来，随着对规模的日益重视，这种范式变得越来越重要，并且此类模型最近被称为为“基础模型”，即”大规模在广泛的数据上训练并适应广泛的下游任务”</p><p>作者的工作是与此高度相关的，尽管分割只是计算机视觉任务的一个子集。作者还将他们的方法的一个方面与另一项工作进行了对比<a href="https://arxiv.org/abs/2108.07258">Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv:2108.07258, 2021. 1, 12</a>，# On the Opportunities and Risks of Foundation Models强调了自监督学习在基础模型中的作用。虽然SA模型是用自监督技术(MAE)初始化的，但它的绝大多数能力来自于大规模的监督训练。在数据引擎可以扩展可用注释的情况下，监督训练提供了一种有效的解决方案。</p><p>SA不可避免地也有一些局限性，SAM是为通用性和使用广度而设计的，不同于以往的很多工作，它不是高IoU交互式分割。虽然SAM可以执行许多任务，但目前尚不清楚如何设计简单的提示符来实现语义和全景分割。最后，还有一些领域特定的工具，它们在各自的领域中依然有希望优于SAM。</p><p>总而言之，Segment Anything项目是将图像分割提升到基础模型时代的一种尝试。这项工作的主要贡献是一个新的任务(提示分割)，模型(SAM)和数据集(SA-1B)，使这一飞跃成为可能。SAM是否达到了基础模型的地位，仍然要看它在社区中是如何使用的，但这项工作的前景，超过1B个掩模的发布，以及作者的快速分割模型将有助于铺平前进的道路。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;随着最近大模型在NLP领域的大展神威，CV届对比起来略显沉闷了一点，不少人好奇，CV相关的大模型也不少（VIT，CLIP），那为什么没有出现类似大模型在NLP领域展示出来的“涌现”能力呢？&lt;/p&gt;</summary>
    
    
    
    <category term="paper Reading" scheme="http://kerwinblog.top/categories/paper-Reading/"/>
    
    
    <category term="CV" scheme="http://kerwinblog.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>强化学习笔记3————Qlearning和SARSA</title>
    <link href="http://kerwinblog.top/2023/05/08/rl-03/"/>
    <id>http://kerwinblog.top/2023/05/08/rl-03/</id>
    <published>2023-05-08T15:15:58.000Z</published>
    <updated>2023-05-14T03:36:38.459Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们学习了如何使用蒙地卡罗（MC）和时序差分（TD）方法来更新一个节点的V值，注意这里的节点指的是状态节点$S_t$,但是在实际的情况中，我们更希望的是知道在该状态下，进行不同的动作$Q$获得的Q值，这样只要知道某个状态下所有动作值的奖励值，也就是知道了具有最高奖励值的是哪个动作，然后直接采取这个动作就行。</p><h2 id="TD之于Q值估算"><a href="#TD之于Q值估算" class="headerlink" title="TD之于Q值估算"></a>TD之于Q值估算</h2><p><img src="/2023/05/08/rl-03/image-20230512122441211.png" alt="image-20230512122441211"></p><p>我们现在用上TD的思路。我们在 $S<em>t$，智能体根据策略pi，选择动作$A_t$，进入$S</em>{(t+1)}$状态，并获得奖励R。 如果你之前对V和Q的理解足够深，那么不难理解上面这张图。$V(St+1)$的意义是，在 $S_{(t+1)}$ 到最终状态获得的奖励期望值。 $Q(S_t,A_t)$ 的意义是，在$Q(S_t,A_t)$到最终状态获得的奖励期望值。 所以我们可以把$V(S_t+1)$看成是下山途中的一个路牌，这个路牌告诉我们下山到底还有多远，然后加上R这一段路，就知道 $Q(S_t,A_t)$ 离山脚有多长的路。</p><p>但在实际操作的时候，会有一个问题。 在这里我们要估算两个东西，一个是V值，一个是Q值。</p><p>人们想出的办法就是，用下一个动作的Q值，代替V值。因为从状态$S<em>{(t+1)}$到动作$A</em>{t+1}$之间没有奖励反馈，所以我们直接用 $A<em>{t+1}$ 的Q价值，代替$S</em>{(t+1)}$价值。 这样不就是可以了吗?</p><p>还有一个问题：在$S<em>{t+1}$下，可能有很多动作$A</em>{t+1}$。不同动作的Q值自然是不同的。 所以$Q(S<em>{t+1},A</em>{t+1})$并不能等价于$V(S_{t+1})$。</p><p>虽然不相等，但不代表不能用其中一个来代表$V(S<em>{t+1})$。人们认为有个可能的动作产生的Q值能够一定程度代表$V(S</em>{t+1})$。</p><ol><li>在相同策略下产生的动作$A_{t+1}$。这就是SARSA。</li><li>选择能够产生最大Q值的动作$A_{t+1}$。这就是Qlearning。</li></ol><h2 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA"></a>SARSA</h2><p>为什么SARSA用相同策略下产生的动作At+1是合理的。答案很简单，它管用。 其实在强化学习，虽然涉及很多数学，但它并不是严谨科学，它更像是工业，只要实际操作管用就行。</p><p>现在我们回到SARSA：</p><p><img src="/2023/05/08/rl-03/image-20230512135902591.png" alt="image-20230512135902591"></p><p>其实SARSA和我们上一篇说的TD估算V值几乎一模一样，只不过我们挪了一下，从V改成Q了。</p><script type="math/tex; mode=display">\text{SARSA}:& Q(S_t,A_t) \leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t})-Q(S_{t},A_{t})] \\\text{TD(0)估算V}:& V(S_t) \leftarrow V(S_t)+ \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)]</script><p>注意，这里的$A<em>{t+1}$是在同一策略产生的。也就是说,$S_t$选$A_t$的策略和$S</em>{t+1}$选$A_{t+1}$是同一个策略。这也是SARSA和Qlearning的唯一区别</p><h2 id="Qlearning"><a href="#Qlearning" class="headerlink" title="Qlearning"></a>Qlearning</h2><p>Qlearning能够产生最大Q值的动作$A<em>{t+1}$的Q值作为$V(S</em>{t+1})$的替代。</p><p><img src="/2023/05/08/rl-03/image-20230512162247449.png" alt="image-20230512162247449"></p><p>道理其实也很简单：因为我们需要寻着的是能获得<strong>最多奖励</strong>的动作，Q值就代表我们能够获得今后奖励的期望值。所以我们只会选择Q值最大的，也只有最大Q值能够代表V值。</p><h3 id="Qlearning算法流程"><a href="#Qlearning算法流程" class="headerlink" title="Qlearning算法流程"></a>Qlearning算法流程</h3><p>我们现在重新整理下，Qleanring的更新流程。 我们将会在任意的state出发</p><ol><li>我们将会用<strong>noisy-greedy的策略</strong>选定动作A</li><li>在完成动作后，我们将会进入新状态St+1；</li><li>检查St+1中所有动作，看看哪个动作的Q值最大；</li><li>用以下的公式更新当前动作A的Q值；</li><li>继续从s’出发，进行下一步更新 1-6步我们作为一个EP，进行N个EP的迭代。</li></ol><p><img src="/2023/05/08/rl-03/image-20230513173856144.png" alt="image-20230513173856144"></p><p><img src="/2023/05/08/rl-03/image-20230513174001172.png" alt="image-20230513174001172"></p><p>在具体实现的时候，有两个方式需要注意：</p><h4 id="Q-table"><a href="#Q-table" class="headerlink" title="Q-table"></a>Q-table</h4><p>Q-table(Q表格) Qlearning算法非常适合用表格的方式进行存储和更新。所以一般我们会在开始时候，先创建一个Q-tabel，也就是Q值表。这个表纵坐标是状态，横坐标是在这个状态下的动作。</p><p><img src="/2023/05/08/rl-03/image-20230513181754597.png" alt="image-20230513181754597"></p><p>我们会初始化这个表的值为0。我们的任务就是，通过算法更新，把各个状态下的动作的Q值，填到上面去。</p><h4 id="noisy-greedy"><a href="#noisy-greedy" class="headerlink" title="noisy-greedy"></a>noisy-greedy</h4><p>之前说过，在选择动作的时候，理论上每次都会使用当前状态下，Q值最大的动作。这样的选择方式，我们称为“贪婪”(greedy)。</p><p>因为我们只选择Q值最大的动作，所以有一些动作没被更新过没有被选择的过的动作，将更新不到。Q值也永远为0。</p><p>举个例子：</p><p><img src="/2023/05/08/rl-03/image-20230513190146178.png" alt="image-20230513190146178"></p><p>假设某次智能体经过路径(途中的红色线路)，根据Qlearning算法更新公式，我们计算得到某动作Q值为3。</p><p>由于其他动作还没执行过，因此他们保持初始值(一般为0)。按照贪婪算法，下一次智能体来到S的时候，会选择Q值最大的动作，也就是Q=3。于是红色路径再次被执行，Q值被更新。然后再一次，智能体仍然只会选红色线路。</p><p>但事实上，Q值最大的可能是其他的动作，但其他动作没有Q值，只是因为没有被“探索”出来。事实上我们会希望智能体在开始的时候更多随机行走去探索，而后面更多按照Q值去走动。在每次选择动作的时候，就给我们要选择的动作叠加一个噪音。所谓噪音，就是在原来的值上增加一个随机值。<strong>注意！这些噪音只是在选择的时候，临时加上，每次都随机的。只干扰了当前选择，并不会影响真正的Q值。</strong>当我们认为智能体对环境的了解已经足够充分，我们就可以慢慢减少噪音的大小。在实做中，我们只需要在我们每次游戏后，将会减少产生噪音的方差，这样对干扰仍然有干扰，但这种干扰将会逐渐减少。直到相对于真正的Q值没有影响的程度。最终，agent将会按照自己的策略选择动作。</p><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>那我们如何使用noisy-greedy策略来更新Q值呢，这里我们看看代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.argmax(Q[s, :] + np.random.randn(1, env.action_space.n) * (1. / (i + 1)))</span><br></pre></td></tr></table></figure><p>这一行代码我们可以切开几个步骤来看一下：</p><ul><li>首先，Q[s, :] 我们看一下table表的s行，就是我们当前的状态对应各个动作的Q值。</li><li>其次，np.random.randn(1, env.action_space.n) 就是我们制造出来的噪音，我们希望噪音随着迭代的进行，将会越来越小。 因此我们乘以 (1. / (i + 1))。当i越来越大的时候，噪音就越来越小了。</li><li>最后，我们通过np.argmax()获得最大Q值对应的列号，也就是对应的动作。这里要注意，argmax找出最大值后，并不是返回<strong>最大值</strong>，而是返回<strong>最大值的列号</strong>，也就是<strong>动作</strong>。同学在这里要注意理解，我们需要的是动作A，而不是Q值。</li></ul><p><img src="/2023/05/08/rl-03/image-20230513191620017.png" alt="image-20230513191620017"></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s1, r, d, _ = env.step(a)</span><br></pre></td></tr></table></figure><p>env.step() 我们把动作传入到环境中，环境会给我们返回4个返回值。</p><ul><li>new_state: 示例代码用s1表示。这个表示我们执行动作后，新的状态。</li><li>reward: 示例代码中用r表示，执行动作a后，获得的收获</li><li>done：一个标志位，表示这个是否最终状态。</li><li>_ : 其实是info，但我们一般用不到这个值；因此我们把它先忽略。</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Q[s, a] = Q[s, a] + lr  (r + lambd  np.max(Q[s1, :]) - Q[s, a])</span><br></pre></td></tr></table></figure><p>我们用newstate的Q值，更新我们现在状态的Q值。我们对应更新公式，就很容易理解了。 注意比较：这里np.max和之前np.argmax函数的区别在于，np.max是返回最大值。而np.argmax返回时最大的行数或者列数。</p><p><img src="/2023/05/08/rl-03/image-20230513191916006.png" alt="image-20230513191916006"></p><p>最后，我们更新Q值的任务已经完成，把游戏进行下去。把下一个状态s1赋值给s，重新开始新一步，和新一步的更新。</p><p>但在开始之前，我们检查一下，下一个状态是否就是终止状态了，如果是，这一次游戏就算是完成，开始一次迭代。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Qlearning和SARSA是多么鼎鼎大名，但直觉上理解还是很简单的。</p><p>现在我们来总结一下整个思路： 1. Qlearning和SARSA都是基于TD(0)的。不过在之前的介绍中，我们用TD(0)估算状态的V值。而Qlearning和SARSA估算的是动作的Q值。 2. Qlearning和SARSA的核心原理，是用下一个状态St+1的V值，估算Q值。 3. 既要估算Q值，又要估算V值会显得比较麻烦。所以我们用下一状态下的某一个动作的Q值，来代表St+1的V值。 4. Qlearning和SARSA唯一的不同，就是用什么动作的Q值替代St+1的V值。 - SARSA 选择的是在St同一个策略产生的动作。 - Qlearning 选择的是能够产生最大的Q值的动作。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;前面我们学习了如何使用蒙地卡罗（MC）和时序差分（TD）方法来更新一个节点的V值，注意这里的节点指的是状态节点$S_t$,但是在实际的情况中，我们更希望的是知道在该状态下，进行不同的动作$Q$获得的Q值，这样只要知道某个状态下所有动作值的奖励值，也就是知道了具有最高奖励值的</summary>
      
    
    
    
    <category term="Notes" scheme="http://kerwinblog.top/categories/Notes/"/>
    
    
    <category term="RL" scheme="http://kerwinblog.top/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>强化学习笔记2————蒙地卡罗与时序差分</title>
    <link href="http://kerwinblog.top/2023/05/06/rl-02/"/>
    <id>http://kerwinblog.top/2023/05/06/rl-02/</id>
    <published>2023-05-06T14:34:21.000Z</published>
    <updated>2023-05-12T03:13:58.617Z</updated>
    
    <content type="html"><![CDATA[<p>上一节中我们介绍了马尔科夫链和在强化学习中Q值和V值的定义，那么在实际训练的时候，我们怎么去计算一个节点的Q值或者V值呢，这就要提到著名的蒙地卡罗算法（mc）和时序差分算法（td）</p><h2 id="蒙地卡罗"><a href="#蒙地卡罗" class="headerlink" title="蒙地卡罗"></a>蒙地卡罗</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>蒙地卡罗的算法原理并不难理解，下面这张图可以很好的概括。</p><p><img src="/2023/05/06/rl-02/image-20230512094103652.png" alt="image-20230512094103652"></p><p>具体来说，蒙地卡罗的算法流程分为以下几个步骤：</p><ol><li>我们把智能体放到环境的任意状态；</li><li>从这个状态开始按照策略进行选择动作，并进入新的状态。</li><li>重复步骤2，直到最终状态；</li><li>我们从最终状态开始向前回溯：计算每个状态的G值。</li><li>重复1-4多次，然后平均每个状态的G值，这就是我们需要求的V值。</li></ol><p>首先我们要理解每一次我们算的G值的意义。G值的意义在于，在这一次游戏中，某个状态到最终状态的奖励总和(理解时可以忽略折扣值)</p><ul><li>第一步，我们根据策略往前走，一直走到最后，期间我们什么都不用算，还需要记录每一个状态转移，我们获得多少奖励r即可。</li><li>第二步，我们从终点往前走，一遍走一遍计算G值。G值等于上一个状态的G值(记作G’),乘以一定的折扣(gamma),再加上r。</li></ul><p>当我们进行多次试验后，我们有可能会经过某个状态多次，通过回溯，也会有多个G值。 重复我们刚才说的，每一个G值，就是每次到最终状态获得的奖励总和。而V值时候某个状态下，我们通过影分身到达最终状态，所有影分身获得的奖励的平均值。</p><p><img src="/2023/05/06/rl-02/image-20230512101027119.png" alt="image-20230512101027119"></p><h3 id="再进一步"><a href="#再进一步" class="headerlink" title="再进一步"></a>再进一步</h3><p>蒙地卡罗算法单独拿出来，在强化学习中效率还是比较低的。所以会结合其他的方式进行应用。这些我们会在后面具体算法中讲到。</p><p>在这一篇中，希望大家能够理解两点： 1. G的意义：在某个路径上，状态S到最终状态的总收获。 2. V和G的关系：V是G的平均数。</p><p>到这里要注意一点：V和策略是相关的，那么在这里怎么体现呢？这个非常重要，因为在PPO算法中，离线策略就与这个有关。这里可以稍微先说一下。</p><p>我们仍以上图为例子，以策略A进行游戏。其中有100次经过S点，经过S点后有4条路径到达最终状态，计算G值和每条路径次数分别如下：</p><p><img src="/2023/05/06/rl-02/image-20230512101123437.png" alt="image-20230512101123437"></p><p>策略A采用平均策略，这时候 V = 5。</p><p>现在我们采用策略B，<strong>由于策略改变，经过某条路径的概率就会产生变化。因此最终试验经过的次数就不一样了。</strong></p><p><img src="/2023/05/06/rl-02/image-20230512102336449.png" alt="image-20230512102336449"></p><p>最终计算的 V = 7.55。</p><h3 id="蒙地卡罗的缺陷"><a href="#蒙地卡罗的缺陷" class="headerlink" title="蒙地卡罗的缺陷"></a>蒙地卡罗的缺陷</h3><p>在实际引用中，蒙地卡罗虽然比动态规划消耗要少一点；而且并不需要知道整个环境模型。</p><p>但蒙地卡罗有一个比较大的缺点，就是每一次游戏，都需要先从头走到尾，再进行回溯更新。如果最终状态很难达到，那小猴子可能每一次都要转很久很久才能更新一次G值。</p><h2 id="时序差分算法"><a href="#时序差分算法" class="headerlink" title="时序差分算法"></a>时序差分算法</h2><h3 id="TD和MC的比较"><a href="#TD和MC的比较" class="headerlink" title="TD和MC的比较"></a>TD和MC的比较</h3><p>TD算法对蒙地卡罗(MC)进行了改进。 1. 和蒙地卡罗(MC)不同：TD算法只需要走N步。就可以开始回溯更新。 2. 和蒙地卡罗(MC)一样：小猴需要先走N步，每经过一个状态，把奖励记录下来。然后开始回溯。 3. 那么，状态的V值怎么算呢？其实和蒙地卡罗一样，我们就假设N步之后，就到达了最终状态了。 - 假设“最终状态”上我们之前没有走过，所以这个状态上的纸是空白的。这个时候我们就当这个状态为0. - 假设“最终状态”上我们已经走过了，这个状态的V值，就是当前值。然后我们开始回溯。</p><h3 id="TD原理的直观理解"><a href="#TD原理的直观理解" class="headerlink" title="TD原理的直观理解"></a>TD原理的直观理解</h3><p>我们可以把TD看成是这样一种情况：</p><p>我们从A状态，经过1步，到B状态。我们什么都不管就当B状态是最终状态了。</p><p>但B状态本身就带有一定的价值，也就是V值。其意义就是从B状态到最终状态的总价值期望。(这一点在之前Q值和V值那篇已经说明过，就不在赘述了。)</p><p>我们假设B状态的V值是对的，那么，通过回溯计算，我们就能知道A状态的更新目标了。</p><p>这就有点像从山顶像知道要下山的路有多长。 MC能直接走一趟，看一下到底有多远。 TD则轻巧一点，先走一段路看一下，看一下有没有路牌指示到山脚还有多远。如果有，那么就把刚刚走的那段路加上路牌指示到山脚的距离相加即可。 但又同学可能会问，在一开始，我们根本没有路牌呀，所以也不知道到底到山脚有多远。 没错，这是对的。但当我们走很多次的时候，路牌系统就能慢慢建立起来。 例如第一次，只有到了山脚，我才知道山脚前一站离山脚的的真实距离。于是我更新了山脚前一站的路牌。第二次，我在山脚前一站路就能看到路牌，所以我就可以更新山脚前一站的路牌了…一直到山顶，就这样一直建立整座山的路牌系统。</p><h3 id="更新公式"><a href="#更新公式" class="headerlink" title="更新公式"></a>更新公式</h3><p>刚刚我们对TD有个直观的理解：TD并走走完整段路程，而是半路就截断。用半路的路牌，更新当前的路牌。所以我们只需要把MC的更新目标，改为TD的更新目标即可。在MC，G是更新目标，而在TD，我们只不过把更新目标从G，改成$r+gamma*V$</p><p><img src="/2023/05/06/rl-02/image-20230512105624840.png" alt="image-20230512105624840" style="zoom:67%;"></p><h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;上一节中我们介绍了马尔科夫链和在强化学习中Q值和V值的定义，那么在实际训练的时候，我们怎么去计算一个节点的Q值或者V值呢，这就要提到著名的蒙地卡罗算法（mc）和时序差分算法（td）&lt;/p&gt;
&lt;h2 id=&quot;蒙地卡罗&quot;&gt;&lt;a href=&quot;#蒙地卡罗&quot; class=&quot;heade</summary>
      
    
    
    
    <category term="Notes" scheme="http://kerwinblog.top/categories/Notes/"/>
    
    
    <category term="RL" scheme="http://kerwinblog.top/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>强化学习笔记1————马尔科夫链与Q值和V值</title>
    <link href="http://kerwinblog.top/2023/05/04/rl-01/"/>
    <id>http://kerwinblog.top/2023/05/04/rl-01/</id>
    <published>2023-05-04T13:34:21.000Z</published>
    <updated>2023-05-11T15:21:54.035Z</updated>
    
    <content type="html"><![CDATA[<p>最近工作上的事情比较少，有时间来学一下自己一直比较感兴趣的强化学习，之前多次有想法去系统的学习一下，但都因为其他事情中断了，这次打算趁着这段时间比较空闲，把强化学习入个门，为了督促自己学下去，开个坑记录一下，争取用1-2周完成这个系列。</p><p>网上的教程大多数原自David Silver大神的课程（<a href="https://www.bilibili.com/video/av45357759/?from=search&amp;seid=14511237738925991591">B站链接</a>），各种翻译水平层次不齐，看到不好的教程直接把人劝退。在广泛的搜罗后发现知乎有一位大神的讲解很好，本系列也是结合视频以及他的文章所做的学习笔记。<a href="https://zhuanlan.zhihu.com/p/111895463">原文链接<span id="more"></span></a></p><h1 id="马尔科夫链">马尔科夫链</h1><p>谈起强化学习就一定离不开马尔科夫链，就像谈起神经网络就离不开神经元一样。作为在大学《随机课程》里面就接触到过的概念，这里理解起来还是比较容易的的，这里的简要的回顾一下。</p><p>70年代的时候，数学家们为了更好的研究强化学习问题，提出了重要的假设——<strong>马尔科夫性</strong>（Markovproperty），它讲的是系统的下一个状态<span class="math inline">\(S_{t+1}\)</span>仅与当前状态<span class="math inline">\(S_t\)</span>有关，而与之前的所有状态无关，即：<span class="math display">\[P[s_{t+1}|s_t]=P[S_{t+1}|s_1,s_2,...s_t]\]</span>这个公式强大的地方在于我们可以把历史全部甩去，只对当前状态负责任，注意到上式是一个条件概率，也就是说：我当前看到一帧图像，我未来可能会看到的下一帧图像的概率，等于从我历史上看到的所有图像、到下一帧图像的概率；以打砖块为例，我当前把这个游戏打成这个德行，它可能是有很多很多次操作促成的，但没关系，我都不管，未来我要怎么操作，只和我当前的德行有关，至于过去是怎么做的，我完全不管。因为有了马尔科夫性，我当前概率由很多的历史状态过来，它们都有相应的概率，于是就可以将这些概率展开，直接进行状态向前推移或回退的计算。就算状态数量很多，我至少“扔掉了历史的包袱”。</p><figure><img src="/2023/05/04/rl-01/v2-44177b73b63d6ee182d574a1aeaa28ac_720w.jpg" alt="怎样正确理解马尔科夫链？"><figcaption aria-hidden="true">怎样正确理解马尔科夫链？</figcaption></figure><p>在强化学习中，马尔科夫链的经典表达图例这样的。在马尔科夫链中，有三个重要的元素：S，A，R。我们分别来看一下，他们代表的是什么。然后大家就会明白，为什么马尔科夫链是一个很好很常用的模型。</p><ol type="1"><li>智能体在环境中，观察到状态(S)；</li><li>状态(S)被输入到智能体，智能体经过计算，选择动作(A);</li><li>动作(A)使智能体进入另外一个状态(S)，并返回奖励(R)给智能体。</li><li>智能体根据返回，调整自己的策略。重复以上步骤，一步一步创造马尔科夫链。</li></ol><h3 id="总结">总结</h3><ol type="1"><li>马尔科夫链是用来描述智能体和环境互动的过程。</li><li>马尔科夫链包含三要素：state，action，reward</li></ol><ul><li>state：只有智能体能够观察到的才是state。</li><li>action：智能体的动作。</li><li>reward：引导智能体工作学习的主观的值。</li><li>马尔科夫链的不确定性</li><li>核心思想：如果你不希望孩子有某种行为，那么当这种行为出现的时候就进行惩罚。如果你希望孩子坚持某种行为，那么就进行奖励。这也是整个强化学习的核心思想</li></ul><h1 id="v值和q值">V值和Q值</h1><p>学习V值和Q值之前牢记一句话：<strong>V是对状态节点的估算，Q是对动作节点的估算</strong>。其中，我们估算的是，从该节点，一直到最终状态，能够获得的奖励的总和的平均值。</p><figure><img src="/2023/05/04/rl-01/image-20230511231143059.png" alt="image-20230511231143059"><figcaption aria-hidden="true">image-20230511231143059</figcaption></figure><p><strong>V值的定义</strong></p><p>从V值的计算，我们可以知道，V值代表了这个状态的今后能获得奖励的期望。从这个状态出发，到达最终状态，平均而言能拿到多少奖励。所以我们轻易比较两个状态的价值。V值跟我们选择的策略有很大的关系。<strong>会根据不同的策略有所变化</strong>！具体来说，会更加每个测量选择的概率不同而不同，因为最后算期望的的时候，其实是各个Q值的概率加权平均。</p><p><strong>Q值的定义</strong></p><p>如果了解V值的定义，那么理解Q值也不会有什么困难。Q值和V值的概念是一致的，都是衡量在马可洛夫树上某一个节点的价值。只不过V值衡量的是状态节点的价值，而Q值衡量的是动作节点的价值。用大白话总结就是：从某个状态选取动作A，走到最终状态很多很多次；最终获得奖励总和的平均值，就是Q值。<strong>【敲黑板】</strong>与V值不同，Q值和策略并没有直接相关，而与环境的状态转移概率相关，而环境的状态转移概率是不变的。</p><h2 id="v值和q值关系">V值和Q值关系</h2><p>总结一下，从以上的定义，我们可以知道Q值和V值的意义相通的： 1.都是马可洛夫树上的节点； 2. 价值评价的方式是一样的： - 从当前节点出发 -一直走到最终节点-所有的奖励的期望值.所以，其实Q和V之间是可以相互换算的。</p><p><strong>从Q到V</strong></p><p>我们先来看看，怎样用Q值算V值。</p><p>从定义出发，我们要求的V值，就是从状态S出发，到最终获取的所获得的奖励总和的期望值。也就是蓝色框部分。</p><p>S状态下有若干个动作，每个动作的Q值，就是从这个动作之后所获得的奖励总和的期望值。也就是红色框部分。</p><p>假设我们已经计算出每个动作的Q值，那么在计算V值的时候就不需要一直走到最终状态了，只需要走到动作节点，看一下每个动作节点的Q值，根据策略，计算Q的期望就是V值了。</p><figure><img src="/2023/05/04/rl-01/image-20230511230401913.png" alt="image-20230511230401913"><figcaption aria-hidden="true">image-20230511230401913</figcaption></figure><p>大白话就是：一个状态的V值，就是这个状态下的所有动作的Q值，在策略下的期望。</p><p><strong>从V到Q</strong></p><p>现在我们换个角度，看一下怎样从V换算成Q值。</p><figure><img src="/2023/05/04/rl-01/image-20230511230542554.png" alt="image-20230511230542554"><figcaption aria-hidden="true">image-20230511230542554</figcaption></figure><p>道理还是一样，就是用Q就是V的期望！而且这里不需要关注策略，这里是环境的状态转移概率决定的。注意当我们选择A，并转移到新的状态时，就能获得奖励，我们必须把这个<strong>奖励也算上！</strong></p><figure><img src="/2023/05/04/rl-01/image-20230511230502923.png" alt="image-20230511230502923"><figcaption aria-hidden="true">image-20230511230502923</figcaption></figure><p>其中，<strong>折扣率</strong>：在强化学习中，有某些参数是人为<strong>主观</strong>制定。这些参数并不能推导，但在实际应用中却能解决问题，所以我们称这些参数为<strong>超参数</strong>，而折扣率就是一个超参数。与金融产品说的贴现率是类似的。我们计算Q值，目的就是把未来很多步奖励，折算到当前节点。但未来n步的奖励的10点奖励，与当前的10点奖励是否完全等价呢？未必。所以我们人为地给未来的奖励一定的折扣，例如：0.9,0.8，然后在计算到当前的Q值。</p><p>现在我们知道如何从V到Q，从Q到V了。但实际应用中，我们更多会从V到V。</p><p>但其实从V到V也是很简单的。把公式代进去就可以了。</p><figure><img src="/2023/05/04/rl-01/image-20230511231341431.png" alt="image-20230511231341431"><figcaption aria-hidden="true">image-20230511231341431</figcaption></figure><h3 id="总结-1">总结</h3><ol type="1"><li>比起记住公式，其实我们更应该注意Q值和V值的意义：他们就像一个路牌一样，告诉我们从马可洛夫树的一个节点出发，下面所有节点的收获的期望值。也就是假设从这个节点开始，走许多许多次，最终获取的奖励的平均值。</li><li>V就是子节点的Q的期望！但要注意V值和策略相关。</li><li>Q就是子节点的V的期望！但要注意，记得把R计算在内。</li></ol><p>目前来看，计算某一个节点的Q值和V值，需要许多次试验，取其中的平均值。但实际上，我们不但需要求一个节点的值，而是求所有节点的值。如果我们每一个节点都用同样的方法，消耗必然会很大。所以人们发明了许多方式去计算Q值和V值，基于价值计算的算法就是围绕Q和V展开的。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近工作上的事情比较少，有时间来学一下自己一直比较感兴趣的强化学习，之前多次有想法去系统的学习一下，但都因为其他事情中断了，这次打算趁着这段时间比较空闲，把强化学习入个门，为了督促自己学下去，开个坑记录一下，争取用1-2周完成这个系列。&lt;/p&gt;
&lt;p&gt;网上的教程大多数原自David Silver大神的课程（&lt;a href=&quot;https://www.bilibili.com/video/av45357759/?from=search&amp;amp;seid=14511237738925991591&quot;&gt;B站链接&lt;/a&gt;），各种翻译水平层次不齐，看到不好的教程直接把人劝退。在广泛的搜罗后发现知乎有一位大神的讲解很好，本系列也是结合视频以及他的文章所做的学习笔记。&lt;a href=&quot;https://zhuanlan.zhihu.com/p/111895463&quot;&gt;原文链接&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Notes" scheme="http://kerwinblog.top/categories/Notes/"/>
    
    
    <category term="RL" scheme="http://kerwinblog.top/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>Let’s think step by step</title>
    <link href="http://kerwinblog.top/2023/05/03/0503/"/>
    <id>http://kerwinblog.top/2023/05/03/0503/</id>
    <published>2023-05-02T19:11:02.000Z</published>
    <updated>2023-05-10T11:01:25.349Z</updated>
    
    <content type="html"><![CDATA[<p>最近large language model 非常火，而CoT（Chain ofThought）作为一种和LLM模型交互的方式一经提出就引发了广泛的关注。这篇文章就主要介绍了通过CoT的方法来使用大模型解决推理任务的一些范式。）<span id="more"></span>本文参考视频（<a href="https://www.bilibili.com/video/BV1t8411e7Ug/?spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=8c6b292e0d1364ecdc0fe7062157584c">链接</a>）</p><h1 id="背景知识">背景知识</h1><p><img src="/2023/05/03/0503/987b9d2de91b325ce76eb9b171dc9342b4d88e3b.png@888w_!web-note.webp" alt="img" style="zoom:50%;"></p><p>这篇文章中说，只要在每个答案之前加上一句“Let's think step bystep”，就可以立即在两个比较困难的数学问题数据上涨点，而且涨点非常明显。由于这个方法比较简单，只是加了一句话就能够非常明显地涨点，所以立即引发了大家对于这一领域的关注，也就是“AI 是不是也需要鼓励来获得更好的表现”。</p><h2 id="语言模型的本质是对任意一段文本序列的概率进行建模"><strong>语言模型的本质是对任意一段文本序列的概率进行建模</strong></h2><p>如果将语言模型看成一个大黑盒的话，它的输入是一段文本序列，输出也是一段文本序列，通过训练语言模型，就能使得给定的文本序列和输出的文本序列拼接起来所组成的一整段文本序列的概率尽可能比较大的。</p><h1 id="方法">方法</h1><h2 id="如何用-gpt-3-这类的大语言模型来做零样本单样本和少样本学习"><strong>如何用GPT-3 这类的大语言模型来做零样本、单样本和少样本学习?</strong></h2><p><img src="/2023/05/03/0503/5c97101ebf5f629163900389c9e957e8176e651b.png@888w_!web-note.webp" alt="img" style="zoom:67%;"></p><p>对于 GPT-3 来说，也就是图中的 transformerdecoder，无论是在零样本、单样本还是少样本的情况下，它们的输入都是一段文本序列，输出也是一段文本序列</p><p>少样本与零样本的唯一区别就是中间多出了一些参考样例，它们其实都是在续写前缀（只是零样本的输入没有任何参考，而少样本的输入有一些参考样例来帮助语言模型推断如何根据任务输入生成相应的任务输出）</p><h2 id="用一个训练好的大语言模型来求解推理任务的几种范式"><strong>用一个训练好的大语言模型来求解推理任务的几种范式</strong></h2><h3 id="zero-shot">Zero-shot</h3><p><img src="/2023/05/03/0503/a729244b69ed8cd332c9ec68e0c7d6601a163eae.png@888w_!web-note.webp" alt="img" style="zoom:50%;"></p><p>文献：<a href="https://arxiv.org/abs/2205.11916">Large LanguageModels are Zero-Shot Reasoners</a></p><p>语言模型的输入是一道数学题连接一个字符串“The answeris”，然后让语言模型进行续写</p><h3 id="zero-shot-cot"><strong>Zero-Shot-CoT</strong></h3><p><img src="/2023/05/03/0503/63e3df153d21668cd25c5bdfc1978087ef7de086.png@888w_!web-note.webp" alt="img" style="zoom:50%;"></p><p>语言模型的输入还是一道数学题连接一个字符串“Let's think step bystep”，然后让语言模型进行续写</p><p>这种情况下，语言模型会续写出中间推理步骤，并最终生成答案。</p><p>chain of thought的定义：在应对推理任务时，在给出最终答案之前所产生的中间推理步骤，他们载体是一系列的短句子</p><p>chain of thought也可以和最后的答案合在一起，作为一个整体。但是作者还是将中间解题步骤叫做CoT，这样才能更好地表达模拟人类一步一步思考而最终得出答案的过程这一内涵</p><p>因为 CoT 是作者所提出的一个新事物，所以作者强调了 CoT中几个比较有意思的地方</p><p>首先，CoT原则上能够让模型把一个多步的问题分解出各种中间步骤，使那些具有更多推理步的问题有机会分配到更多的计算量（如果是从最后的将拼接好的问题、答案样例以及所要求解的问题和前缀输入到语言模型中产生最后的答案这一步来看，对于一个更难的问题，在续写的时候，CoT就使得语言模型能够产生更多的中间推理步骤，因为语言模型在生成输出的时候是一个一个token 进行生成的，那么如果问题越难，CoT又使得生成的中间步骤越多，那么整体上生成的 token的数量也会越多，自然而然在求解更难的问题的时候就会使用到更多的计算量。就好比人类在遇到更难得问题的时候，可能就会耗费更多的脑力，这样CoT 也能够让计算机能够对更难的问题分配更多的计算资源）</p><p>CoT提供了可解释性，也就是在不知道答案的情况下，也能够知道答案是怎样得来的，也就是所谓的中间推理步骤</p><p>作者认为 CoT在原则上能够适用于任何人类能够用语言所能解决的问题，而不仅仅是数学、逻辑、常识这类的问题。因为CoT 本身的载体就是一系列的短句子，本身也是人类语言</p><p>当一个语言模型训练好之后，就能够通过 few-shot prompting这种范式，在每个样例中写上中间推理步骤，再拼接好所要求解的问题输入到语言模型，就能够引发语言模型续写中间推理步骤，再得出最后的答案（像Zero-Shot CoT 就发现，甚至都不需要在 few-shot 这些样例中添加 CoT，可以仅凭“let's think step by step”作为 CoT 的推理；而 Auto CoT，也就是“Let's think not just step by step but one byone”使用了多个“let's think step by step”就可以自动地构造 few-shot的样例，从而弥补了 Zero-shot 和 Few-shot 之间的性能差异）。</p><h3 id="manual-cot">Manual-CoT</h3><p><img src="/2023/05/03/0503/545a1f4e8b6fd2e1c039affdfe093fe8df6e6735.png@888w_!web-note.webp" alt="img" style="zoom:50%;"></p><p>文献：<a href="https://arxiv.org/abs/2201.11903">Chain of ThoughtPrompting Elicits Reasoning in Large Language Models</a></p><p>这种情况下使用到了少样本学习，在输入问题之前，手动设计一些问题和答案的样例（样例的答案给出中间推理步骤），这些问题和答案都需要手动构造，所以叫Manual-CoT</p><p>语言模型的输入是一些手动设计的问题和答案的参考样例连接一个真正需要求解的问题，然后让语言模型进行续写</p><p>这里少样本训练中的问题和答案的样例都需要人为构造并手动设计，因此为了和第四种自动CoT 做区分，这里称为 Manual-CoT</p><p>Manual-CoT 比 Zero-Shot-CoT 的性能要好，因为它采用的是 few shot，在输入中提供了一些问题、中间推理步骤以及答案的样例给语言模型进行参考。但是，提供这些样例需要进行人工设计，这就需要一定的人工成本</p><h3 id="auto-cot"><strong>Auto-CoT</strong></h3><p>文献：<a href="https://arxiv.org/abs/2210.03493">Automatic Chain ofthought Prompting in Large Language Models</a></p><p>Auto-CoT 其实也是受到了 Manual-CoT 的启发，既然Manual-CoT 比Zero-Shot-CoT的性能要好，而且性能好的关键就在于人工设计的问题、中间推理步骤和答案的样例，那么就可以考虑将这部分进行自动化，从而节省人工成本</p><p>实时发现是可行的，做法主要分为两步</p><ol type="1"><li>通过多样性选取有代表性的问题</li><li>对于每一个采样的问题拼接上“Let's think step by step”（类似于Zero-Shot-CoT）输入到语言模型，让语言模型生成中间推理步骤和答案，然后把这些所有采样的问题以及语言模型生成的中间推理步骤和答案全部拼接在一起，构成少样本学习的样例，最后再拼接上需要求解的问题一起输入到语言模型中进行续写</li></ol><p>最终模型续写出了中间的推理步骤以及答案，并且质量非常高</p><p>值得一提的是，在十个数据集上 Auto-CoT 是可以匹配甚至超越 Manual-CoT的性能，也就说明自动构造的 CoT的问题、中间推理步骤和答案样例比人工设计的还要好，而且还节省了人工成本</p><p>在 Auto-CoT 中，其实也是用到了很多个“Let's think step bystep”对每个采样的问题分别触发中间推理步骤和答案，这也是为什么叫它“Let'sthink not just step by step but also one byone”，也就是AI需要多鼓励几次</p><h1 id="总结">总结</h1><p>最近看到ChatGPT这么火，加班肝了一下GPT_1到GPT_3的文章，自己总结了下：</p><ul><li>GPT的主要卖点是用Decoder搞了一个大模型。</li><li>GPT2把战场转移到了Zeor-shot，在更大的数据上训出来了更大的模型</li><li>GPT3又用了更大的数据集和模型，并且提出few-shot会更好，还提出了in-context learning的概念。</li><li>Instruct GPT在GPT3的基础上用户RLHF做了强化学习。</li><li>Chart GPT 论文还没出，不过应该是在GPT3.5上做了类似InstructGPT类似的优化。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近large language model 非常火，而CoT（Chain of
Thought）作为一种和LLM模型交互的方式一经提出就引发了广泛的关注。这篇文章就主要介绍了通过CoT的方法来使用大模型解决推理任务的一些范式。）&lt;/p&gt;</summary>
    
    
    
    <category term="Notes" scheme="http://kerwinblog.top/categories/Notes/"/>
    
    
    <category term="Rec System" scheme="http://kerwinblog.top/tags/Rec-System/"/>
    
    <category term="LLM" scheme="http://kerwinblog.top/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Token Merging Your ViT But Faster</title>
    <link href="http://kerwinblog.top/2023/04/28/0402/"/>
    <id>http://kerwinblog.top/2023/04/28/0402/</id>
    <published>2023-04-27T19:11:02.000Z</published>
    <updated>2023-05-09T13:26:59.420Z</updated>
    
    <content type="html"><![CDATA[<p>Transformer 乘着最近大模型的井喷又火了一把，在NLP领域的统治地位好像已经无可撼动了，Transformer 模型使用了 Self-Attention 机制，<strong>不采用</strong> RNN 的<strong>顺序结构</strong>，使得模型<strong>可以并行化训练</strong>，而且能够<strong>拥有全局信息。</strong>记得几年前接触CV里面的Transformer结构<strong><a href="https://arxiv.org/abs/2010.04159">DETR</a></strong>和<a href="https://arxiv.org/abs/2010.11929"><strong>Vit</strong></a>，惊讶于他们的效果的同时始终觉得将图片tokens化的方式太不美观了，同时所需要的训练资源太大了，学生党根本尝试不了。最近看到一篇论文，提出了一种无需训练即可加速 ViT 模型，提高吞吐量的方法 <a href="https://arxiv.org/pdf/2210.09461.pdf">Token Merging (ToMe)</a>。ToMe 通过一种轻量化的匹配算法，逐步合并 ViT 内部的相似的 tokens，实现了在基本不损失性能的前提下，大幅提升 ViT 架构的吞吐量。<span id="more"></span></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>与卷积神经网络 (CNN) 相比，视觉 Transformer 模型 (ViT) 有一系列优良的性质，比如：</p><ul><li>Transformer 模型的 Attention 模块和 MLP 模块主要有矩阵乘法这种可以加速的操作构成。</li><li>Transformer 支持一些性能强大的自监督学习任务 (掩码图像建模 MAE 等等)。</li><li>Transformer 适配多种模态的输入数据 (图片，文本，音频等)。</li><li>Transformer 对于超大规模数据集 (ImageNet-22K) 的泛化性好，预训练之后的模型在下游任务中 (比如 ImageNet-1K 图像分类任务) 表现卓越。</li></ul><p>但是在资源受限的边缘设备 (如手机和无人机) 上实际运行 Transformer 不太友好，因为 Transformer 模型又相对较大的延时。一种常见的加速视觉 Transformer 模型的方法是对 token (图片 Patch) 进行剪枝，但是 token 剪枝的缺点有：</p><ol><li>需要额外的训练过程，对资源不友好。</li><li>token 剪枝限制了模型的实用性，当 token 数量随着输入的变化而发生变化时，无法进行批处理 (Batch Inference)。为了解决这个问题，大多数 token 剪枝的工作借助了 Mask，对冗余的 token 进行遮挡。但是这样的做法并没有真正剪去这些冗余的 token，使得这些方法并不能在实际业务中真正加速。</li><li>token 剪枝带来的信息损失限制了可以允许剪枝的 token 数量。</li></ol><p>另一种加速 ViT 的做法是对 token (图片 Patch) 进行融合。和本文方法最接近的 Token Pooling 使用了一个缓慢的基于 k-means 的方法，但是速度较慢，不适用于现成的模型。</p><p>本文希望做一个无需训练并且兼顾性能-速度权衡的 token 融合方法。因为其无需训练的优良属性，对于大模型将会非常友好。在训练过程中使用 ToMe，可以观察到训练速度增长，总训练时间缩短了一半。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="Token-Merging-的基本思路"><a href="#Token-Merging-的基本思路" class="headerlink" title="Token Merging 的基本思路"></a><strong>Token Merging 的基本思路</strong></h2><p>Token Merging 的基本思路是在一个 ViT 模型中间插入一些 token merging 的模块，希望把这些模块植入 ViT 以后，训练和推理的速度都有提升。基本作法是在每一个层之后减少$r$ 个 token，那么一个有$ L$ 层的 Transformer 模型从头到尾减少的 token 数量就是 $Lr$ 。这个 $r $值越高，减少的 token 数量就越多，但是精度也会越差。而且值得注意的是，无论一张输入图片有多少个 tokens，都会减少$Lr $个 token，而不是像上文的 token 剪枝算法那样使得 token 的数量动态变化。为什么这么设计呢？原因就是如上文所述当 token 数量随着输入的变化而发生变化时，无法进行批处理 (Batch Inference)，使得这些方法并不能在实际业务中真正加速。</p><p>如下图1所示是 Token Merging 的示意图，ToMe 的位置被插在 Attention 模块和 MLP 模块之间，因为作者希望借助 Attention 中的特征帮助决定该去融合哪些 tokens。</p><p><img src="/2023/04/28/0402/v2-1858db858eb14c5ed58d93fbce3fe7a9_720w.jpg" alt="img"></p><h3 id="什么样的-tokens-是相似的"><a href="#什么样的-tokens-是相似的" class="headerlink" title="什么样的 tokens 是相似的"></a><strong>什么样的 tokens 是相似的</strong></h3><p>根据上面的基本思路，要考虑的第1个问题是我们应该合并哪些 tokens，即什么样的 tokens 可以被认为是相似的 tokens？一种比较直接的想法是距离比较近的 tokens 是相似的，但是并不是最优解。</p><p>如下图2所示为消融实验结果，意在探索什么样的 tokens 是相似的。消融实验使用的模型是 MAE 训练策略下得到的 ViT-L/16 预训练模型 (acc: 85.96%, im/s: 93.3)，不再进行任何额外训练。使用$ r=8$ 合并，这将在网络的24层上逐渐移除 98% 的 tokens。</p><p>如左图所示为使用什么特征衡量相似度，作者发现<strong>使用 Key 来衡量相似度对性能最友好</strong>，因为 Attention 模块中的 Key 已经总结了每个 token 中包含的信息，以便用于 Attention 中的 dot-product 相似度。如右图所示为使用什么距离衡量相似度，作者发现<strong>使用余弦距离来衡量 token 之间的相似度</strong>可以获得最好的精度-速度权衡。</p><p>如下图3所示，把不同 head 的 Key 进行取平均操作，而不是拼接在一起，更有助于效率。</p><p><img src="/2023/04/28/0402/v2-c1391c9a28478d8263222c4812847de1_720w.jpg" alt="img"></p><h3 id="Token-Merging-的具体步骤：二分软匹配"><a href="#Token-Merging-的具体步骤：二分软匹配" class="headerlink" title="Token Merging 的具体步骤：二分软匹配"></a><strong>Token Merging 的具体步骤：二分软匹配</strong></h3><p>在定义了 tokens 的相似性之后，下面就需要一种快速的方法来确定要匹配哪些 tokens，以便在实际运行时能够快速将 tokens 的数量减少$ r$ 。这个过程对于延时的要求很高，因为在 ViT 模型中要对可能上千个 tokens 执行匹配 $L $次，所以这个匹配算法的运行时间必须完全可以忽略不计。</p><p><img src="/2023/04/28/0402/v2-89c18ef59b13443f788ffcdaaddfeaee_720w.jpg" alt="img"></p><ol><li><p>把 ToMe 模块输入的所有 tokens 分为相同大小的2个集合 $\mathbb{A},\mathbb{B}$ 。</p></li><li><p>把从集合$\mathbb{A}$ 中的每个 token 到 $\mathbb{B}$ 中与其最相似的 token 画一条边。</p></li><li><p>只留下最相似的$r $条边，其余删掉。</p></li><li>融合仍然相连的$r$条边 (特征取均值)。</li><li>把这两个集合拼在一起，得到 ToMe 模块的融合结果。</li></ol><h2 id="Token-Merging-的后续操作：调节注意力权重"><a href="#Token-Merging-的后续操作：调节注意力权重" class="headerlink" title="Token Merging 的后续操作：调节注意力权重"></a><strong>Token Merging 的后续操作：调节注意力权重</strong></h2><p>前文提到，ToMe 模块会融合 $r$个 token。在 ViT 模型里面，一个 token 代表输入图片的一个 Patch，比如输入图片有 $N $个 Patch，就是有 $N$个 token。Attention 矩阵的维度也是$ N\times N$ 的，它代表了 $N $个 Patch 之间的相关关系。但是现在我们融合了 $r $个 token 之后呢，Attention 矩阵的维度应该是 $(N−r)×(N−r)$ 的，融合了 token 之后，有的 Key 应该占的 Attention 比重大一些，因为它融合了多个 token 的信息。所以作者在这里定义了一个行向量$ {s}$ 。 $s\in \mathbb{R}^{1\times N}$ 是包含每个 token 大小 (token 所代表的 Patch 数量) 的行向量。通过上式将行向$ s$ 直接加在 Attention 矩阵上面，相当于是人为增加了有些 Key 的 attention weight，而这些 key 恰好是发生了融合的 Key。</p><p>到目前为止，已经能够直接向已经训练好的 ViT 模型中添加 ToMe 模块。使用 ToMe 模块进行训练虽然不是必须的，但是它可以减少准确度下降，并且加快训练速度。ToMe 模块本质上是 token 的均值操作，因此可以视为是一种池化操作 (Pooling)。因此，我们可以按照平均池化操作 (Average Pooling) 的方式进行反向传播。</p><h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>如下图13所示是在网络的结尾处的每个合并的 token 所对应的输入 Patch。可以发现，ToMe 方法造成的 token 融合的效果和分割很像。比如，在第2张图中，哈士奇的腿、身体和脸被合并到了不同的 token 中。在第3张图中，猴子的手、身体、脸、眼睛和嘴都被合并到了不同的 token 中。在最后1张图中，所有实例 (狗) 中相同的部分会被合并在一起。值得注意的是，与剪枝不同，ToMe 这种 token 融合的方法能够合并背景和前景中的大量冗余的 tokens，而且不丢失信息。</p><p><img src="/2023/04/28/0402/image-20230509201607336.png" alt="image-20230509201607336"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ToMe 是一个无需训练并且兼顾性能-速度权衡的 token 融合方法，意在缩减 ViT 模型中大量冗余的 tokens。Token Merging 的基本思路是在一个 ViT 模型中间插入一些 token merging 的模块，希望把这些模块植入 ViT 以后，训练和推理的速度都有提升。在图像和视频中多个模型的实验结果表明，这种 token 融合的方法能够合并背景和前景中的大量冗余的 tokens，提高 ViT 模型的吞吐量，而且不丢失信息。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Transformer 乘着最近大模型的井喷又火了一把，在NLP领域的统治地位好像已经无可撼动了，Transformer 模型使用了 Self-Attention 机制，&lt;strong&gt;不采用&lt;/strong&gt; RNN 的&lt;strong&gt;顺序结构&lt;/strong&gt;，使得模型&lt;strong&gt;可以并行化训练&lt;/strong&gt;，而且能够&lt;strong&gt;拥有全局信息。&lt;/strong&gt;记得几年前接触CV里面的Transformer结构&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.04159&quot;&gt;DETR&lt;/a&gt;&lt;/strong&gt;和&lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;&lt;strong&gt;Vit&lt;/strong&gt;&lt;/a&gt;，惊讶于他们的效果的同时始终觉得将图片tokens化的方式太不美观了，同时所需要的训练资源太大了，学生党根本尝试不了。最近看到一篇论文，提出了一种无需训练即可加速 ViT 模型，提高吞吐量的方法 &lt;a href=&quot;https://arxiv.org/pdf/2210.09461.pdf&quot;&gt;Token Merging (ToMe)&lt;/a&gt;。ToMe 通过一种轻量化的匹配算法，逐步合并 ViT 内部的相似的 tokens，实现了在基本不损失性能的前提下，大幅提升 ViT 架构的吞吐量。&lt;/p&gt;</summary>
    
    
    
    <category term="paper Reading" scheme="http://kerwinblog.top/categories/paper-Reading/"/>
    
    
    <category term="CV" scheme="http://kerwinblog.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>《Bid Optimization by Multivariable Control in Display Advertising》阅读笔记</title>
    <link href="http://kerwinblog.top/2023/04/15/BidOptimization/"/>
    <id>http://kerwinblog.top/2023/04/15/BidOptimization/</id>
    <published>2023-04-15T12:49:00.000Z</published>
    <updated>2023-05-25T11:23:00.657Z</updated>
    
    <content type="html"><![CDATA[<p>推荐与广告可以说是很多互联网公司的两个重要业务，其中推荐是为了 DAU的增长，或者说流量的增长，而广告则是利用这些流量进行变现。两者的要解决的问题也很相似，都是在每条流量到来的时候，要从一个庞大的候选集中选出topk 个候选返回，基本都采用 召回+精排的架构，中间还可能插入粗排，本质上都是在效果与工程之间做trade-off。<span id="more"></span></p><p>如果说两者技术上最大的diff，我认为是出价，因为在广告场景中引入了广告主(advertiser)这一角色，因此我们除了考虑用户体验，还需要满足金主爸爸们的诉求（如跑量、成本等），才能带来持续的收入增长，而金主爸爸们表达其诉求的最直接的手段就是出价，其含义就是愿意为每个click/convert 付出多少钱(truthful telling)。这带出来的就是 bidding这一研究领域，关于这个领域在 <a href="https://github.com/wnzhang/rtb-papers">rtb-papers</a>中有很多相关的 paper。</p><p>本文主要讲的是 2019 KDD 阿里的 <a href="https://arxiv.org/abs/1905.10928">Bid Optimization byMultivariable Control in Display Advertising</a>，这篇 paper解决了出价的两个的核心问题：<strong>出价公式和调价策略</strong>，从最优的出价公式的推导到出价控制器的构建，文章的总体的建模思路非常值得学习，整个推导的paradigm 能够推广到更一般的出价场景, 实践性也较强，推荐读原文。</p><p><strong>出价从技术上可认为主要由两大部分组成：出价公式和控制器</strong>，比如说常见的cpc 出价公式是 bid * ctr, 最常见的控制器则是 <a href="https://en.wikipedia.org/wiki/PID_controller#:~:text=A%20proportional–integral–derivative%20controller,applications%20requiring%20continuously%20modulated%20control.">PID</a>。出价公式我们能比较好理解，那为什么要控制器来调价而不是按照广告主给的出价来投呢？我认为主要有以下两个原因</p><ol type="1"><li><strong>为了满足广告主的各种诉求</strong>；如需要匀速投放时，即在一天内均匀地花完预算，这样就需要通过控制器来控制运算花费曲线趋势与大盘流量曲线的趋势保持一致；如需要保成本时，需要通过控制器在成本高了时压价,低了时提价；需要跑量同时允许在成本有略微上涨时，可以在成本可控的情况下更激进一点出出价</li><li><strong>ctr/cvr 的预估不是完全准确的</strong>；常见的 ctr/cvr高估时，容易导致超成本，因为这时候计算出来的 ecpm = bid×ctr×cvr也相当于是高估了；其根本原因是在 ctr/cvr 预估中没有一个绝对的 groundtruth, 我们能拿到的是点击/转化与否，但是要预估的则是点击/转化的概率</li></ol><p>前面提到，paper中主要讲了两部分内容，最优出价公式的推导和控制器，下面描述的内容也会主要从这两方面进行描述</p><h2 id="最优出价公式">最优出价公式</h2><p>paper中要解决的场景是在保住点击成本和预算时，最大化广告主的在所有参竞价值(value)，因此这个优化问题可写成如下形式</p><p><img src="/2023/04/15/BidOptimization/bid_optimization_LP1.jpg"></p><p>上式中的各个符号含义如下</p><ul><li><span class="math inline">\(N\)</span>,广告计划的总的可参竞次数(opportunities)</li><li><span class="math inline">\(x_i\)</span>, 第 i 次竞价获胜的概率</li><li><span class="math inline">\(wp_i\)</span> 第 i 次竞价的 winningprice，即 bid price 要大于等于这个值才能获胜</li><li><span class="math inline">\(B\)</span>, 计划的总预算</li><li><span class="math inline">\(C\)</span>, 计划设置的点击成本</li></ul><p>值得注意的是，我们<strong>不需要直接求解出上面的最优化问题的解，而只是需要求出取值为最优时的的解形式，然后作为最优出价公式</strong>，也就是说我们并不关心上面的<span class="math inline">\(x_i\)</span>的最优解取值，而关心的是其他变量满足什么样的形式时，<span class="math inline">\(x_i\)</span> 的解是最优的。</p><h3 id="paper-原始推导">paper 原始推导</h3><p>paper 通过最优化中的对偶理论将上面的原问题(primalproblem)转为对偶问题(dual problem)，如下图所示；对偶理论的详细描述可参考<a href="https://sites.math.washington.edu/~burke/crs/407/notes/section4.pdf">DualityTheory</a> 这个讲义</p><p><img src="/2023/04/15/BidOptimization/bid_optimization_dual_problem_LP2.jpg"></p><p>上图中的 <span class="math inline">\(p,q,r\)</span>都是对偶问题中变量，对应于原问题中的三类约束：预算，成本和对 <span class="math inline">\(x\)</span>的范围的约束，根据互补松弛定理(Complementary Slackness)，可得到下面两个式子,互补松弛定理的详细描述同样可参考上面的那个链接。</p><p><img src="/2023/04/15/BidOptimization/bidding_optimization_Complementary_Slackness.jpg"></p><p>上面的公式中 <span class="math inline">\(x^∗_i\)</span> 和 <span class="math inline">\(r^∗_i\)</span>分别表示原问题和对偶问题的最优解，后面带*上标的符号均表示最优解（这里论文的公式有一个错误，8式中的<span class="math inline">\(p,q,r_i\)</span>实际上应该是<span class="math inline">\(p^∗,q^∗,r_i^∗\)</span>），至此为止，上面都是基于最优化理论推导出来的一些公式，但是<strong>接下来这一步就有点跳了</strong>，paper中直接令最优出价公式为 <span class="math display">\[bid^* = \frac{1}{p^*+q^*}\times CTR_i \timesCVR_i+\frac{q^*}{p^*+q^*}\times C \times CTR_i\]</span> 则上面那些公式中表示给广告主带来的价值 <span class="math inline">\(v_i\)</span> 可写成 <span class="math inline">\(v_i=bid_i−wp_i\)</span>,将公式（6）代入这个式子，再将 <span class="math inline">\(v_i\)</span>代入上图中的公式(8)，可得到下面的公式（10）及其分类讨论的结果</p><p><img src="/2023/04/15/BidOptimization/bid_optimization_classify_situation.jpg"></p><p>上面的两个分类讨论的结果实际上表明了<strong>无论最优解 <span class="math inline">\(x^*\)</span> 是赢得这次竞价(<span class="math inline">\(x=1\)</span>)还是输掉这次竞价(<span class="math inline">\(x=0\)</span>)，按照公式(6)进行出价时，总能保证解是最优的</strong></p><h3 id="小结">小结</h3><p>综合考虑上面两部分的推导，可以得到最优的出价公式如上面公式(6)所示，公式中的<span class="math inline">\(p\)</span> 和 <span class="math inline">\(q\)</span>是两个超参，是对偶问题中需要求解的变量，<strong>如果需要求解，意味着需要拿到参竞的所有后验数据，但是实际中在参竞时就需要通过这些参数给出参竞的bid，这似乎就成了一个先有鸡还是先有蛋的问题了</strong>，后面会通过控制器描述如何解决这一问题，具体思想就是不直接求解原始的最优化问题，而是通过近似的方式来逐渐逼近最优解</p><p>回到公式(6)的最优出价公式，如果将其写成 <code>c_bid * ctr</code>的形式，有如下公式</p><p><img src="/2023/04/15/BidOptimization/bid_optimization_cbid.jpg"></p><h2 id="调价策略">调价策略</h2>]]></content>
    
    
    <summary type="html">&lt;p&gt;推荐与广告可以说是很多互联网公司的两个重要业务，其中推荐是为了 DAU
的增长，或者说流量的增长，而广告则是利用这些流量进行变现。两者的要解决的问题也很相似，都是在每条流量到来的时候，要从一个庞大的候选集中选出
topk 个候选返回，基本都采用 召回+精排
的架构，中间还可能插入粗排，本质上都是在效果与工程之间做
trade-off。&lt;/p&gt;</summary>
    
    
    
    <category term="paper Reading" scheme="http://kerwinblog.top/categories/paper-Reading/"/>
    
    
    <category term="广告" scheme="http://kerwinblog.top/tags/%E5%B9%BF%E5%91%8A/"/>
    
    <category term="DL" scheme="http://kerwinblog.top/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>Position bias</title>
    <link href="http://kerwinblog.top/2023/03/19/0319/"/>
    <id>http://kerwinblog.top/2023/03/19/0319/</id>
    <published>2023-03-18T19:11:02.000Z</published>
    <updated>2023-05-07T13:37:29.454Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景">背景</h1><p>CTR预估模型的训练通常采用曝光点击数据，该数据是一种隐式反馈数据，所以会不可避免地产生各种偏差问题，其中，位置偏差对CTR的影响极大而备受关注。用户通常倾向于点击靠前位置的商品，并且CTR会随着曝光位置的增大而迅速下降。因此，直接在曝光数据上进行训练，模型会偏向于靠前位置的商品集合，造成位置偏差的问题。<span id="more"></span></p><h1 id="现有方案尝试">现有方案尝试</h1><h2 id="position作为特征">position作为特征</h2><p>这类方法比较简单，但是在训练、预测的时候还是有很多需要注意的细节，在特征中加入position特征通常有两种方式：</p><ol type="1"><li><p>直接将position作为普通特征加入；</p></li><li><p>将position特征单独构建网络，如利用biasNet建模用户的position bias然后concat到主网络，或者如YouTube的shallowtower的方式输出对应的logit然后和maintower的logit相加。另外，为了避免过度依赖位置特征，YouTube训练的时候在shallowtower设置drop-out rate 为10%。此时，无论在biasNet还是在shallowtower中，除了positionbias信息，也可以额外增加如设备信息等，或者建模用户自身的bias（不同用户具有不同的点击率）等。</p><p><img src="/2023/03/19/0319/6638ee4bd90bbabe13ba4cc62b3799b0.png" alt="6638ee4bd90bbabe13ba4cc62b3799b0.png" style="zoom:50%;"></p></li></ol><p>在预测时，由于商品在送入模型时没有position。通常的做法有以下几种：</p><ol type="1"><li>将position设置为missing，如上文提到的shallow tower。</li><li>给position设置不同的默认值。可以根据线上效果分别尝试然后选择最优的position，通常取position=1，因为离线训练的时候position-1的样本较多。但是不同的position值可能会导致完全不同的推荐结果，并且会影响线上打分的分布（如取position=1会使得整体的打分偏高）。因此这种方式是次优的，并且由于场景特性不同，不具备泛化性。</li><li>暴力遍历法，计算每个item在所有位置的分数，然后通过贪心算法等方式进行排序，但缺点时时间开销过大。</li></ol><h2 id="position作为模块">position作为模块</h2><p>这类方法的主要思路是将position单独构建成一个模块，但position模块不参与主网络模块打分计算，然后融合position模块和主网络模块的结果（如两个模块输出的prediction相乘）参与loss的计算，如华为的PAL。</p><p><img src="/2023/03/19/0319/484a9c208df843f73210dcba71aa8d4b.png" alt="484a9c208df843f73210dcba71aa8d4b.png" style="zoom: 50%;"></p><p>如上图，离线训练单独对position进行建模，将position输出的logit经过sigmoid得到分数probSeen（建模用户看到position的概率），主网络的logit经过sigmoid后得到分数pCTR（建模无偏的打分概率）。然后两个分数相乘得到bCTR，将bCTR与真实的label计算损失。这其实是一种类似PropensityScore的思想。</p><p>在预测时，这种方式直接采用主网络的打分pCTR就可以了。</p><h2 id="蒸馏模型">蒸馏模型</h2><p>通过蒸馏的方式也可以使模型学习到positionbias信息。主要有两种解决方法，一种是用一份无偏的数据集去训练一个TeacherNetwork，然后用知识蒸馏的方式指导学生网络，但是这种无偏的数据集在业界往往很难获取。另一种方案是使用<a href="https://arxiv.org/abs/1907.05171">优势特征蒸馏</a>的方式，将position作为优势特征，教师网络和学生网络使用相同的模型结构，但教师网络增加position特征。</p><figure><img src="/2023/03/19/0319/image-20230505221522918.png" alt="image-20230505221522918"><figcaption aria-hidden="true">image-20230505221522918</figcaption></figure><h2 id="multi-head-position">Multi Head Position</h2><p>在上文将position作为特征时我们提到了暴力遍历法，这种方案在线上部署时主要的耗时部分在于主网络的重复推理，那么我们其实可以改造一下网络结构，使得主网络在一次推理的情况下就可以输出所有position的打分不就行啦？</p><p>网络的结构也很简单，在主网络后面为每个position增加子网络，训练时每个样本只会更新对应position的子网络结构，<a href="https://arxiv.org/pdf/2106.05482.pdf">美团的DPIN</a>就是如此，直接通过模型预估出item在每个position上的分数，然后线上通过贪心算法实现排序。</p><p><img src="/2023/03/19/0319/image-20230505224406761.png" alt="image-20230505224406761"></p><p>上图展示的是整体的网络结构。左下角的basemodel完成的功能就是对item，user和context信息进行抽象提取，模型就是很简单的特征拼在一起过MLP。中间的部分文中称为DeepPosition-wise InteractionModules，是主体部分。在这一部分如何得到每一个位置上的信息建模呢？把用户曾经在该位置产生的正向行为的序列建模，作为该位置信息的抽象。即position1这里，仅仅使用用户在1号位置点击过的item的特征建模，灰色这里夹杂了一个小attention，是用context作为key启动的。到了“TransformerBlock”下面的这些特征，就已经是每一个对应位置的建模了。这里的transformerblock把QKV都设定成了所有位置特征的拼接，然后得到更加综合的表示。我个人理解，把用户行为拆到每一个位置上当然对，但是这样下来每一个位置也很稀疏，这一步大综合相当于做了一个兜底，使得每一个位置最终输出的特征都不会太弱。有了这些结果，接下来就按照上面说的做组合，得到最终结果。</p><h1 id="新的尝试">新的尝试</h1><p>在尝试了以上大部分方法之后，在实际应用中我是对模型的结果做了一些调整，细节的地方不便透露，这里大概介绍一些模型的整体架构。</p><h2 id="模型结构">模型结构</h2><h3 id="mainnet">MainNet</h3><p>整体模型分为3个主要部分，其中，MainNet的输入主要是用户基础特征、商品特征以及用户序列特征，输出主logits，因为基础模块的推理复杂度较大，因此这部分在网络底层，每次推理只需要执行一次。</p><h3 id="position-interaction">Position Interaction</h3><p>这部分结构参考了DPIN，构建position和用户兴趣的深度交叉表示。但是由于我们样本中用户的行为并不是在同一个场景中收集的，无法利用用户在不同position下的行为序列（各场景的position信息存在差异）进行交叉，因此这里直接对所有position进行selfattention，用于表达不同position间的交叉关系，因此这个模块会输出每个位置的深度非线性交叉表达。由于移除了用户在不同position下的行为序列，这个模块在进行实验时带来的收益有限。</p><h3 id="positionnet">PositionNet</h3><p>positionNet位于网络的顶层，输入是主网络输出的主logits、每个position的表达以及每个position经过selfattention后的表达，输出是最终的预估分数。</p><h4 id="masked-position-logit">Masked position logit</h4><p>假设场景中的position有N个，我们在顶层网络会为每个position构建一个子网络positionsubnetwork，由于position的数量较多，因此设置了阈值截断n，n&lt;=N,设当前样本的position为i，当i&lt;=n时，会进入我们构建的第i个子网络，当i&gt;=n时，所有的样本会进入第n个子网络，因此我们一共设置了n个子网络。实际上根据线上的真实情况，大部分的流量都集中在topposition，因此我们只需要关注topposition的效果。此外，对于靠后position的样本量通常比较少，单独为每个position构建一个subnetwork的话由于样本不足，可能会存在无法充分训练的问题。因此将靠后的position合并到一个网络会改善这个问题。</p><p>position sub network 的输入包含了主网络输出的logits，对应positioni的向量表达以及对应position i 经过selfattention后的向量表达。对每个样本，在每个position sub network均会输出对应的logit，但在反向传播时我们只更新该样本对应的position的子网络，这部分通过positionmask实现，即经过positionmask后只有该样本对应的position的子网络的logit会被保留。</p><h4 id="joint-logit">Joint logit</h4><p>除了position sub network之外，我们额外增加了一个jointsubnet。该网络的输入只包含主网络输出的logits，并且所有position的样本都会更新这个子网络，这个网络的输出为jointlogit。</p><h4 id="final-logit">Final logit</h4><p>在最后的融合阶段，我们将每个position subnetwork输出的n个logit（注意这里不是经过mask后的logit）和jointsubnet输出的joint logit进行融合，最终输出finallogit。为了能够合理观察，同时分配给不同logit对应的权重，我们直接采用了单层网络进行线性融合。</p><h4 id="loss">Loss</h4><p>最终的loss为Masked position logit 、Joint logit以及Finallogit的和，损失函数为交叉熵函数。 <span class="math display">\[Loss = loss_{position\_masked}+loss_{joint}+loss_{final}\]</span></p><h1 id="总结">总结</h1><p>对positionbias的研究不仅有利于得到更加精准的CTR以提高推荐效果，在与广告进行混排的时候，由于涉及到广告计费，精准的CTR预估显得更加重要，感觉目前最大的难点就是如何解决不同position的样本差距的问题。希望对解决这方面问题有经验的大佬不吝赐教~</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;背景&lt;/h1&gt;
&lt;p&gt;CTR预估模型的训练通常采用曝光点击数据，该数据是一种隐式反馈数据，所以会不可避免地产生各种偏差问题，其中，位置偏差对CTR的影响极大而备受关注。用户通常倾向于点击靠前位置的商品，并且CTR会随着曝光位置的增大而迅速下降。因此，直接在曝光数据上进行训练，模型会偏向于靠前位置的商品集合，造成位置偏差的问题。&lt;/p&gt;</summary>
    
    
    
    <category term="Notes" scheme="http://kerwinblog.top/categories/Notes/"/>
    
    
    <category term="Rec System" scheme="http://kerwinblog.top/tags/Rec-System/"/>
    
  </entry>
  
  <entry>
    <title>混排到底该怎么排？</title>
    <link href="http://kerwinblog.top/2023/03/03/%E6%B7%B7%E6%8E%92%E6%8E%A2%E7%B4%A2/"/>
    <id>http://kerwinblog.top/2023/03/03/%E6%B7%B7%E6%8E%92%E6%8E%A2%E7%B4%A2/</id>
    <published>2023-03-03T13:56:49.000Z</published>
    <updated>2023-05-06T14:03:36.377Z</updated>
    
    <content type="html"><![CDATA[<p>混排，往往是的推荐系统的最后一个环节，在这个阶段，自然内容（后面简称item）需要与营销内容（后面简称ad）进行混合，生成最终推送给用户的 list</p><p>如果以 Long Term Value(LTV)的视角来看,这是个在 LT 和 V 之间做trade-off 的过程，ad 如果出得过多，必然会挤压 item的数量和位置，进而影响用户体验和留存即 LT，但相应的广告收入，或者说Average revenue per user(<a href="https://www.investopedia.com/terms/a/arpu.asp">ARPU</a>)会提升，反之亦然。<span id="more"></span></p><p>所以业界往往的做法是定一个用户体验的约束，在这个约束下尽可能优化 ad的效率，即达到收入最大化，因此很自然可以把这个建模成一个最优化问题，LinkedIn在 2020 年的这篇 paper就是这么做的，<a href="https://dl.acm.org/doi/pdf/10.1145/3394486.3403391">Ads Allocationin Feed via Constrained Optimization</a></p><p>直观地看混排这个问题，有 2 个子问题需要解决 （1）怎么计算每个 item 或ad 在每个位置上的价值：因为 item 和 ad是各自排序的，目标不同，最终的值的量纲也不同，这么把两者的 scale拉到可比范围是一个需要讨论的问题 （2）怎么分配能让最终 list价值最大化：在 item 和 ad 的价值确认后，怎么插入 item 和 ad的位置，从而达到整个 list 的最大化</p><p>上面提到的 LinkedIn 的 paper重点是在解决第二个问题，部分内容也涉及到第一个问题 ；本文会先讲一下这篇paper 的建模方法，然后讨论下计算 item 和 ad价值的一些思路，混排中一些其他需要注意的事项</p><h2 id="建模方案">建模方案</h2><p>paper把问题建模成如下图最优化问题（单次请求的最优，目前不考虑请求间的优化）</p><p><img src="/2023/03/03/%E6%B7%B7%E6%8E%92%E6%8E%A2%E7%B4%A2/Rerank_optimize.png"></p><p>各符号含义如下</p><ul><li><span class="math inline">\(i\)</span>: 一次请求中位置的 index</li><li><span class="math inline">\(x_i\)</span>: 是否在第 i个位置插入ad</li><li><span class="math inline">\(j\)</span>: 请求的 index</li><li><span class="math inline">\(u^o_i\)</span>: item 在第 i 个位置的engagement utility（可理解为内容本身的价值，item和ad都有）</li><li><span class="math inline">\(u^a_i\)</span>: ad 在第 i 个位置的engagement utility</li><li><span class="math inline">\(r_i\)</span>: ad 在第 i 个位置的 revenueutility（可理解为商业价值，item没有这个价值）</li><li><span class="math inline">\(C\)</span>: 全局 engagement utility的一个门槛, 一种可能的方式是设置成可能最大的 engagementutility（没有广告） 的一个百分比</li></ul><p>通过对偶拉格朗日可以求解出如下的解，式子中的 <span class="math inline">\(α\)</span>是上面第一个约束的拉格朗日乘数；这个变量的物理含义是一个bid，paper 称其为 “shadow bid”，作用是把 engagement utility 的 scale变换至 revenue utility 的 scale；则最终在位置 i 插入 ad 或 item的价值如下图表 1 所示</p><p><img src="/2023/03/03/%E6%B7%B7%E6%8E%92%E6%8E%A2%E7%B4%A2/Rerank_solution.png" alt="img" style="zoom: 67%;"></p><p>上面的最优化问题的约束只是总体 list 的 engagement utility要大于特定预制，但混排往往还有一些硬约束，在paper中提到的是：top slot 和min gap，分别表示第一个广告的位置约束，两个广告最小间隔的约束；除了这两个约束，一些常见的约束还有showtime gap 约束（出现广告的评率）、ad load约束等(广告出现比例的约束)</p><p><img src="/2023/03/03/%E6%B7%B7%E6%8E%92%E6%8E%A2%E7%B4%A2/image-20230423222916631.png"></p><p>这两个约束并没有直接体现在最优化的建模里，而是体现在最后的混排算法里，整个算法流程如下</p><p><img src="/2023/03/03/%E6%B7%B7%E6%8E%92%E6%8E%A2%E7%B4%A2/Rerank_algorithm.png" alt="img" style="zoom:50%;"></p><h2 id="建模关键问题讨论">建模关键问题讨论</h2><p>上面的建模虽然比较直观，但涉及到的一些需要解决的关键问题，</p><h3 id="shadow-bid-α-的计算">shadow bid <span class="math inline">\(α\)</span> 的计算</h3><p>paper 提到的 shadow bid，在经济学上称为<a href="https://zh.wikipedia.org/wiki/影子价格">影子价格</a>，物理含义是表示的是增加一个单位的资源所带来的边际收益，关于影子价格更多讨论可参考<a href="https://www.zhihu.com/question/23510001">线性规划中的影子价格怎么理解？</a></p><p>这里的影子价格 <span class="math inline">\(α\)</span>是一个关于 <span class="math inline">\(C\)</span> 的函数，paper提到获取这个值的三种方法，</p><p><img src="/2023/03/03/%E6%B7%B7%E6%8E%92%E6%8E%A2%E7%B4%A2/Rerank_shadowbid.png" alt="img" style="zoom:67%;"></p><p>从上图可知，基本思路是 （1）确定 <span class="math inline">\(C\)</span> 的值之后求解原问题，得到 <span class="math inline">\(α\)</span> （2）通过在线 ab 实验来确定这个参数（3）离线回放（感觉这里跟第一个是重合的，因为求解原问题也是需要回放历史数据，只是用多长的历史数据，以及更新频率有多大）</p><h3 id="uo_iua_i-和-r_i的计算"><span class="math inline">\(u^o_i\)</span>、<span class="math inline">\(u^a_i\)</span> 和 <span class="math inline">\(r_i\)</span>的计算</h3><p>关于这几个值的获取，paper 并没有提供明确的方法，只是提到了<strong><span class="math inline">\(u^a_i\)</span>、 <span class="math inline">\(u^o_i\)</span> and <span class="math inline">\(r_i\)</span> are drawn from the same distributionas was the historical data</strong></p><p>但如果真的这么做，存在的问题必然是历史数据会很稀疏，容易出现新的 item或 ad没有数据，或者把统计历史数据的维度拉得更大，这样容易导致效果变差，数据没区分性；因此，最终还是需要往预估方向去做</p><p>如果考虑实际的业务，<span class="math inline">\(u^o_i\)</span> 和<span class="math inline">\(r_i\)</span> 的值比较好获取，直接取原本 item排序和 ad 排序中各自的分数即可，但 <span class="math inline">\(u^a_i\)</span> 的值应该如何获取？（其实这里的<span class="math inline">\(u^o_i\)</span> 和 <span class="math inline">\(r_i\)</span>的值的获取还有个问题，就是怎么获取一个 item 或 ad 在所有位置的 <span class="math inline">\(u\)</span> 或 <span class="math inline">\(r\)</span>，这部分在下面的 position bias会讨论）</p><p>如果让 ad 直接走推荐侧的模型，在物理意义上是 make sense的，但可能会存在 2 个问题（1）ad 的特征未必能跟 item的完全对齐（2）需要保证 <span class="math inline">\(u^a_i\)</span>&lt;<span class="math inline">\(u^o_i\)</span></p><p>对于这里的第二个问题，笔者有点疑惑，在Feeds流中，肯定会有一些广告商品本身的自然效率也是很高的，感觉没必要进行一个这样的现在，更应该考虑的是，如果广告品自身能竞得某个位置，加上revenueutility可能会竞得一个更好的位置，那么如何根据这个改变来进行二价计费的修改，这个以后会介绍一些做法。</p><h3 id="position-bias">position bias</h3><p>这个问题在上面的 <span class="math inline">\(u^a_i\)</span> 、<span class="math inline">\(u^o_i\)</span> 和 <span class="math inline">\(r_i\)</span> 的获取讨论中提到了如果在计算 <span class="math inline">\(u^a_i\)</span> 、<span class="math inline">\(u^o_i\)</span> 和 <span class="math inline">\(r_i\)</span>不考虑位置信息，那必然是有偏的，因为不同位置的ctr、cvr 等天然不一致</p><p>paper 里提到了一种方法，也是实际比较常用的：<strong>training阶段使用实际的 position，serving 阶段使用统一的position，同时保留一张映射表，映射不同位置跟 serving 时使用的统一位置的discount，映射表可通过后验数据统计获取得到，最终预估值乘上这个 discount就能等到不同位置的预估值</strong></p><p>这种做法的缺点是这张映射表需要经常更新，所以更好的做法是把这张表做到模型里，让模型训练过程中就能学到这个变化，预估阶段同时预估所有的位置的score。</p><p>关于这个问题，笔者在工作中也做过一些尝试，并取得了不错的效果，后面有空可以介绍一下~</p><h3 id="gap-effect">gap effect</h3><p>对于推荐而言，往往序准确就可以了，但 a<strong>d因为涉及到实际扣费，会要求 ctr，cvr预估足够准确</strong>，才能避免扣费不准确，引起广告主的客诉等问题</p><p>除了上面的 position bias 会影响 ctr 准确性，两个 ad 之间的 gap大小也会影响 ad 的 ctr 等，如下图所示，ad 的 gap 之间如果过小，ad的点击率会过低，但 item 不会出现这种情况，本质上还是因为 ad的密集度过高，即使前面提到了有min-gap 这一类硬规则</p><p><img src="/2023/03/03/%E6%B7%B7%E6%8E%92%E6%8E%A2%E7%B4%A2/Rerank_gap_effect.jpg" alt="img" style="zoom:33%;"></p><p>paper 提出的做法是给增加一个 gap 特征来捕捉这个信息，paper做了如下推导，最终生效的形式跟 position_discount 有点像，等价于在原来的ctr 基础上乘上一个 gap_discount, 下图中的 <span class="math inline">\(g\)</span> 是 <a href="https://en.wikipedia.org/wiki/Logit">Logit function</a>, <span class="math inline">\(y_{ij}\)</span> 表示是否发生点击（还可进一步把<span class="math inline">\(β\)</span> 参数化，做成个性化的参数）</p><p><img src="/2023/03/03/%E6%B7%B7%E6%8E%92%E6%8E%A2%E7%B4%A2/Rerank_gap_effect_logit.jpg" alt="img" style="zoom:67%;"></p><p>而如果在上面的混排算法上加上 gap effect 的影响，会有如下的流程</p><p><img src="/2023/03/03/%E6%B7%B7%E6%8E%92%E6%8E%A2%E7%B4%A2/Rerank_gap_effect_algo.jpg" alt="img" style="zoom: 33%;"></p><h2 id="item-与-ad-价值度量的另一种思路">item 与 ad价值度量的另一种思路</h2><p>paper 认为 item 价值是 engagement utility，ad 的价值是 engagementutility + revenue utility</p><p>而上面也提到了获取 ad 的 engagement utility会比较难，因此可以也可以考虑使用另一种思路来度量 item 和 ad 的价值</p><p>因为涉及到扣费，ad 的价值是比较好衡量的，一般采用的是 ecpm = bid ×ctr ×cvr（为了讨论方便，此处省略hidden_cost），因此很自然的一个想法是，<strong>能否为每个item 也赋予一个 bid，这样也能在 item 侧算出一个类似 ecpm 的指标，与 ad侧能进行比较</strong></p><p>紧接着的问题是，item 侧的 bid 的含义是什么？ad 侧的 bid是有明确的物理含义的：广告主愿意为一个转化付的钱（还会叠加调价策略修改原始的广告主出价），但在item 侧并没有广告主这一角色，由谁来出这个 bid 呢？</p><p>最直观的方法就是基于 ad 和 item 各自预估值的量级的差异，拍一个 item侧的全局固定的 bid，但这样显然不是最优的，而且这个 bid的量级也需要及时监控和更新，因为随着迭代，两边的预估值的 sacle会发生变化</p><p>而如果从另一个角度来看，ad更多是表达广告主的诉求，目标就是要更多的用户转化；item更多是平台的诉求，目标是要为平台带来更多的用户和留存时间；留存时间越长，也意味着可供平台变现的流量会更多，或者说<strong>用户指标其实也是跟平台长期收入挂钩的</strong>；因此可以基于大盘来测算用户指标与平台收入的关系，以stay_duration 为例，可以对 stay_duration 分桶，测算 stay_duration与平台长期收入指标的相关关系，建立一个函数 <span class="math inline">\(f\)</span>使得 <span class="math inline">\(bid=f(stayduration)\)</span>当然这里的变量也不一定是 stay_duration，也可以是考虑 dislike、active等各种信号，关键是要<strong>把用户在平台的留存、活跃等信息，与平台长期收入量化挂钩，然后基于这个收入倒推出一个item 侧的 bid</strong></p><p>对于 ad 而言，如果与 item 混排时，把 ad 插在 items 的第 <span class="math inline">\(k\)</span>位带来的收益, 不仅仅是广告本身的ecpm，还可进一步考虑由于广告插入给总体 items 带来的损失（VCG计费的思想），即<strong>混排时 ad 的价值也可以考虑成“ad 带来收益 - item后移的损失”</strong>；这样的话如果插入的位置导致 item后移的损失较大，则总体的 ad 价值会下降，避免对 item 的挤压</p><h2 id="硬规则是否最优">硬规则是否最优</h2><p>上面提到了混排中存在着各种硬规则，比如说广告出现的首位，广告之间的min_gap，show_timegap等，从技术角度来说，这样的离散的硬规则显然不是最优的，会极大限制算法的搜索空间；而从业务的角度，这种硬规则往往是业务发展初期拍定的，随着业务发展，对当前业务是否合理也是需要重新评估的</p><p>举个例子，一个对 ad 敏感和一个对 ad不敏感的用户，使用同一套硬规则并不是最优的，因为同样的广告频率，在敏感用户那转化率会比较低，同时对用户的留存影响也会比较大（相对于不敏感的用户）；而如果给对ad 不敏感的用户出更多的 ad，同时减少敏感用户的ad，打平总体的 ad数，最终总体的 engagement utility 和 revenue utility理论上也是更优的</p><p>具体的实现上，一般需要设置一套规则和计算方法，能够计算出硬规则中的所有可能情况下的utility，然后加入到 list 的总体价值中</p><p>而这其实也涉及到推荐或广告系统里的优化的一个方向，就是个性化，一般大盘策略上对所有用户都相同的超参或策略，都会有个性化的空间</p><p>当然在这个过程也要注意个性化往往也会有个限制，以混排为例，需要对不敏感的用户有体验保护，不能逮住这部分用户一个劲地薅</p><h2 id="小结">小结</h2><p>本文从 LinkedIn 的一篇 paper展开，介绍了混排需要解决的问题以及一种建模方法，paper的建模不复杂，关键是建模中使用的各个 utility 的获取，paper在这部分并没有说得比较详细，可能也是因为这部分跟实际业务耦合比较紧密；因此，本文后面也讨论了一种item 和 ad 价值可能的计算方式：为 item 计算一个 bid， 同时考虑 ad 插入对item 的影响</p><p>另外，paper 也提到了 position bias 、gap effect等在混排中常见的问题，本文针对 paper提出的方法和业界的一些做法做了讨论</p><p>最后，也探讨了一个比较比较开放的问题，就是混排中涉及到的 hardrule，hard rule 一般是红线，但从技术视角来看肯定不是最优的，同时 hardrule 是否合理，以及如何从业务角度和技术角度来 soften the hard rule，拿到engagement utility 和 revenue utility两部分的收益，也许是个值得讨论的问题</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;混排，往往是的推荐系统的最后一个环节，在这个阶段，自然内容（后面简称item）需要与营销内容（后面简称
ad）进行混合，生成最终推送给用户的 list&lt;/p&gt;
&lt;p&gt;如果以 Long Term Value(LTV)的视角来看,这是个在 LT 和 V 之间做
trade-off 的过程，ad 如果出得过多，必然会挤压 item
的数量和位置，进而影响用户体验和留存即 LT，但相应的广告收入，或者说
Average revenue per user(&lt;a href=&quot;https://www.investopedia.com/terms/a/arpu.asp&quot;&gt;ARPU&lt;/a&gt;)
会提升，反之亦然。&lt;/p&gt;</summary>
    
    
    
    <category term="Notes" scheme="http://kerwinblog.top/categories/Notes/"/>
    
    
    <category term="Rec System" scheme="http://kerwinblog.top/tags/Rec-System/"/>
    
    <category term="广告" scheme="http://kerwinblog.top/tags/%E5%B9%BF%E5%91%8A/"/>
    
  </entry>
  
  <entry>
    <title>U-Net in U-Net for Infrared Small Object Detection</title>
    <link href="http://kerwinblog.top/2023/03/02/0302/"/>
    <id>http://kerwinblog.top/2023/03/02/0302/</id>
    <published>2023-03-02T06:31:02.000Z</published>
    <updated>2023-05-09T11:20:12.397Z</updated>
    
    <content type="html"><![CDATA[<h2 id="section"></h2><p>最近看到一篇文章比较有意思，是对在图像分割领域的"常青树"u-net做的改进，由于自己之前在实验室做过图像分割的项目，曾经也魔改了一版u-net，所以对相关的工作还是比较感兴趣的。这篇论文的题目就很吸引眼球，<a href="https://arxiv.org/pdf/2212.00968.pdf">UIU-Net</a>，在u-net里面套了一个u-net，话不多说，让我们来看看具体是怎么做的。<span id="more"></span></p><h1 id="背景">背景</h1><p>本文旨在研究红外小目标检测问题，并提出一种基于深度学习的U-net inU-net方法来提高检测性能，在这一部分，作者首先介绍了红外小目标检测的背景和相关研究。</p><p>红外小目标检测是一项重要的研究领域，在许多应用中都具有重要的研究价值，如军事侦查、目标跟踪、航空航天和安防等。与可见光相比，红外图像具有更高的对比度和更强的穿透力，可以穿越烟雾、雾霾等环境，且不受光线干扰。因此，红外图像可以更好地获取目标的信息，特别是在夜间和恶劣天气的条件下。</p><p>然而，红外小目标检测是一项具有挑战性的任务。由手红外小目标的特征不明显，目标尺寸小、形态不规则，加之背景复杂多变，对检测算法的要求较高。传统的基手特征工程的方法通常需要人工设计特征，此难以达到最佳效果。近年来，深度学习技术的发展为红外小目标检测带来了新的机遇。目前，已经有很多学者通过深度学习方法对红外小目标检测进行了研究。其中，一些深度学习方法包括基于卷积神经网络(CNN)的方法、基于循环神经网络（RNN)的方法、基于卷积和循环神经网(CNN-RNN)的方法等。这些方法在一定程度上提高了红外小目标检测的性能，但是仍然存在一些问题，如对小目标的检测灵敏度不高、对背景的适应性不够等。</p><p>因此，本文提出了一种新的U-Net inU-Net方法，用手提高红外小目标检测的性能。该方法利用U-Net网络的优势，堆叠两个U-Net网络，提取更好的特征和增强对小目标的检测灵敏度和准确性。同时，引入了多尺度图像金字塔结构和注意力机制，进一步提高了检测性能。该方法具有良好的可扩展性和通用性，可以应用于其他因像分割和目标检测任务。</p><figure><img src="/2023/03/02/0302/image-20230508193540253.png" alt="image-20230508193540253"><figcaption aria-hidden="true">image-20230508193540253</figcaption></figure><h1 id="方法">方法</h1><p>本文提出了一种名为U-Net inU-Net的方法，用于提高红外小目标的性能。该方法将两个U-Net网络嵌套在一起，用于提取更好的特征和增强对小标的检测灵敏度和准确性。本章节将从以下几个方面详细介绍该方法的具体实现步骤。</p><figure><img src="/2023/03/02/0302/image-20230508201832780.png" alt="image-20230508201832780"><figcaption aria-hidden="true">image-20230508201832780</figcaption></figure><h2 id="多尺度图像金字塔结构">多尺度图像金字塔结构</h2><p>在红外小目标检测中，目标的大小可能会发生变化，因此需要使用不同尺度的图像来进行处理。在U-NetinU-Net方法中，我们采用了一种多尺度图像金宇搭结构，用于提高对不同大小目标的处理能力。该结构包括一组不同尺寸的国像，以便网络可以处理不同大小的目标。在每个尺寸下，都使用了一个U-Net网络来提取特征，这些特征将被传递到下一尺寸的U-net网络中，以进一步提取和处理特征。通过使用多个不同尺度的图像，U-Netin U-Net方法可以更好地处理不同大小的目标，从而提高检测性能。</p><h2 id="注意力机制">注意力机制</h2><p>在红外小目标检测中，小目标的检测是一个具有挑战性的问题。为了增强对小目标的检测灵敏度，我们采用了一种注意力机制，以过滤掉不相关的特征，同时增强与小目标有关的特征。该机制引入了一种基于特征的门控机制，以过滤掉不相关的特征，并增强与小日标相关的特征。具体来说，我们通过卷积操作将特征图转换为一组特征向量。然后，通过使用门控机制来过滤不相关的特征，并增强与小目标相关的特征。最后，我们将过滤后的特征向量重新转换回特征图，以供后续处理使用。通过过滤不相关的特征，注意力机制可以使网络更好地关注与小目标相关的特征，从而提高对小目标的检测灵敏度和准确性。</p><figure><img src="/2023/03/02/0302/image-20230508202239905.png" alt="image-20230508202239905"><figcaption aria-hidden="true">image-20230508202239905</figcaption></figure><p>本文提出了一种Interactive-cross Attention (IC-A)Module，如上图所示。</p><ol type="1"><li>首先，将输入图像通过第一个U-net进行特征提取。</li><li>然后，将特征图像输入到IC-A模块中，该模块包含了一系列迭代的上下文聚合，用于提取不同尺度的特征。</li><li>在每个迭代中，IC-A模块将上一层的特征图像沿着不同的方向（上、下、左、右）进行卷积操作，并将得到的特征图像与上一层特征图像相加，以产生新的特征图像。这样可以在不同的尺度上捕获更多的上下文信息。</li><li>通过多次迭代，IC-A模块可以在不同的尺度上聚合上下文信息，从而获得更准确的检测结果。</li></ol><h2 id="u-net-in-u-net网络架构">U-Net in U-Net网络架构</h2><p>U-Net inU-Net方法堆叠了两个U-net网络，用于提取更好的特征和增强对小目标的检测灵敏厦和准确性。第一个U-Net网络用于提取图像的特征，第二个U-Net网络则用于小目标的检测。这两个网络都使用了卷积神经网络（CNN)夹提取特征，并通过跳跃连接进一步提高检测性能。第一个U-Net网络由编码器和解码器部分组成。编码器部分采用卷积和最大池化操作，用于提取特征。解码器部分则采用反卷积和上采样操作，将特征区映射回原始图像大小。这个网络的输出是特征图。第二个U-Net网络也由编码器和解码器两部分组成。编码器部分采用与第一个U-Net网络相同的卷积和最大池化操作。解码器部分也采用反卷积和上采样操作，将特征图映射回原始因像大小。不同的是，第二个U-Net网络的输出是概率图，用于表示每个像素点是否属于小目标。</p><p>在训练过程中，我们使用二元交叉熵损失函数来优化模型，计算网络输出概率图与标注图像之问的差异。我们使用Adam优化算法来更新网络参数，训练过程是一个端对端的过程，同时对两个U-Net网络进行优化。</p><p>在测试过程中，我们对每个尺度的图像进行检测，在不同尺度的检测结果上应用非极大值抑制(NMS)算法，以过滤掉重叠的检测结果。最终，我们将所有尺度的检测结果合并，得到最终的检测结果。</p><p>该方法的主要优点在于堆叠了两个U-Net网络，减少了对特征的丢失和信息的混淆。通过跳跃连接，网络可以从不同层级的特征图中获取更准确的信息，从而提高检测性能。此外，该方法还具有良好的可扩展性和通用性，可以应用于其他图像分割和目标检测任务。</p><figure><img src="/2023/03/02/0302/image-20230508204404177.png" alt="image-20230508204404177"><figcaption aria-hidden="true">image-20230508204404177</figcaption></figure><h1 id="总结">总结</h1><p>除此之外，本文还介绍了使用的数据增强的方法，主要是对图像进行了随机旋转、缩放等，不过这都是很常见的预处理方法了，总的来说作者的思路比较新颖，对U-net做的改进也比较有意思，但是没有看到对参数量以及推理时间的实验，个人怀疑这种方式的推理耗时可能增加的不少。不过能看到比较有意思的CV工作换换口味还是挺不错的~</p><p>​</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;section&quot;&gt;&lt;/h2&gt;
&lt;p&gt;最近看到一篇文章比较有意思，是对在图像分割领域的&quot;常青树&quot;u-net做的改进，由于自己之前在实验室做过图像分割的项目，曾经也魔改了一版u-net，所以对相关的工作还是比较感兴趣的。这篇论文的题目就很吸引眼球，&lt;a href=&quot;https://arxiv.org/pdf/2212.00968.pdf&quot;&gt;UIU-Net&lt;/a&gt;，在u-net里面套了一个u-net，话不多说，让我们来看看具体是怎么做的。&lt;/p&gt;</summary>
    
    
    
    <category term="paper Reading" scheme="http://kerwinblog.top/categories/paper-Reading/"/>
    
    
    <category term="DL" scheme="http://kerwinblog.top/tags/DL/"/>
    
    <category term="CV" scheme="http://kerwinblog.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>猜你想去？</title>
    <link href="http://kerwinblog.top/2023/02/24/0224/"/>
    <id>http://kerwinblog.top/2023/02/24/0224/</id>
    <published>2023-02-23T19:11:02.000Z</published>
    <updated>2023-05-11T13:34:49.811Z</updated>
    
    <content type="html"><![CDATA[<p>最近在使用滴滴打车，突然发现一个功能——”猜你想去“，试着点了一下发现准确的猜到了我想去的地点，突然觉得这个功能还挺有趣的（虽然感觉有点不适--！），上网查了下发现在2017年滴滴就在KDD上发表相关论文了（<a href="http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p2151.pdf">链接</a>）<span id="more"></span>，读了一些还挺有意思的。这篇论文主要为了解决两个问题，第一个是出租车调度的问题，即出租车订单的全局分配，文章针对全局最优分配进行了优化，这部分就不展开了。第二个问题就是目的地预测了。</p><h1 id="背景">背景</h1><p>目的地预测系统是滴滴出行APP端的一个用户体验优化。当APP打开时，就可以相对准确预测出目的地这将会比较好的优化用户的体验。过去的尝试主要基于多层神经网络的分类模型，但此模型的问题在于有可能预测出一些不相关的地点，或者地点别名。这种结果发生时，客户依旧需要手工输入目的地名称。</p><p>由此，作者提出一个给予客户历史打车记录的目的地，给予贝叶斯模型来预测目的地概率列表，为客户提供候选。</p><h1 id="方法">方法</h1><p>作者通过观察发现几个重要的pattern,（<strong>1）同一个人经常在同样的时间去同样的地方；（2）通常而言，同一个人经常去的候选目标是个固定的集合。（3）订单地点和目标高度相关。</strong> 基于这些观察，作者提出了贝叶斯模型来用来预测。上述特征列表如下：</p><figure><img src="/2023/02/24/0224/image-20230503224138688.png" alt="image-20230503224138688"><figcaption aria-hidden="true">image-20230503224138688</figcaption></figure><h2 id="目的地预测业务问题转化">目的地预测—业务问题转化</h2><p>首先求该用户在出发时刻T下出行前往每个目的地的条件概率。</p><figure><img src="/2023/02/24/0224/image-20230503224903893.png" alt="image-20230503224903893"><figcaption aria-hidden="true">image-20230503224903893</figcaption></figure><p>其中<span class="math inline">\(X\)</span>是输入的特征，包含用户特征和上下文特征,（时间&amp;经纬度），针对某一个客户，针对不同目的地<span class="math inline">\(y_i\)</span>的概率分布为:</p><figure><img src="/2023/02/24/0224/image-20230503225043912.png" alt="image-20230503225043912"><figcaption aria-hidden="true">image-20230503225043912</figcaption></figure><p>注意这里的目的地候选集合全部都是用户历史去到过的地点。而另一个部分<span class="math inline">\(p(X|Y=y_j)\)</span>代表了在给定地点的情况下，估计用户的时间&amp;经纬度分布。</p><figure><img src="/2023/02/24/0224/image-20230503225825386.png" alt="image-20230503225825386"><figcaption aria-hidden="true">image-20230503225825386</figcaption></figure><p>这里作者做了大量统计，发现在给定目的地的情况下，用户出发的时间分布是符合正太分布的。那么该如何得到正太分布的均值和方差了，作者给出了一个巧妙的解法。</p><p>首先看看传统方法是怎么解决的。</p><p><img src="/2023/02/24/0224/image-20230503230428688.png" alt="image-20230503230428688" style="zoom:67%;"></p><p>首先计算每一个时刻对应的向量角度，再分别计算它的正弦余弦值来表示它的x，y轴坐标，然后再把计算时刻对应的和向量对应的夹角转化为对应的时刻。</p><p>由于每一天的时间是一个按照24小时一轮的循环分布，直接用用传统高斯分布的方法计算均值和方差是不合理的。 文中提供的方式是把均值<span class="math inline">\(\mu\)</span>当做待求解变量，放在一个凸优化问题里面进行求解。 具体如下：</p><figure><img src="/2023/02/24/0224/image-20230503230752827.png" alt="image-20230503230752827"><figcaption aria-hidden="true">image-20230503230752827</figcaption></figure><p>这里说明一下，作者规定距离不能为负，且不能超过12，如果超过则会用24减去。这也很好理解。比如晚上23点和凌晨2点的距离是3小时，但是23-2结果是21，这时需要用24-21得到3。由上式可以得到：</p><figure><img src="/2023/02/24/0224/image-20230503231251638.png" alt="image-20230503231251638"><figcaption aria-hidden="true">image-20230503231251638</figcaption></figure><p>求解出 <span class="math inline">\(\mu\)</span> 后然后 <span class="math inline">\(\sigma\)</span> 也可以通过以下方式得到：</p><figure><img src="/2023/02/24/0224/image-20230503231346292.png" alt="image-20230503231346292"><figcaption aria-hidden="true">image-20230503231346292</figcaption></figure><p>其他特征的条件概率，基本都是按照高斯分布求解参数， 无需赘述。把统计得到的条件概率代入：</p><figure><img src="/2023/02/24/0224/image-20230503232245828.png" alt="image-20230503232245828"><figcaption aria-hidden="true">image-20230503232245828</figcaption></figure><p>通过这三个特征，算出周日到周六每一天的预测即可。作者提供了一张展示图， 说明这三个特征可以把数据分的很好。</p><figure><img src="/2023/02/24/0224/v2-05541b4818cee71b6528b633ba518c47_720w.webp" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><h2 id="总结">总结</h2><ol type="1"><li>根据用户历史订单数据，估计每个目的地的发单时刻集合的均值和方差。<img src="/2023/02/24/0224/image-20230503232533070.png" alt="image-20230503232533070"></li><li>根据当前时间，计算每个目的地的<span class="math inline">\(P(T|x_i)\)</span>和频率<span class="math inline">\(P(x_i)\)</span></li><li>计算每个目的的概率 <img src="/2023/02/24/0224/image-20230503232801133.png" alt="image-20230503232801133"></li><li>对概率进行排序，设定阈值，返还最优可能。</li></ol><p>文章提出的目的地预测模型， 可以说朴实无华却有效。亮点是用凸优化处理出发时间这种24小时轮询分布的方式简单直接切很有效。</p><p>这是一篇典型的AI应用于解决实际问题的例子，从中也可以总结出这种方法的一般步骤。</p><ol type="1"><li>抽象问题的具体化</li><li>探索与尝试新方法</li><li>实验与评估</li><li>生活场景的新应用</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近在使用滴滴打车，突然发现一个功能——”猜你想去“，试着点了一下发现准确的猜到了我想去的地点，突然觉得这个功能还挺有趣的（虽然感觉有点不适-
-！），上网查了下发现在2017年滴滴就在KDD上发表相关论文了（&lt;a href=&quot;http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p2151.pdf&quot;&gt;链接&lt;/a&gt;）&lt;/p&gt;</summary>
    
    
    
    <category term="paper Reading" scheme="http://kerwinblog.top/categories/paper-Reading/"/>
    
    
    <category term="Rec System" scheme="http://kerwinblog.top/tags/Rec-System/"/>
    
  </entry>
  
  <entry>
    <title>Dynamic Knapsack Optimization Towards Efficient Multi-Channel Sequential Advertising</title>
    <link href="http://kerwinblog.top/2023/01/14/0112/"/>
    <id>http://kerwinblog.top/2023/01/14/0112/</id>
    <published>2023-01-14T11:42:22.000Z</published>
    <updated>2023-05-06T14:03:36.293Z</updated>
    
    <content type="html"><![CDATA[<p>今天介绍一篇阿里出品的广告序列投放算法 <a href="https://arxiv.org/abs/2006.16312">链接</a>，这篇文章通过考虑用户在端上的长期价值，并且通过将序列投放问题建模为动态背包问题，最后给出了近似解。<span id="more"></span>总得来说是比较有意思的解决问题的方式，但是我不太认为在线上环境能够取得很不错的效果，原因后面分析。</p><h1 id="背景">背景</h1><p>现在的电商平台中，通过在一定预算约束下去优化GMV，也就是提高广告主的ROI是广告主的核心诉求之一。现有的绝大多数出价策略将一段时间的GMV优化问题拆解为：对每次用户的请求进行独立优化，并且认为这些独立优化汇总结果可以等同于这一段时间的GMV最优化。事实上，这类策略得到解可能只是次优解，因为他们以孤立的视角把消费者和广告限定在了单次交互中，而忽略了一段时间内多次交互可能产生的其他影响。</p><p>上面一段话可能有点绕，作者在这里举了一个例子，首先，同一个消费者在一段时间内（比如3天）会多次访问淘宝，并且随机地在淘宝不同场景出现（例如首页、购后），这为同一个广告和同一个消费者在不同场景多次接触创造了机会；其次，大量的成交并非发生在消费者和广告的初次接触中，而是发生在第二次或者以后更多次的接触中，通过ab实验，作者发现广告和消费者的前序接触会影响消费者对该广告在后续接触中的点击率和转化率，说明多次的接触对消费的心智有积累影响的效应，在这样的背景下，单次请求优化结果的积累很容易导致次优解。</p><p>然而，基于长期价值的序列投放算法在解决预算约束下GMV的优化问题时存在诸多挑战：</p><ol type="1"><li>优化目标是长期的累积价值，而决策的粒度是单次的；如何基于长期价值的预估获得最优的单次决策？</li><li>长期价值预估模型的学习离不开策略探索生成的序列数据。长期价值预估模型和决策模型的学习如何保证收敛性？如何保证决策的最优性？如何提升探索策略的效率？</li><li>如何保障预算约束的满足？</li></ol><p>针对这些挑战，作者逐一给出了解决方案，首先，将预算约束问题建模为背包问题：背包中物品的价值为&lt;用户，ad&gt;形成的序列价值（长期成交、收藏加购等），物品的重量为此序列中发生的成本（消耗）；我们按照性价比（序列价值/成本）由高到低逐个选择物品，直到选出的物品总消耗刚好不超过预算约束。这里，由于物品的重量远远小于背包的容量，按性价比排序的贪心算法能够接近最优解。然而，每个序列的价值和成本与运营该序列的广告策略有关，因此这是一个动态背包问题，为求解此动态背包，作者采用了双层优化问题的迭代解法来求解：1）物品的贪心选择，2）物品价值/成本以及对应策略的优化。在此框架下，我们提出了一种理论最优的运营策略，该策略满足强化学习中的Policylteration，能够保证其学习的收敛性。此外，为了使策略在实际场景中落地，，作者提出了一种将连续出价转换为离散动作的方法，能够在不丢失出价精度的情况下，大幅度减少动作的探索时间，提高学习效率。综上，作者将整个算法称之为MSBCB。</p><h1 id="建模方法">建模方法</h1><p><strong>问题定义：</strong>对于某个广告，我们需要在一定的预算约束<span class="math inline">\(B\)</span>下优化一段时间窗口内的成交金额，这个过程可以被建模为一个背包问题：我们把预算约束当做背包的总空间，把用户与广告的每次接触当做一个物品，在<span class="math inline">\(t\)</span>时刻，物品的价值定义为<span class="math inline">\(u_t\)</span>为用户购买期望，物品的重量定义<span class="math inline">\(c_t\)</span>为接触带来的消耗期望；假设总共有<span class="math inline">\(U\)</span>个用户<span class="math inline">\(\{j=1,2,3....,U\}\)</span>，我们对某个用户<span class="math inline">\(j\)</span>的竞价策略为<span class="math inline">\(\pi_j\)</span>，则此广告对该用户未来累积成交价值为<span class="math inline">\(V_G(j|\pi_j)=\mathbb E|\sum_{t=0}u_t|\pi_j|\)</span>，未来累积消耗为<span class="math inline">\(V_C(j|\pi_j)=\mathbb E|\sum_{t=0}c_t|\pi_j|\)</span>；那么，此背包问题被形式化建模为： <span class="math display">\[\max\limits_{\mathcal X,\varPi}\sum^U_{j=1}x_jV_c(j|\pi_j)\\s.t. \sum_{j=1}^U{x_j}V_G(j|\pi_j)\le B\]</span> 其中，<span class="math inline">\(\varPi=\{\pi_1,\pi_2,...,\pi_U\}\)</span>，<span class="math inline">\(\mathcal X =\{x_1,x_2,...x_U\}\)</span>，而且<span class="math display">\[x_j =\begin{cases}1 &amp;\text{if user} j   \text{ is selected for advertising}\\0 &amp;\text{otherwise}\end{cases}\]</span><strong>求解思路：</strong>我们的目标是需要找到最优的竞价策略<span class="math inline">\(\varPi^*=\{\pi_1^*,\pi_2^*...\pi_U^*\}\)</span>和用户的挑选策略<span class="math inline">\(\mathcal X^*=\{x_1^*,x_2^*,...x_U^*\}\)</span>来最大化目标函数。但是，此背包问题是一个双层优化问题(bileveloptimization problem)，因为对<span class="math inline">\(\varPi\)</span>的优化是内嵌在对<span class="math inline">\(\mathcalX\)</span>的优化中，也就是说，我们要先选定对那些用户进行优化，然后才是如何优化这些用户的策略。当然，用户策略的变化也会对用户的选择产生影响，因为不同的投放策略会导致同一个用户的未来长期价值不同个，相当于各个物品的价值和重量会发生变化，因此影响着最后装进背包的物品，综上，此背包是一个动态背包，各个物品的价值和重量都是动态变化的，<span class="math inline">\(\varPi\)</span>会影响物品的价值和重量，而<span class="math inline">\(\mathcal X\)</span>影响着最后装入背包的物品，<span class="math inline">\(\mathcal X\)</span>和<span class="math inline">\(\varPi\)</span>互相影响着。对于此问题，既然同时优化两个变量比较困难，那么我们就固定一个变量在优化另一个变量，采用迭代的方式求解两个变量，因此，问题被拆解为两部分。</p><h2 id="固定varpi优化mathcal-x">固定<span class="math inline">\(\varPi\)</span>,优化<span class="math inline">\(\mathcal X\)</span></h2><p>给定每个用户策略 <span class="math inline">\(\varPi\)</span>，然后预估每个用户未来价值（成交、消耗），然后通过“贪心方式”逐个选出高性价比（ROI）用户，直到广告满足广告主预算。定义用户性价比CPR(Cost-PerformanceRatio)，选用户的过程：</p><p><img src="/2023/01/14/0112/image-20230502191418479.png" alt="image-20230502191418479" style="zoom:67%;"></p><p>其中，<span class="math inline">\(\text{CPR}_{thr}\)</span>是一个阈值，<span class="math inline">\(\text{CPR}_j=\)</span> <strong>未来累计成交价值 /未来累计消耗；</strong>通过贪心方式把用户按照性价比从高到低排序，然后筛选大于阈值的所有用户：<span class="math inline">\(\CPR_j \ge \CPR_{thr}\)</span>。值得注意的是，由于用户粒度的消耗相对于广告主的预算是忽略不计的（各个物品的重量相对背包的重量是非常小的）。因此在这个背包问题中，此贪心框架是基本上逼近最优解的。</p><h2 id="固定mathcal-x优化varpi">固定<span class="math inline">\(\mathcalX\)</span>,优化<span class="math inline">\(\varPi\)</span></h2><p>当知道哪些用户被装进背包后，进一步优化各个<strong>用户策略</strong><span class="math inline">\(\pi_j\)</span>使其对商品的未来累计成交价值最多，同时尽可能减小消耗（使物品价值变大且重量变小）。提出了策略的最优方案：<span class="math display">\[\pi_j^*=argmax_{\pi_j}[V_G(j|\pi_j)-CPR_{thr}*V_c(j|\pi_j)]\]</span> 可以通过反证法来证明此方案的合理性：</p><p>证：假设此方案非最优，那么至少存在一个更好的用户策略<span class="math inline">\(\pi^{&#39;&#39;}\)</span>能使<span class="math inline">\(\Delta V_G(j)/ \Delta V_C(j) \gtCPR_{thr}\)</span>;则存在以下变换：<span class="math inline">\(\frac{\Delta V_G(j)}{\Delta V_C(j)} \gt CPR_{thr}\Leftrightarrow\frac{V_C(j|\pi^{&#39;&#39;})-V_C(j|\pi^*)}{V_G(j|\pi^{&#39;&#39;})-V_G(j|\pi^*)}\gt CPR_{thr} \Leftrightarrow[V_G(j|\pi^{&#39;&#39;}_j)-CPR_{thr}*V_c(j|\pi^{&#39;&#39;}_j)] \gt[V_G(j|\pi^*_j)-CPR_{thr}*V_c(j|\pi^*_j)]\)</span>，最后一个等式说明策略<span class="math inline">\(\pi_j^*\)</span>不满足最优方案的公式，说明不能找到一个<span class="math inline">\(\pi^{&#39;&#39;}\)</span>使得<span class="math inline">\(\frac{\Delta V_G(j)}{\Delta V_C(j)} \gtCPR_{thr}\)</span>。</p><p>同时，选择此方案有如下优势：</p><ol type="1"><li>将广告优化目标由除法变成减法：论文中将奖励定义为ROI：<span class="math inline">\(reward =V_G(\pi)/V_C(\pi)\)</span>，由于ROI相加没有意义，变成减法 <span class="math inline">\(reward = V_G(\pi)-\lambdaV_C(\pi)\)</span>后，奖励就是线性可加的，能够通过强化学习求解；</li><li>当证明<span class="math inline">\(\lambda =CPR_{thr}\)</span>时，用强化学习优化reward能够保证当前动态背包问题的最优性质。</li></ol><p><strong>迭代求解：</strong></p><p>一旦确定动态背包中阈值<span class="math inline">\(CPR_{thr}\)</span>，可以根据后续方案求得用户最优的竞价策略 <span class="math inline">\(\pi_j^*\)</span> ;但是实际应用中，因为一开始无法知道广告主的预算阈值的准确值<span class="math inline">\(CPR_{thr}^*\)</span> 。因此需要预设<span class="math inline">\(CPR_{thr}\)</span>来优化竞价策略，然后基于当前的策略和真实反馈进一步更新阈值<span class="math inline">\(CPR_{thr}\)</span>，过程如下:</p><ul><li>固定 <span class="math inline">\(\mathcal X\)</span> ，优化<span class="math inline">\(\varPi\)</span> ：给定一个 <span class="math inline">\(CPR_{thr}\)</span> ，优化得到一个最优竞价策略$^*={_1^*..._U^*} $</li><li>固定 <span class="math inline">\(\varPi\)</span> ，优化<span class="math inline">\(\mathcal X\)</span> ：根据 <span class="math inline">\(CPR_{thr}\)</span>和用户<span class="math inline">\(\pi_j\)</span> ,贪心求解动态背包问题，然后通过真实反馈消耗更新 <span class="math inline">\(CPR_{thr}\)</span></li></ul><h1 id="方案细节">方案细节</h1><h2 id="固定mathcal-x优化varpi-强化学习求解">固定<span class="math inline">\(\mathcal X\)</span>,优化<span class="math inline">\(\varPi\)</span> ——强化学习求解</h2><p>给定阈值 <span class="math inline">\(CPR_{thr}^*\)</span>，优化用户每次广告触达上的出价策略，应用强化学习解决这个优化未来长期价值的reward问题。对用户状态干预的抓手（动作）主要是出价/ 调价，提出一种基于DQN的“<strong>动作约减</strong>”(action spacereduction)方法，能够在不损失出价精度情况下将连续的动作离线到0/1空间，不需要人为调参。</p><p><strong>动作约减</strong>：对一个广告动作定义投放（<span class="math inline">\(\hat{a}_t = 1\)</span> ）和不投放（<span class="math inline">\(\hat{a}_t = 0\)</span>）两种，可以直接使用DQN类算法进行求解；但是需要解决以下两个问题：1）如何将长期价值预估转化为投/不投动作策略；2）投和不投动作对应的出价分别是多少才不会损失出价精度？</p><p>针对问题1），需要分析长期价值的预估值与动作策略的映射关系，2.2章节中单步奖励被定义为减法<span class="math inline">\(reward = V_G(\pi)-\lambda V_C(\pi)\)</span>后，根据马尔可夫决策过程，对每个动作的Q值进行定义： <span class="math inline">\(Q(s,\hat{a}_t) = Q_G(s,\hat{a})-CPR^*_{thr}*Q_C(s,\hat{a}_t)\)</span> ,其中 <span class="math inline">\(Q_G(s,\hat{a}_t)\)</span>表示此动作下未来ed成交期望，<span class="math inline">\(Q_C(s,\hat{a}_t)\)</span>表示此动作下未来的消耗期望。那么，动作策略可表示为：<span class="math display">\[\hat{a}_t^* =\begin{cases}1 &amp;\text{if} \ Q (s,\hat{a}_t=1)\gtQ(s,\hat{a}_t=0) \\0 &amp;\text{otherwise}\end{cases}\]</span>基于两个动作的Q值，能够对动作进行决策；推导广告出价的准确值：对于 $_t=0<span class="math inline">\(，定义出价\)</span>_t=0 $,保证此广告在竞价中不会最终展现；对于 $_t=1 $ ，作出以下推导：</p><p><span class="math display">\[Q(s,\hat{a}_t=1) \gt Q(s,\hat{a}_t=0)\\\Leftrightarrow Q_G(s,\hat{a}_t=1)-CPR_{thr}^**Q_C(s,\hat{a}_t=1) \gt\\    Q_G(s,\hat{a}_t=0)-CPR_{thr}^**Q_C(s,\hat{a}_t=0) \\    \LeftrightarrowQ_G(s,\hat{a}_t=1)-CPR_{thr}^**(\text{bid}_t^{2nd}*CTR+Q_C^{next}(s,\hat{a}_t=1)\gt \\ Q_G(s,\hat{a}_t=0)-CPR_{thr}^**(0+Q_C^{next}(s,\hat{a}_t=0) \\    \Leftrightarrow  \mathbf {bid}_t^{2nd}\lt(\frac{Q_G(s,\hat{a}_t=1)}{CPR_{thr}^* * CTR} -\frac{Q_C^{next}(s,\hat{a}_t=1)}{CTR})\\-(\frac{Q_G(s,\hat{a}_t=0)}{CPR_{thr}^* * CTR} -\frac{Q_C^{next}(s,\hat{a}_t=0)}{CTR})\]</span></p><p>其中，<span class="math inline">\(Q_C^{next}(s,\hat{a}_t)\)</span>是下一时刻动作 <span class="math inline">\(\hat{a}_t\)</span>未来累计消耗的Q值；将最后一个式子中右边部分设为<span class="math inline">\(b_t^*\)</span> ,并且把 <span class="math inline">\(b_{t}^*\)</span> 作为 <span class="math inline">\(\hat{a}_t=1\)</span> 时的最优出价。</p><p>综上，将连续的动作空间 <span class="math inline">\((a_t \in[0,bid_{max}])\)</span>约减到一个二维空间 ，策略上无论是 <span class="math inline">\(\hat{a}_t=1\)</span> 还是 <span class="math inline">\(\hat{a}_t=0\)</span> ，都可以用 <span class="math inline">\(\hat{a}_t=b_t^*\)</span>来作为最终出价。因此，只需要预估4个长期价值，然后算出最终出价 <span class="math inline">\(b_{t}^*\)</span>并参与竞价；这四个长期价值分别反应着广告在两个动作下t时刻未来的成交期望，和t+1时刻未来的期望，可以通过bootstrap方式学习，也可以通过MC的方式学习。</p><h2 id="固定-varpi优化-x背包求解">固定 <span class="math inline">\(\varPi\)</span>，优化$ X$——背包求解</h2><p>在给定策略<span class="math inline">\(\varPi =\{\pi_1^*,...,\pi_U^*\}\)</span>后，需要优化背包中的物品，根据2.1节需要对每个用户进行性价比排序，然后贪心取性价比高的用户进行投放，直到最后一个用户满足<span class="math inline">\(CPR_j \ge CPR_{thr}\)</span>，一个用户根据投和不投策略会存在两个CPR值，取最大即可：</p><p><span class="math display">\[CPR_J = \max_{\hat{a}_t \in (0,1)}CPR_j(\hat{a}_t)\]</span> 其中，每个动作的CPR值都可以被以下式子计算： <span class="math display">\[CPR_j (\hat{a_t}) =\begin{cases}\frac{Q_G(s,\hat{a}_t)}{\mathbf{bid_t^{2nd}*CTR+Q_C^{next}(s,\hat{a}_t=1)}}&amp;\text{if} \ \hat{a_t}=1   \\\frac{Q_G(s,\hat{a_t}=1)}{0+Q_C^{next}(s,\hat{a}_t=0)} &amp;   \text{if}\ \hat{a_t}=0 \end{cases}\]</span> 值得注意，上面所有的计算公式除了二价<span class="math inline">\(bid_t^{2nd}\)</span> 外，CPR计算只依赖：<span class="math inline">\(Q_G(s,\hat{a}_t=1)\)</span> 、<span class="math inline">\(Q_C^{next}(s,\hat{a}_t=1)\)</span> 、 <span class="math inline">\(Q_G(s,\hat{a}_t=0)\)</span> 和 <span class="math inline">\(Q_C^{next}(s,\hat{a}_t=1)\)</span>四个长期价值，由于在决策前无法确定二价，因此将二价替换为最优出价<span class="math inline">\(b_t^*\)</span> ,效果和最优性质不变。</p><p>在通过真实反馈消耗更新初始<span class="math inline">\(CPR_{thr}\)</span>时，设计一个PID控制器基于实际消耗和预算之间的差距来更新，让实际消耗与预算持平。</p><h2 id="预估模型">预估模型</h2><p><img src="/2023/01/14/0112/image-20230503142023163.png" alt="image-20230503142023163" style="zoom: 80%;"></p><p>模型的预估对象分成交和消耗两种，因此我们这是一个多任务学习，需要同时学习回归和分类。为应对多任务学习，我们将模型结构进行拆分，底层共享embedding，顶层网络参数解耦，以降低多任务学习互相不利干扰，而且通过validation的方式优化各个loss之间的权重。其次，对于回归任务，由于其存在大量的零样本，导致模型成为一个<strong>零膨胀模型（</strong>Zero-inflatedmodels），其输出基本上全为0，无法用MSEloss来正常学习网络参数。为解决此问题，我们提出两种解决办法：</p><ol type="1"><li><strong>通过合理的负采样来保证证样本的有效学习</strong>，并通过校准技术补偿由样本分布调整造成的预估偏差；</li><li><strong>引入CTR先验，构造CTRloss来辅助回归学习</strong>。我们认为消耗的期望可以拆分成消耗发生的概率与对应的消耗值的点乘，因此我们将未来消耗发生的概率显示地单独用CTR的label来学习，并使其更新不受其他loss的影响；然后我们基于较为准确的消耗概率，再来学习其概率对应下的消耗值，能够有效避免消耗值输出全为0的情况，使MSEloss能正常更新模型参数。</li></ol><h2 id="整体架构">整体架构</h2><figure><img src="/2023/01/14/0112/image-20230503142255287.png" alt="image-20230503142255287"><figcaption aria-hidden="true">image-20230503142255287</figcaption></figure><p>对整个流程进行梳理：</p><ol type="1"><li>首先，当用户请求到达广告平台之后，我们构造用户和广告特征，然后对每个进行四个长期价值的预估，得出每个广告所采取的策略（投/不投）并算出对应的最优出价。</li><li>接着，对于任意广告，我们计算当前用户在两个不同决策下的最高性价比，若此性价比高于此广告的阈值CPRthr，则将当前用户装入此广告的背包中。</li><li>最后，我们拿到用户的反馈，一方面，我们在PID模块中基于预算和实际消耗来更新阈值CPRthr，另一方面，我们构造训练数据来更新强化学习模型参数，使预估的长期价值更准确。</li></ol><h2 id="总结">总结</h2><p>在这个实验中，阿里打破传统ocpc竞价模型的思路，通过强化学习模拟了整个竞价的过程，只能默默献上本人的膝盖了！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天介绍一篇阿里出品的广告序列投放算法 &lt;a href=&quot;https://arxiv.org/abs/2006.16312&quot;&gt;链接&lt;/a&gt;，这篇文章通过考虑用户在端上的长期价值，并且通过将序列投放问题建模为动态背包问题，最后给出了近似解。&lt;/p&gt;</summary>
    
    
    
    <category term="paper Reading" scheme="http://kerwinblog.top/categories/paper-Reading/"/>
    
    
    <category term="Rec System" scheme="http://kerwinblog.top/tags/Rec-System/"/>
    
  </entry>
  
  <entry>
    <title>A Survey of Multi-Domain model</title>
    <link href="http://kerwinblog.top/2022/12/04/20221204/"/>
    <id>http://kerwinblog.top/2022/12/04/20221204/</id>
    <published>2022-12-04T02:32:02.000Z</published>
    <updated>2023-05-06T14:03:36.338Z</updated>
    
    <content type="html"><![CDATA[<p>在实际的业务中，数据往往由多个 domain组成，以广告为例，往往会存在多个转化目标，在 ctr、cvr的预估时也要考虑不同转化目标的影响，因为在不同转化目标下，ctr、cvr的分布(如均值、方差)往往是不一致的。</p><p>解决这个问题最直观的思路是加 domain 相关特征或根据 domain拆模型，前者属于隐式的方法，需要特征的区分性足够强、能被模型学到，但这个足够强没有一个量化的标准，基本只能看实验效果；后者则存在维护成本过高的问题，比如说有n 个 domain 就要拆成 n 个模型。</p><p>本文着重讲如何通过一个模型 serve 多个 domain的方法，主要是在业界验证有收益且公开发表的工作，基本上可以分为 3类：<span id="more"></span></p><ol type="1"><li>multi-head 结构</li><li>LHUC 机制</li><li>GRL 机制</li></ol><h1 id="mmoe">MMOE</h1><p>在一个模型中根据多个 domain 拆成多个 head（每个 head 代表一个domain），通过每个 head 的参数学习特定 domain的分布，是一种比较直观和常见的做法。这类方法的代表是 MMOE: <a href="https://dl.acm.org/doi/pdf/10.1145/3219819.3220007">Modeling TaskRelationships in Multi-task Learning with Multi-gateMixture-of-Experts</a></p><p>下图直观展示了拆 head 的集中常见做法。</p><p><img src="/2022/12/04/20221204/image-20230422222324608.png" alt="image-20230422222324608" style="zoom: 50%;"></p><p>MMOE 中的两个 M，第一个代表 multi-gate，第二个代表multi-expert；multi-expert 比较好理解，从 ensemble 的角度来看，就是在做bagging，而 gate 就是在控制每个 expert 的 weight，multi-gate 则是为每个expert 分配一个 gate，本质上就是做到 domain-wise 的优化</p><p>而 gate 的具体实现，也是一个 mlp, 最终通过 softmax 得到每个 expert 的weight，对于第 <span class="math inline">\(k\)</span>个task，计算过程如下图所示。</p><p><img src="/2022/12/04/20221204/MultiDomainMMOE_GATE.jpg" alt="mmoe gate 结构" style="zoom:50%;"></p><h1 id="star">STAR</h1><p>这是阿里的一篇 paper，应用场景就是比较典型的 CTR 业务，<a href="https://arxiv.org/pdf/2101.11427.pdf">One Model to Serve All: StarTopology Adaptive Recommender for Multi-Domain CTR Prediction</a></p><p>在模型结构上，也是为每个 domain分配一部分自己的参数，从而达到在一个模型中 serve 多个 domain的目的，这一点跟 MMOE 原理上是一样的，文章是这么说的。</p><blockquote><p>Essentially, the network of each domain consists of two factorizednetworks: one centered network shared by all domains and thedomain-specific network tailored for each domain</p></blockquote><p>STAR 基本结构如下图(b)所示，直观来看，有一个公共的 head，同时为每个domain 分配了一个 head，最终每个 head 的参数是公共 head 参数与 domainhead 参数的 element-wise 结果。</p><p><img src="/2022/12/04/20221204/MultiDomainSTAR.jpg" alt="mmoe star 结构" style="zoom:33%;"></p><h2 id="partitioned-normalizationpn">Partitioned Normalization(PN)</h2><p>在上面的结构中，有一个 Partitioned Normalization (PN)的部分，目标是解决统一 batch normalization 在这 multi-domain中不适用的问题</p><p>常规的 batch normalization 会计算 batch 内所有的样本的 mean 和variance，然后做归一化，如下图所示；这里有个<strong>假设就是这批样本是独立同分布(i.i.d.)</strong>的，但multi-domain 本身的要解决的问题就是不同 domain的分布不一样，因此不能直接用原始的 batch normalization；关于 BN为何有效，可参考这篇文章：<a href="https://arxiv.org/pdf/2105.07576.pdf">Rethinking “Batch” inBatchNorm</a></p><p><img src="/2022/12/04/20221204/MultiDomainSTAR_BN.jpg" alt="batch normalization" style="zoom:33%;"></p><p>而 PN 的做法是为原始 BN 中的参数 <span class="math inline">\(γ\)</span> 和 <span class="math inline">\(β\)</span> 生成额外的 domain-specify参数，如下图所示</p><p><img src="/2022/12/04/20221204/MultiDomainSTAR_PN.jpg" alt="partitioned normalization" style="zoom:33%;"></p><h2 id="auxiliary-network">Auxiliary Network</h2><p>这是个小网络，输入是domain indicator(表示这个样本来自哪个domain)，输出是一个预估值，最终输出的预估值会与上面的 STAR的模型加和做最终预估; 作用等价于为每个 domain 增加了一个 bias 项</p><p><img src="/2022/12/04/20221204/MultiDomainSTAR_AT.jpg" alt="Auxiliary Network" style="zoom:33%;"></p><p>实验结果这里不展开，paper 效果显示比一些已有的 multi-domain任务要好（参数量是否打平没提到）；也对 PN 的效果做了消融，结果显示 PN的效果比直接用 BN 要好。</p><h1 id="lhuc">LHUC</h1><p>前面的两篇文章基本思路都是为不同的 domain 分配多一个 head的参数，然后通过不同 head 来描述不同 domain 的差异</p><p>提出 LHUC 这篇文章则没有显式地分 head ，而是通过在 hidden layer上乘上一个 domain-aware embedding，来达到这样的效果：<a href="https://arxiv.org/abs/1601.02828">Learning Hidden UnitContributions for Unsupervised Acoustic Model Adaptation</a></p><p>这篇 paper 最早是在 speech 领域提出的一个方法，基本的思路是为每个speaker 单独调整 nn 中全连接层里部分的参数，从而达到每个 speaker的个性化预估；总体思路比较直观，也很容易把方法迁移至推荐上</p><p>快手的提出的 PEPNet(<a href="https://arxiv.org/pdf/2302.01115.pdf">Parameter and EmbeddingPersonalized Network for Infusing with Personalized PriorInformation</a>) 也是借鉴了 LHUC 的这个思想</p><p>整个模型结构如下图所示，LHUC 部分是最右边的 PPNet 部分，每个 GateNU相当于为每个 hidden layer 生成一个 domain-aware 的 embedding，左边的EPNet 则是相当于为 embedding 不用分生成类似的 embedding。</p><p><img src="/2022/12/04/20221204/MultiDomainLHUC_PEPNet.jpg" alt="PEPNet 结构" style="zoom:33%;"></p><p>Gate NU 可以理解为一个简单两层的 nn网络，输入是<strong>依靠先验知识挑选的能够区分不同 domain的特征</strong>，输出则是一个 tensor（维度大小与 hidden layer 一样。</p><p><img src="/2022/12/04/20221204/MultiDomainLHUC_GateNU1.jpg" alt="Gate NU" style="zoom: 50%;"></p><p>除了结构上的改进，这篇 paper还做了较多的工程上的有优化，这里不详细展开。</p><h1 id="grl">GRL</h1><p>这里主要想介绍 GRL（Gradient ReversalLayer）这个机制，这个机制出自论文 <a href="https://arxiv.org/pdf/1409.7495.pdf">Unsupervised DomainAdaptation by Backpropagation</a></p><p>论文主要想解决的问题是 domain adaption，即在 source domain有较多数据，target domain 较少数据，怎么能够较好地同时解决两个 domain的问题，paper 里提到的方案是从特征层面去解决这个问题</p><blockquote><p>the approach promotes the emergence of “deep” features that are (i)discriminative for the main learning task on the source domain and (ii)invariant with respect to the shift between the domains.</p></blockquote><p>为了达到这个目标，论文提出的 GRL 机制如下图所示。</p><p><img src="/2022/12/04/20221204/MultiDomainGRL.jpg" alt="GRL 机制" style="zoom:33%;"></p><p>从结构上来看，这也是个 multi-head 的结构，但蓝色的 head 是 sourcedomain 的原始 task，<strong>红色的 head（后面简称为 discriminator）则是一个 domain classifier，即是用来区分样本是属于哪一个 domain的</strong></p><p>两个 task 在做 bp 时，蓝色的 head 正常回传梯度，discriminator则在梯度回传到 feature extractor 即图中色绿色部分时，乘上一个 negativeconstant，这就是 GRL 的机制</p><p>那为什么要这么做呢？paper 给出的解释是这样的。</p><blockquote><p>we want to make the features domain-invariant. That is, <strong>wewant to make the distributions $S(f)={G_f(x;θf)|x∼S(x)} <span class="math inline">\(and\)</span> T(f)={G_f(x;θf)|x∼T(x)}$ to besimilar</strong>. Under the covariate shift assumption, this would makethe label prediction accuracy on the target domain to b_e the same as onthe source domain (Shimodaira, 2000).</p></blockquote><p>即希望 feature extractor 抽取出来的特征是 domain-invariant、对 domain不敏感的，或者说<strong>基于抽取出来的特征，discriminator不能很好地区分样本来自哪个 domain</strong></p><p>而如果不加 GRL，正常的 bp 是会 discriminator 能够区分样本来自哪个domain 的，加了 GRL 后，则能够达到上面提到的“discriminator不能很好地区分样本来自哪个 domain”</p><p>那另一个问题来了，即为什么要在 feature extractor 这一层做，而不是在<span class="math inline">\(L_d\)</span> 上就加一个负号？</p><p>因为<strong>如果直接在 <span class="math inline">\(L_d\)</span>上加负号，相当于让 discriminator 把它分到错误的那个 domain，但一个好的feature 应该是让 discriminator 分辨不出它来自哪个domain，而不是把它分到错误的那个 domain</strong></p><p>因此，GRL 机制某种程度上也是一类 featureengineering，用于提取出适用于多个 domain 的 feature。</p><h2 id="小结">小结</h2><p>综上，本文主要介绍了三种解决 multi-domain 的思路，分别是 multi-head结构，LHUC 机制和 GRL机制；据笔者的了解，目前这几种方案在业界都有落地的案例，值得在相应业务中进行尝试~</p><p>multi-head 机制比较直观，为每个 domain 分配一个 head，分别学习不同domain 的分布，MMOE 和 STAR 这一类模型属于这种；LHUC机制则是根据先验选择一批有区分度的特征，通过一个小的 nn学习一个隐变量作用到hidden-layer上（其实也能作用到 embedding 上，PEPNet中没有介绍的 EPNet 就是这部分，原理基本一致）；GRL则是通过训练方式使得模型抽取出来的 feature 是 domain-invariant的，训练的思想跟 GAN 的对抗训练比较类似</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在实际的业务中，数据往往由多个 domain
组成，以广告为例，往往会存在多个转化目标，在 ctr、cvr
的预估时也要考虑不同转化目标的影响，因为在不同转化目标下，ctr、cvr
的分布(如均值、方差)往往是不一致的。&lt;/p&gt;
&lt;p&gt;解决这个问题最直观的思路是加 domain 相关特征或根据 domain
拆模型，前者属于隐式的方法，需要特征的区分性足够强、能被模型学到，但这个足够强没有一个量化的标准，基本只能看实验效果；后者则存在维护成本过高的问题，比如说有
n 个 domain 就要拆成 n 个模型。&lt;/p&gt;
&lt;p&gt;本文着重讲如何通过一个模型 serve 多个 domain
的方法，主要是在业界验证有收益且公开发表的工作，基本上可以分为 3
类：&lt;/p&gt;</summary>
    
    
    
    <category term="Notes" scheme="http://kerwinblog.top/categories/Notes/"/>
    
    
    <category term="Rec System" scheme="http://kerwinblog.top/tags/Rec-System/"/>
    
  </entry>
  
  <entry>
    <title>Learning Representations for Counterfactual Inference</title>
    <link href="http://kerwinblog.top/2022/12/03/paper20221203/"/>
    <id>http://kerwinblog.top/2022/12/03/paper20221203/</id>
    <published>2022-12-03T14:46:54.000Z</published>
    <updated>2023-05-06T14:03:36.374Z</updated>
    
    <content type="html"><![CDATA[<p>​最近工作中用到因果推理相关知识，发现自己基础太弱，读读论文填充一下。首先介绍一篇经典的论文。这篇文章提出了一种利用领域适应和深度神经网络表示学习的框架方法来进行反事实结果推理</p><ol type="1"><li>公式化反事实推理问题为领域适应问题，更具体一点，转化为协变量转变问题。</li><li>利用深度神经网络表示，线性模型和变量选择来进行反事实推理。</li><li>利用reweighting samples的方法使treatment和control groupsdistribution balanced<span id="more"></span></li></ol><h1 id="abstract">Abstract</h1><blockquote><p>Observational studies are rising in importance due to the widespreadaccumulation of data in fields such as healthcare, education, employmentand ecology. We consider the task of answering counterfactual questionssuch as, “Would this patient have lower blood sugar had she received adifferent medication?”. We propose a new algorithmic framework forcounterfactual inference which brings together ideas from domainadaptation and representation learning. In addition to a theoreticaljustification, we perform an empirical comparison with previousapproaches to causal inference from observational data. Our deeplearning algorithm significantly outperforms the previousstate-of-the-art.</p></blockquote><p>​本文主要解决的问题类似于：对于一个患有低血糖的病人来说，接受不同的治疗方式是否是有用的？为了解决这个问题，本文基于域自适应和表征学习提出了一个用于进行反事实推理的算法框架。​这里的域自适应指的是我们在一种样本集上训练出来的模型应该在该数据集的各领域内都是适应的，有点拗口，举个例子，我们希望预测对一个病人使用某种治疗手段是否会让它对病情好转，那么不管这个病人是高中学历还是大学学历，模型的预测结果都是一样的。但是我们实际的训练样本中，大多数的病人都是高中学历，那么我们使用这个模型去预测大学学历的病人时，效果就不一定会好。</p><p>​本文将域自适应和反事实的推理进行了结合，主要方法是提出了一种正则公式让处于不同干预的人群的representations的分布更加接近，后面会详细介绍是怎么做的，本文的主要contributions有以下两点：</p><ul><li><p>我们证明了如何将反事实推理问题看作一个域自适应问题，也可以看作一个特殊的协变量转移问题。</p></li><li><p>我们推导出一系列反事实推理的表征算法。一种是基于线性模型和变量选择，另一种是基于表征的深度学习结构。</p><p>最后，我们的方法要优于基于重加权的样本采样方式,证明了拉进treatment组和control组特征表示对于反事实的推理是有益的。</p></li></ul><h1 id="problem-setup">Problem Setup</h1><p>​这一节作者介绍了一些在因果推理领域常用的定义及标识，这里简单介绍一下。</p><ul><li><span class="math inline">\(\tau\)</span>:我们希望去预估的一些潜在的干预或者行为。比如对于低血糖的病人来说，我们有两种治疗手段。<span class="math inline">\(t\in\tau\)</span></li><li><span class="math inline">\(\chi\)</span>:用户侧的特征，代表这些病人的背景（contexts),<span class="math inline">\(x\in\chi\)</span></li><li><span class="math inline">\(y\)</span>:施加某种干预后的结果，例如给病人采用A手段进行治疗后血糖增加了多少。<span class="math inline">\(Y_t(x)\in y\)</span></li></ul><p>​ 我们希望计算的是<em>individualized treatment effect</em>(ITE)：<span class="math inline">\(Y_1(x)-Y_0(x)\)</span>，但是在现实情况下，我们只能得到对用户施加两种treatment的其中一种的outcome。所以通常我们会计算拥有相似分布<span class="math inline">\(x\)</span>的人群计算他们的<em>average treatmenteffect</em>(ATE):<span class="math inline">\(ATE=E[Y_1(x)−Y_0(x)]\)</span>。一个更常用的方法是通过模型去预估ITE。<span class="math display">\[\hat{ITE}(x_i) =\left\{\begin{aligned}y_i ^F-h(x_i,1-t_i), t=1\\h(x_i,1-t_i)-y_i ^F, t=0\end{aligned}\right.\]</span> ​ 其中<span class="math inline">\(y_i^F(x)\)</span>是我们观察到的事实结果，<span class="math inline">\(y_i^{CF}\)</span>是模型预估出来的反事实结果。这种方式不同于普通的深度学习建模，会有如下问题：假设我们观察到的样本为集合<span class="math inline">\(\hat{P}^F=\left\{x_i,t_i\right\}^n_{i=1}\)</span>，在计算ITE时我们需要计算这群样本另一个treatment下的结果<span class="math inline">\(\hat{P}^{CF}=\left\{x_i,1-t_i\right\}^n_{i=1}\)</span>。我们定义<span class="math inline">\(P^F\)</span>是<span class="math inline">\(\hat{P}^F\)</span>经过模型抽取特征后到分布<em>factualdistribution</em>，同理<span class="math inline">\(P^{CF}\)</span>是<span class="math inline">\(\hat{P}^{CF}\)</span>经过模型抽取特征后到分布<em>counterfactualdistribution</em>。显然这两个分布是不相等的，但如果这个分布的差异较大的话，模型可能会侧重学习到这两个分布的不同对于最终outcome的影响，而不是我们的treatment对于outcome的影响。在深度学习中这被叫做<em>covariateshift</em>，同时这也是一种域自适应问题。举个列子，在进行随机实验时，不同的人群获得不同treatment的概率应该是随机的，但是在实际的观察研究中，往往有很多因素影响着treament的发放，这就导致了<span class="math inline">\(t\)</span>与<span class="math inline">\(x\)</span>不独立。所以上述的两个分布的差异会比较大。</p><h1 id="balancing-counterfactual-regression">Balancing counterfactualregression</h1><p>​ 我们的方法如下图所示，首先我们对于Context学习一个表达<span class="math inline">\(\Phi:\chi \to\mathbb{R}^d\)</span>,这里我们可以使用深度网络也可以使用类似对样本进行加权筛选的方式。同时我去学习一个函数<span class="math inline">\(h\)</span>，这个函数通过预估对当前的用户表征施加不同的treatment后的结果，会同时权衡三个目标：1）使得我们能够观察到的正常样本的预测误差尽可能小。2）使得对于反事实样本的预测的误差尽可能小，这里对于反事实样本，由于我们并没有它的真实label，我们可以通过一些方式来构建一个伪label（后面会讲）。3）这个函数还要对不同干预下的representation进行一个平衡。​针对第一点，可以通过使用最小化error和一些正则方式来确保这个误差尽可能小，第二点，我们利用的是最近邻的方法，来构造反事实，即<span class="math inline">\(y_{i:t_i=0}^{CF}=y_{NN_i:i\neq0}^{CF}\)</span>其中<span class="math inline">\(NN_i\)</span>表示最近邻的邻居。本质是在模拟样本的反事实，有点类似于matching的方法。平衡好不同干预下的representation。第三点，我们通过最小化<em>discrepancydistance</em>这是域自适应的距离度量公式，后文会有详细介绍。</p><p>​直观地说，减少treatment人群和control人群之间差异的表示防止模型在试图从事实领域推广到反事实领域时使用数据的“unreliable”方面。例如，如果在我们的样本中，几乎没有男性接受过药物A，那么推断男性对药物A的反应很容易出错，因此可能需要更谨慎地使用性别特征。</p><p>​ 我们希望最小化的损失函数如下：</p><p>​<br><span class="math display">\[B_{\mathcal{H},\alpha,\gamma}(\Phi,h)=\frac{1}{n}\sum_{i=1}^n|h(\Phi(x_i),t_i)-y_i^F|+\\\alpha \\text{disc}_\mathcal{H}(\hat{P}_\Phi^F,\hat{P}_\Phi^{CF})+\frac{\gamma}{n}\sum_{i=1}^n|h(\Phi(x_i),1-t_i)-y_{j(i)}^F|\]</span></p><p>​ 上式中，<span class="math inline">\(\alpha,\gamma&gt;0\)</span>是用来控制我们对于两个分布的相似度的限制力度，其中<span class="math inline">\(\text{disc}\)</span>是在后文定义的一种度量分布之间距离的函数。<span class="math inline">\(j(i)\in \text{arg min}_{j\in{\{1...n\}} s.t.t_j=1-t_i}\text{d}(x_i,x_j)\)</span> 即<span class="math inline">\(j\)</span>是在样本空间里和<span class="math inline">\(i\)</span>最相近的一个样本，注意每个样本只能被选择一次。</p><h2 id="method">Method</h2><p>​一个简单的方法去做样本间分布的平衡是只选择那些在treatment组和control组分布相似的特征，但是，那些被舍弃的不相似的特征往往是对预测结果很重要的，直接把他们忽略会影响预测的准确性。一种解决方式是限制这些不相似特征对于预测结果的影响，我们基于这种方式，学习一组稀疏的权重参数，通过平衡模型的预测能力已经样本之间的相似度来确定特征的重要性。作者介绍了两种方法，1）线性模型。2）深度模型。这里主要对深度模型做一些介绍</p><p>​ 如上图所示，作者在标准的前馈神经网络做了一些改进，首先，<span class="math inline">\(d_r\)</span>层通过学习一个表达<span class="math inline">\(\Phi(x)\)</span>将输入特征<span class="math inline">\(x\)</span>进行映射，这一这里使用的样本是没有带上treatment特征的，同时，这一层的输出会被用来计算上文提到的<span class="math inline">\(\text{disc}_\mathcal{H}(\hat{P}_\Phi^F,\hat{P}_\Phi^{CF})\)</span>,之后的<span class="math inline">\(d_o\)</span>层会将treatment特征作为额外特征加入。同时，注意到treatment的还有一根线连接到<span class="math inline">\(\text{disc}\)</span>的计算上，这是因为我们需要这个标签来指导当前样本是属于treatment组还是control组。​为什么要在网络中间将treatment作为额外特征加入而不是在网络一开始就一起训练呢，个人理解这是因为treatment往往很稀疏，为了避免treatment特征在网络前半段被大量其他特征淹没，所以在网络中间部分加入。同时，这样生成的表征<span class="math inline">\(\Phi\)</span>也完全代表了与treatment无关的特征。</p><p>​最后作者通过一系列公式证明了上文中提到的分布差异对反事实的预测结果是有影响的，这里就不重复证明了，感兴趣可以去看原论文。最后的最后作者也指出当我们遇到多treatment情况时，可能需要更适合的算法。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;​
最近工作中用到因果推理相关知识，发现自己基础太弱，读读论文填充一下。首先介绍一篇经典的论文。这篇文章提出了一种利用领域适应和深度神经网络表示学习的框架方法来进行反事实结果推理&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;公式化反事实推理问题为领域适应问题，更具体一点，转化为协变量转变问题。&lt;/li&gt;
&lt;li&gt;利用深度神经网络表示，线性模型和变量选择来进行反事实推理。&lt;/li&gt;
&lt;li&gt;利用reweighting samples的方法使treatment和control groups
distribution balanced&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    <category term="paper Reading" scheme="http://kerwinblog.top/categories/paper-Reading/"/>
    
    
    <category term="DL" scheme="http://kerwinblog.top/tags/DL/"/>
    
    <category term="causal" scheme="http://kerwinblog.top/tags/causal/"/>
    
  </entry>
  
  <entry>
    <title>再看AUC</title>
    <link href="http://kerwinblog.top/2022/05/27/0527/"/>
    <id>http://kerwinblog.top/2022/05/27/0527/</id>
    <published>2022-05-27T13:42:02.000Z</published>
    <updated>2023-05-06T14:03:36.307Z</updated>
    
    <content type="html"><![CDATA[<p>这个月断断续续看了两篇文章，一篇是多场景建模的MMoE，感觉这篇文章的Motivation很makesense ，后面有机会可以尝试一下。另一篇收获不大，就不细说了，不过最近开始着手模型的训练，在和同事交流时，发现自己对于AUC的理解还不太深刻，于是感觉加班补了一下。</p><p>首先要了解AUC，就得知道ROC 曲线和 PR 曲线，ROC 曲线和 PR曲线是评估机器学习算法性能的两条重要曲线，两者概念比较容易混淆，但是两者的使用场景是不同的。我这里主要讲述两种曲线的含义以及应用的场景。<span id="more"></span></p><h2 id="定义">定义</h2><p>ROC 曲线和 PR 曲线都是用在二分类中，且涉及到下图的几个概念(摘自 <a href="https://www.biostat.wisc.edu/~page/rocpr.pdf">The RelationshipBetween Precision-Recall and ROC Curves</a>)</p><p><img src="/2022/05/27/0527/image_1cfpcf7brpr9i4k1h0aklg8vt9.png" alt="roc vs pr" style="zoom:67%;"></p><p>上面四个指标用大白话解释如下</p><p><strong>Recall：查全率，正样本中被预测出来是正的比例(越大越好)Precision：查准率，预测的正样本中被正确预测的比例(越大越好) TruePositive Rate：跟 Recall 定义一样 （越大越好) FPR :负样本中被预测为正的比例(越小越好)</strong></p><p>对于一个二分类问题，往往要设定一个 threshold，当预测值大于这个threshold 时预测为正样本，小于这个 threshold 时预测为负样本。如果以Recall 为横轴，Precision 为纵轴，那么设定一个 threshold时，便可在坐标轴上画出一个点，设定多个 threshold则可以画出一条曲线，这条曲线便是 PR 曲线。</p><p><strong>PR 曲线是以 Recall 为横轴，Precision 为纵轴；而 ROC曲线则是以FPR 为横轴，TPR 为纵轴。</strong></p><p>那么两者的关系是怎样的？</p><h2 id="对比">对比</h2><p><a href="https://www.biostat.wisc.edu/~page/rocpr.pdf">TheRelationship Between Precision-Recall and ROC Curves</a>中证明了以下两条定理</p><p><strong>定理1</strong>：对于一个给定的的数据集，ROC空间和PR空间存在一一对应的关系，因为二者包含完全一致的混淆矩阵。我们可以将ROC曲线转化为PR曲线，反之亦然。</p><p><strong>定理2</strong>：对于一个给定数目的正负样本数据集，曲线 A 在ROC 空间优于曲线 B ，当且仅当在 PR 空间中曲线 A 也优于曲线 B。</p><p>定理 2 中 “曲线A优于曲线B” 是指曲线 B 的所有部分与曲线 A 重合或在曲线A之下。<strong>而在ROC空间，ROC曲线越凸向左上方向效果越好。与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。</strong></p><p>从定理 2 来看，ROC 空间和 PR空间两个指标似乎具有冗余性，那么为什么还需要这同时两个指标呢？答案是在<strong>两者在样本不均衡的情况下表现有较大差异</strong>。</p><p>下图是ROC曲线和Precision-Recall曲线的对比，摘自 <a href="https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf">Anintroduction to ROC analysis</a></p><p><img src="/2022/05/27/0527/ROC_PR.png" alt="ROC_PR.png-89.5kB" style="zoom:50%;"></p><p>图 (a) 和 (b) 是在样本正负比例为 1:1 下的 ROC 曲线和PR 曲线，图(c) 和(d) 是在样本正负比例为 1:100 下的 ROC 曲线和PR 曲线。</p><p>从结果来看：<strong>当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。</strong></p><p>文章 <a href="http://alexkong.net/2013/06/introduction-to-auc-and-roc/">ROC和AUC介绍以及如何计算AUC</a>以及<a href="https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf">Anintroduction to ROC analysis</a>中都认为这是个优点，原因是在实际的数据集中经常会出现类不平衡（classimbalance）现象，即负样本比正样本多很多（或者相反），而 <strong>ROC这种对不平衡样本的鲁棒性使得其曲线下的面积 AUC不会发生突变</strong>。</p><p>那么，AUC 意味这什么？首先 <strong>AUC值是一个概率值，表示随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率</strong>。</p><p>因此，AUC值实际上反映了模型的 rank能力，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面。这个指标尤其适用在某些场景下(如CTR 预估)，每次要返回的是最有可能点击的若干个广告(根据CTR排序,选择排在前面的若干个)，实际上便是在考验模型的排序能力。除此之外，CTR中存在着样本不均衡的问题，正负样本比例通常会大于 1:100，如果采用 PR曲线，则会导致 AUC 发生剧变，无法较好反映模型效果。</p><p>然而，<strong>ROC曲线不会随着类别分布的改变而改变的优点在一定程度上也是其缺点</strong>。因为ROC 曲线这种不变性其实影响着的是 AUC值，或者说是评估分类器的整体性能。但是在<strong>某些场景下，我们会更关注正样本，这时候就要用到PR 曲线了。</strong></p><p>比如说信用卡欺诈检测，我们会更关注 precision 和recall，比如说如果要求预测出为欺诈的人尽可能准确，那么就是要提高precision；而如果要尽可能多地预测出潜在的欺诈人群，那么就是要提高recall。一般来说，提高二分类的 threshold 就能提高 precision，降低threshold 就能提高 recall，这时便可观察 PR 曲线，得到最优的threshold。</p><h2 id="总结">总结</h2><p>综上，有以下几条结论（参考 <a href="https://zhuanlan.zhihu.com/p/34655990">机器学习之类别不平衡问题(2) —— ROC和PR曲线</a>）</p><ol type="1"><li>ROC曲线由于兼顾正例与负例，所以适用于评估分类器的整体性能(通常是会计算AUC，表示模型的rank性能)，相比而言PR曲线完全聚焦于正例。</li><li>如果有<strong>多份数据且存在不同的类别分布</strong>。比如信用卡欺诈问题中每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为类别分布改变可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较；反之，如果想测试不同类别分布下对分类器的性能的影响，则PR曲线比较适合。</li><li>如果想要评估在<strong>相同的类别分布下正例的预测情况</strong>，则宜选PR曲线。类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计，所以大部分时候还是PR曲线更好。(参考上面Quora 的例子)</li><li>最后可以根据具体的应用，在曲线上找到最优的点，得到相对应的precision，recall，f1score等指标，去调整模型的阈值，从而得到一个符合具体应用的模型。</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;这个月断断续续看了两篇文章，一篇是多场景建模的MMoE，感觉这篇文章的Motivation很make
sense ，后面有机会可以尝试一下。另一篇收获不大，就不细说了，不过
最近开始着手模型的训练，在和同事交流时，发现自己对于AUC的理解还不太深刻，于是感觉加班补了一下。&lt;/p&gt;
&lt;p&gt;首先要了解AUC，就得知道ROC 曲线和 PR 曲线，ROC 曲线和 PR
曲线是评估机器学习算法性能的两条重要曲线，两者概念比较容易混淆，但是两者的使用场景是不同的。我这里主要讲述两种曲线的含义以及应用的场景。&lt;/p&gt;</summary>
    
    
    
    <category term="Notes" scheme="http://kerwinblog.top/categories/Notes/"/>
    
    
    <category term="Rec System" scheme="http://kerwinblog.top/tags/Rec-System/"/>
    
  </entry>
  
</feed>
